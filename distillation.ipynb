{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distillation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E20iEpB1qB_U",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXEF8f3moEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pytorch basic functions/classes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import torchvision functions/classes for MNIST import and data loaders\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Other imports\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Set device on which code is run\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl08sOn-p-WI",
        "colab_type": "text"
      },
      "source": [
        "Defining support functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZ1UPkh2iN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define support function used to convert label to one-hot encoded tensor\n",
        "def convert_labels(labels):\n",
        "    target = torch.zeros([len(labels), 10], dtype=torch.float32)\n",
        "    for i, l in enumerate(labels):\n",
        "      target[i][l] = 1.0\n",
        "    return target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MepCPdkTqMD3",
        "colab_type": "text"
      },
      "source": [
        "Define our network model (the hidden layers size is specified through the constructor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFrlhvun-U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define MLP model and its layers\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=1200, dropout=0.0, hidden_dropout=0.0):\n",
        "        super(Model, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden1 = nn.Linear(784, hidden_size, bias=True)\n",
        "        self.hidden1_dropout = nn.Dropout(hidden_dropout)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.hidden2_dropout = nn.Dropout(hidden_dropout)\n",
        "        self.hidden3 = nn.Linear(hidden_size, 10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = self.hidden1_dropout(x)\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.hidden2_dropout(x)\n",
        "        x = self.hidden3(x)\n",
        "        return x#, F.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwzMAeT7qZCW",
        "colab_type": "text"
      },
      "source": [
        "Downloading MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONsjY_Fmpe0t",
        "colab_type": "code",
        "outputId": "aea8df1b-2f4b-40b8-a8c2-7d8e331dc0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Define transform from PIL image to tensor and normalize to 1x768 pixels\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomAffine(0, (1/14, 1/14)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Set batch size for data loaders\n",
        "batch_size = 128\n",
        "\n",
        "# (Down)load training set\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# (Down)load test set\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# (Down)load training set without a specific digit\n",
        "trainset_noDigit = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "\n",
        "#   Set here the digit to exclude\n",
        "idx = trainset_noDigit.train_labels!=3\n",
        "\n",
        "trainset_noDigit.targets = trainset_noDigit.targets[idx]\n",
        "trainset_noDigit.data = trainset_noDigit.data[idx]\n",
        "trainset_noDigit.transform = train_transform\n",
        "\n",
        "trainloader_noDigit = torch.utils.data.DataLoader(trainset_noDigit, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "'''examples = enumerate(trainloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "#fig'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'examples = enumerate(trainloader)\\nbatch_idx, (example_data, example_targets) = next(examples)\\n\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure()\\nfor i in range(6):\\n  plt.subplot(2,3,i+1)\\n  plt.tight_layout()\\n  plt.imshow(example_data[i][0], cmap=\\'gray\\', interpolation=\\'none\\')\\n  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\\n  plt.xticks([])\\n  plt.yticks([])\\n#fig'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdksm3J5qfzu",
        "colab_type": "text"
      },
      "source": [
        "Training the Deep Teacher Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rOCDtvFrTa4",
        "colab_type": "code",
        "outputId": "0b685595-bc48-4437-8997-df2254a7da9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Setup model and move it to the GPU\n",
        "net = Model(dropout=0.2, hidden_dropout=0.5)\n",
        "net.to(device)\n",
        "\n",
        "# Set up loss function and optimizer: \n",
        "#     using cross entropy loss because it's better for classification task\n",
        "\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum=0.9)\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.00001)\n",
        "\n",
        "# Run over 100 epochs (1 epoch = visited all items in dataset)\n",
        "for epoch in range(1000):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "\n",
        "    if(epoch%100 == 0 and epoch != 0):\n",
        "\n",
        "      learning_rate = learning_rate * 0.5 #- (0.001) # or maybe decrease by (learning_rate * 0.1)\n",
        "      optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum=0.9)\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "\n",
        "        # This for not cross entropy\n",
        "        #target = convert_labels(labels).to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        target = labels.to(device).long()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += len(data)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # print every epoch\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save model after having finished training\n",
        "PATH = './mnist_dropout_100_epoch.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Saved Model')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 0.517\n",
            "[2] loss: 0.268\n",
            "[3] loss: 0.200\n",
            "[4] loss: 0.173\n",
            "[5] loss: 0.155\n",
            "[6] loss: 0.140\n",
            "[7] loss: 0.131\n",
            "[8] loss: 0.121\n",
            "[9] loss: 0.117\n",
            "[10] loss: 0.109\n",
            "[11] loss: 0.106\n",
            "[12] loss: 0.103\n",
            "[13] loss: 0.099\n",
            "[14] loss: 0.095\n",
            "[15] loss: 0.092\n",
            "[16] loss: 0.089\n",
            "[17] loss: 0.087\n",
            "[18] loss: 0.085\n",
            "[19] loss: 0.084\n",
            "[20] loss: 0.082\n",
            "[21] loss: 0.079\n",
            "[22] loss: 0.077\n",
            "[23] loss: 0.078\n",
            "[24] loss: 0.075\n",
            "[25] loss: 0.073\n",
            "[26] loss: 0.073\n",
            "[27] loss: 0.071\n",
            "[28] loss: 0.071\n",
            "[29] loss: 0.071\n",
            "[30] loss: 0.068\n",
            "[31] loss: 0.068\n",
            "[32] loss: 0.067\n",
            "[33] loss: 0.066\n",
            "[34] loss: 0.065\n",
            "[35] loss: 0.064\n",
            "[36] loss: 0.062\n",
            "[37] loss: 0.063\n",
            "[38] loss: 0.063\n",
            "[39] loss: 0.062\n",
            "[40] loss: 0.061\n",
            "[41] loss: 0.062\n",
            "[42] loss: 0.059\n",
            "[43] loss: 0.060\n",
            "[44] loss: 0.057\n",
            "[45] loss: 0.059\n",
            "[46] loss: 0.057\n",
            "[47] loss: 0.058\n",
            "[48] loss: 0.057\n",
            "[49] loss: 0.056\n",
            "[50] loss: 0.056\n",
            "[51] loss: 0.056\n",
            "[52] loss: 0.057\n",
            "[53] loss: 0.056\n",
            "[54] loss: 0.053\n",
            "[55] loss: 0.053\n",
            "[56] loss: 0.053\n",
            "[57] loss: 0.053\n",
            "[58] loss: 0.052\n",
            "[59] loss: 0.054\n",
            "[60] loss: 0.053\n",
            "[61] loss: 0.052\n",
            "[62] loss: 0.051\n",
            "[63] loss: 0.051\n",
            "[64] loss: 0.049\n",
            "[65] loss: 0.050\n",
            "[66] loss: 0.051\n",
            "[67] loss: 0.049\n",
            "[68] loss: 0.049\n",
            "[69] loss: 0.049\n",
            "[70] loss: 0.050\n",
            "[71] loss: 0.047\n",
            "[72] loss: 0.048\n",
            "[73] loss: 0.049\n",
            "[74] loss: 0.048\n",
            "[75] loss: 0.047\n",
            "[76] loss: 0.047\n",
            "[77] loss: 0.047\n",
            "[78] loss: 0.048\n",
            "[79] loss: 0.047\n",
            "[80] loss: 0.048\n",
            "[81] loss: 0.046\n",
            "[82] loss: 0.046\n",
            "[83] loss: 0.046\n",
            "[84] loss: 0.046\n",
            "[85] loss: 0.045\n",
            "[86] loss: 0.046\n",
            "[87] loss: 0.047\n",
            "[88] loss: 0.046\n",
            "[89] loss: 0.044\n",
            "[90] loss: 0.045\n",
            "[91] loss: 0.044\n",
            "[92] loss: 0.045\n",
            "[93] loss: 0.044\n",
            "[94] loss: 0.044\n",
            "[95] loss: 0.043\n",
            "[96] loss: 0.044\n",
            "[97] loss: 0.045\n",
            "[98] loss: 0.044\n",
            "[99] loss: 0.044\n",
            "[100] loss: 0.043\n",
            "[101] loss: 0.040\n",
            "[102] loss: 0.039\n",
            "[103] loss: 0.037\n",
            "[104] loss: 0.037\n",
            "[105] loss: 0.036\n",
            "[106] loss: 0.036\n",
            "[107] loss: 0.038\n",
            "[108] loss: 0.037\n",
            "[109] loss: 0.036\n",
            "[110] loss: 0.037\n",
            "[111] loss: 0.035\n",
            "[112] loss: 0.036\n",
            "[113] loss: 0.037\n",
            "[114] loss: 0.036\n",
            "[115] loss: 0.035\n",
            "[116] loss: 0.035\n",
            "[117] loss: 0.037\n",
            "[118] loss: 0.035\n",
            "[119] loss: 0.035\n",
            "[120] loss: 0.034\n",
            "[121] loss: 0.036\n",
            "[122] loss: 0.035\n",
            "[123] loss: 0.035\n",
            "[124] loss: 0.037\n",
            "[125] loss: 0.036\n",
            "[126] loss: 0.036\n",
            "[127] loss: 0.035\n",
            "[128] loss: 0.035\n",
            "[129] loss: 0.034\n",
            "[130] loss: 0.034\n",
            "[131] loss: 0.034\n",
            "[132] loss: 0.034\n",
            "[133] loss: 0.034\n",
            "[134] loss: 0.035\n",
            "[135] loss: 0.034\n",
            "[136] loss: 0.035\n",
            "[137] loss: 0.035\n",
            "[138] loss: 0.034\n",
            "[139] loss: 0.034\n",
            "[140] loss: 0.033\n",
            "[141] loss: 0.034\n",
            "[142] loss: 0.033\n",
            "[143] loss: 0.034\n",
            "[144] loss: 0.033\n",
            "[145] loss: 0.033\n",
            "[146] loss: 0.033\n",
            "[147] loss: 0.032\n",
            "[148] loss: 0.033\n",
            "[149] loss: 0.033\n",
            "[150] loss: 0.033\n",
            "[151] loss: 0.033\n",
            "[152] loss: 0.033\n",
            "[153] loss: 0.034\n",
            "[154] loss: 0.033\n",
            "[155] loss: 0.032\n",
            "[156] loss: 0.033\n",
            "[157] loss: 0.033\n",
            "[158] loss: 0.032\n",
            "[159] loss: 0.033\n",
            "[160] loss: 0.033\n",
            "[161] loss: 0.032\n",
            "[162] loss: 0.031\n",
            "[163] loss: 0.032\n",
            "[164] loss: 0.032\n",
            "[165] loss: 0.032\n",
            "[166] loss: 0.033\n",
            "[167] loss: 0.032\n",
            "[168] loss: 0.032\n",
            "[169] loss: 0.033\n",
            "[170] loss: 0.033\n",
            "[171] loss: 0.032\n",
            "[172] loss: 0.031\n",
            "[173] loss: 0.033\n",
            "[174] loss: 0.032\n",
            "[175] loss: 0.032\n",
            "[176] loss: 0.031\n",
            "[177] loss: 0.030\n",
            "[178] loss: 0.031\n",
            "[179] loss: 0.031\n",
            "[180] loss: 0.032\n",
            "[181] loss: 0.032\n",
            "[182] loss: 0.032\n",
            "[183] loss: 0.030\n",
            "[184] loss: 0.032\n",
            "[185] loss: 0.032\n",
            "[186] loss: 0.032\n",
            "[187] loss: 0.031\n",
            "[188] loss: 0.031\n",
            "[189] loss: 0.032\n",
            "[190] loss: 0.030\n",
            "[191] loss: 0.031\n",
            "[192] loss: 0.032\n",
            "[193] loss: 0.031\n",
            "[194] loss: 0.030\n",
            "[195] loss: 0.031\n",
            "[196] loss: 0.031\n",
            "[197] loss: 0.030\n",
            "[198] loss: 0.031\n",
            "[199] loss: 0.030\n",
            "[200] loss: 0.030\n",
            "[201] loss: 0.030\n",
            "[202] loss: 0.029\n",
            "[203] loss: 0.029\n",
            "[204] loss: 0.028\n",
            "[205] loss: 0.029\n",
            "[206] loss: 0.029\n",
            "[207] loss: 0.027\n",
            "[208] loss: 0.028\n",
            "[209] loss: 0.027\n",
            "[210] loss: 0.028\n",
            "[211] loss: 0.028\n",
            "[212] loss: 0.028\n",
            "[213] loss: 0.030\n",
            "[214] loss: 0.028\n",
            "[215] loss: 0.028\n",
            "[216] loss: 0.028\n",
            "[217] loss: 0.028\n",
            "[218] loss: 0.029\n",
            "[219] loss: 0.028\n",
            "[220] loss: 0.028\n",
            "[221] loss: 0.028\n",
            "[222] loss: 0.028\n",
            "[223] loss: 0.029\n",
            "[224] loss: 0.028\n",
            "[225] loss: 0.027\n",
            "[226] loss: 0.028\n",
            "[227] loss: 0.027\n",
            "[228] loss: 0.027\n",
            "[229] loss: 0.028\n",
            "[230] loss: 0.028\n",
            "[231] loss: 0.028\n",
            "[232] loss: 0.027\n",
            "[233] loss: 0.027\n",
            "[234] loss: 0.028\n",
            "[235] loss: 0.028\n",
            "[236] loss: 0.026\n",
            "[237] loss: 0.027\n",
            "[238] loss: 0.027\n",
            "[239] loss: 0.028\n",
            "[240] loss: 0.027\n",
            "[241] loss: 0.028\n",
            "[242] loss: 0.028\n",
            "[243] loss: 0.027\n",
            "[244] loss: 0.028\n",
            "[245] loss: 0.027\n",
            "[246] loss: 0.028\n",
            "[247] loss: 0.027\n",
            "[248] loss: 0.026\n",
            "[249] loss: 0.027\n",
            "[250] loss: 0.027\n",
            "[251] loss: 0.027\n",
            "[252] loss: 0.028\n",
            "[253] loss: 0.027\n",
            "[254] loss: 0.027\n",
            "[255] loss: 0.028\n",
            "[256] loss: 0.027\n",
            "[257] loss: 0.027\n",
            "[258] loss: 0.027\n",
            "[259] loss: 0.027\n",
            "[260] loss: 0.026\n",
            "[261] loss: 0.027\n",
            "[262] loss: 0.027\n",
            "[263] loss: 0.026\n",
            "[264] loss: 0.027\n",
            "[265] loss: 0.026\n",
            "[266] loss: 0.027\n",
            "[267] loss: 0.027\n",
            "[268] loss: 0.026\n",
            "[269] loss: 0.026\n",
            "[270] loss: 0.027\n",
            "[271] loss: 0.026\n",
            "[272] loss: 0.026\n",
            "[273] loss: 0.026\n",
            "[274] loss: 0.026\n",
            "[275] loss: 0.027\n",
            "[276] loss: 0.026\n",
            "[277] loss: 0.026\n",
            "[278] loss: 0.026\n",
            "[279] loss: 0.027\n",
            "[280] loss: 0.027\n",
            "[281] loss: 0.026\n",
            "[282] loss: 0.026\n",
            "[283] loss: 0.027\n",
            "[284] loss: 0.027\n",
            "[285] loss: 0.026\n",
            "[286] loss: 0.026\n",
            "[287] loss: 0.026\n",
            "[288] loss: 0.026\n",
            "[289] loss: 0.026\n",
            "[290] loss: 0.027\n",
            "[291] loss: 0.026\n",
            "[292] loss: 0.026\n",
            "[293] loss: 0.025\n",
            "[294] loss: 0.027\n",
            "[295] loss: 0.026\n",
            "[296] loss: 0.026\n",
            "[297] loss: 0.026\n",
            "[298] loss: 0.026\n",
            "[299] loss: 0.026\n",
            "[300] loss: 0.026\n",
            "[301] loss: 0.025\n",
            "[302] loss: 0.026\n",
            "[303] loss: 0.025\n",
            "[304] loss: 0.026\n",
            "[305] loss: 0.026\n",
            "[306] loss: 0.025\n",
            "[307] loss: 0.025\n",
            "[308] loss: 0.026\n",
            "[309] loss: 0.025\n",
            "[310] loss: 0.024\n",
            "[311] loss: 0.025\n",
            "[312] loss: 0.024\n",
            "[313] loss: 0.025\n",
            "[314] loss: 0.025\n",
            "[315] loss: 0.025\n",
            "[316] loss: 0.026\n",
            "[317] loss: 0.026\n",
            "[318] loss: 0.025\n",
            "[319] loss: 0.024\n",
            "[320] loss: 0.025\n",
            "[321] loss: 0.025\n",
            "[322] loss: 0.024\n",
            "[323] loss: 0.024\n",
            "[324] loss: 0.025\n",
            "[325] loss: 0.026\n",
            "[326] loss: 0.024\n",
            "[327] loss: 0.025\n",
            "[328] loss: 0.026\n",
            "[329] loss: 0.024\n",
            "[330] loss: 0.025\n",
            "[331] loss: 0.026\n",
            "[332] loss: 0.025\n",
            "[333] loss: 0.025\n",
            "[334] loss: 0.024\n",
            "[335] loss: 0.025\n",
            "[336] loss: 0.025\n",
            "[337] loss: 0.025\n",
            "[338] loss: 0.025\n",
            "[339] loss: 0.025\n",
            "[340] loss: 0.025\n",
            "[341] loss: 0.024\n",
            "[342] loss: 0.025\n",
            "[343] loss: 0.025\n",
            "[344] loss: 0.025\n",
            "[345] loss: 0.024\n",
            "[346] loss: 0.025\n",
            "[347] loss: 0.026\n",
            "[348] loss: 0.025\n",
            "[349] loss: 0.024\n",
            "[350] loss: 0.024\n",
            "[351] loss: 0.024\n",
            "[352] loss: 0.025\n",
            "[353] loss: 0.024\n",
            "[354] loss: 0.024\n",
            "[355] loss: 0.024\n",
            "[356] loss: 0.025\n",
            "[357] loss: 0.024\n",
            "[358] loss: 0.025\n",
            "[359] loss: 0.026\n",
            "[360] loss: 0.025\n",
            "[361] loss: 0.026\n",
            "[362] loss: 0.025\n",
            "[363] loss: 0.025\n",
            "[364] loss: 0.024\n",
            "[365] loss: 0.024\n",
            "[366] loss: 0.024\n",
            "[367] loss: 0.024\n",
            "[368] loss: 0.025\n",
            "[369] loss: 0.023\n",
            "[370] loss: 0.024\n",
            "[371] loss: 0.024\n",
            "[372] loss: 0.024\n",
            "[373] loss: 0.025\n",
            "[374] loss: 0.025\n",
            "[375] loss: 0.024\n",
            "[376] loss: 0.024\n",
            "[377] loss: 0.024\n",
            "[378] loss: 0.023\n",
            "[379] loss: 0.024\n",
            "[380] loss: 0.025\n",
            "[381] loss: 0.024\n",
            "[382] loss: 0.025\n",
            "[383] loss: 0.025\n",
            "[384] loss: 0.025\n",
            "[385] loss: 0.024\n",
            "[386] loss: 0.024\n",
            "[387] loss: 0.025\n",
            "[388] loss: 0.025\n",
            "[389] loss: 0.024\n",
            "[390] loss: 0.025\n",
            "[391] loss: 0.024\n",
            "[392] loss: 0.024\n",
            "[393] loss: 0.024\n",
            "[394] loss: 0.024\n",
            "[395] loss: 0.025\n",
            "[396] loss: 0.025\n",
            "[397] loss: 0.023\n",
            "[398] loss: 0.024\n",
            "[399] loss: 0.025\n",
            "[400] loss: 0.024\n",
            "[401] loss: 0.023\n",
            "[402] loss: 0.024\n",
            "[403] loss: 0.024\n",
            "[404] loss: 0.023\n",
            "[405] loss: 0.024\n",
            "[406] loss: 0.024\n",
            "[407] loss: 0.023\n",
            "[408] loss: 0.023\n",
            "[409] loss: 0.024\n",
            "[410] loss: 0.024\n",
            "[411] loss: 0.025\n",
            "[412] loss: 0.022\n",
            "[413] loss: 0.023\n",
            "[414] loss: 0.024\n",
            "[415] loss: 0.024\n",
            "[416] loss: 0.023\n",
            "[417] loss: 0.024\n",
            "[418] loss: 0.024\n",
            "[419] loss: 0.024\n",
            "[420] loss: 0.024\n",
            "[421] loss: 0.023\n",
            "[422] loss: 0.023\n",
            "[423] loss: 0.023\n",
            "[424] loss: 0.023\n",
            "[425] loss: 0.024\n",
            "[426] loss: 0.024\n",
            "[427] loss: 0.023\n",
            "[428] loss: 0.023\n",
            "[429] loss: 0.025\n",
            "[430] loss: 0.023\n",
            "[431] loss: 0.024\n",
            "[432] loss: 0.024\n",
            "[433] loss: 0.023\n",
            "[434] loss: 0.023\n",
            "[435] loss: 0.024\n",
            "[436] loss: 0.023\n",
            "[437] loss: 0.024\n",
            "[438] loss: 0.023\n",
            "[439] loss: 0.024\n",
            "[440] loss: 0.023\n",
            "[441] loss: 0.024\n",
            "[442] loss: 0.022\n",
            "[443] loss: 0.023\n",
            "[444] loss: 0.024\n",
            "[445] loss: 0.023\n",
            "[446] loss: 0.023\n",
            "[447] loss: 0.024\n",
            "[448] loss: 0.023\n",
            "[449] loss: 0.023\n",
            "[450] loss: 0.023\n",
            "[451] loss: 0.023\n",
            "[452] loss: 0.023\n",
            "[453] loss: 0.024\n",
            "[454] loss: 0.023\n",
            "[455] loss: 0.024\n",
            "[456] loss: 0.023\n",
            "[457] loss: 0.024\n",
            "[458] loss: 0.023\n",
            "[459] loss: 0.023\n",
            "[460] loss: 0.024\n",
            "[461] loss: 0.023\n",
            "[462] loss: 0.023\n",
            "[463] loss: 0.022\n",
            "[464] loss: 0.023\n",
            "[465] loss: 0.024\n",
            "[466] loss: 0.023\n",
            "[467] loss: 0.023\n",
            "[468] loss: 0.024\n",
            "[469] loss: 0.022\n",
            "[470] loss: 0.024\n",
            "[471] loss: 0.023\n",
            "[472] loss: 0.023\n",
            "[473] loss: 0.023\n",
            "[474] loss: 0.022\n",
            "[475] loss: 0.023\n",
            "[476] loss: 0.024\n",
            "[477] loss: 0.023\n",
            "[478] loss: 0.023\n",
            "[479] loss: 0.024\n",
            "[480] loss: 0.023\n",
            "[481] loss: 0.024\n",
            "[482] loss: 0.023\n",
            "[483] loss: 0.023\n",
            "[484] loss: 0.024\n",
            "[485] loss: 0.023\n",
            "[486] loss: 0.024\n",
            "[487] loss: 0.024\n",
            "[488] loss: 0.023\n",
            "[489] loss: 0.023\n",
            "[490] loss: 0.024\n",
            "[491] loss: 0.023\n",
            "[492] loss: 0.022\n",
            "[493] loss: 0.023\n",
            "[494] loss: 0.022\n",
            "[495] loss: 0.023\n",
            "[496] loss: 0.023\n",
            "[497] loss: 0.023\n",
            "[498] loss: 0.023\n",
            "[499] loss: 0.022\n",
            "[500] loss: 0.024\n",
            "[501] loss: 0.024\n",
            "[502] loss: 0.023\n",
            "[503] loss: 0.023\n",
            "[504] loss: 0.023\n",
            "[505] loss: 0.022\n",
            "[506] loss: 0.022\n",
            "[507] loss: 0.024\n",
            "[508] loss: 0.022\n",
            "[509] loss: 0.023\n",
            "[510] loss: 0.022\n",
            "[511] loss: 0.023\n",
            "[512] loss: 0.024\n",
            "[513] loss: 0.023\n",
            "[514] loss: 0.023\n",
            "[515] loss: 0.022\n",
            "[516] loss: 0.023\n",
            "[517] loss: 0.023\n",
            "[518] loss: 0.023\n",
            "[519] loss: 0.023\n",
            "[520] loss: 0.021\n",
            "[521] loss: 0.023\n",
            "[522] loss: 0.023\n",
            "[523] loss: 0.024\n",
            "[524] loss: 0.023\n",
            "[525] loss: 0.023\n",
            "[526] loss: 0.022\n",
            "[527] loss: 0.023\n",
            "[528] loss: 0.023\n",
            "[529] loss: 0.023\n",
            "[530] loss: 0.022\n",
            "[531] loss: 0.023\n",
            "[532] loss: 0.023\n",
            "[533] loss: 0.023\n",
            "[534] loss: 0.023\n",
            "[535] loss: 0.023\n",
            "[536] loss: 0.024\n",
            "[537] loss: 0.023\n",
            "[538] loss: 0.023\n",
            "[539] loss: 0.023\n",
            "[540] loss: 0.023\n",
            "[541] loss: 0.023\n",
            "[542] loss: 0.022\n",
            "[543] loss: 0.023\n",
            "[544] loss: 0.023\n",
            "[545] loss: 0.022\n",
            "[546] loss: 0.022\n",
            "[547] loss: 0.023\n",
            "[548] loss: 0.023\n",
            "[549] loss: 0.022\n",
            "[550] loss: 0.023\n",
            "[551] loss: 0.023\n",
            "[552] loss: 0.022\n",
            "[553] loss: 0.022\n",
            "[554] loss: 0.023\n",
            "[555] loss: 0.022\n",
            "[556] loss: 0.022\n",
            "[557] loss: 0.023\n",
            "[558] loss: 0.024\n",
            "[559] loss: 0.023\n",
            "[560] loss: 0.023\n",
            "[561] loss: 0.023\n",
            "[562] loss: 0.023\n",
            "[563] loss: 0.023\n",
            "[564] loss: 0.023\n",
            "[565] loss: 0.023\n",
            "[566] loss: 0.023\n",
            "[567] loss: 0.023\n",
            "[568] loss: 0.023\n",
            "[569] loss: 0.023\n",
            "[570] loss: 0.023\n",
            "[571] loss: 0.022\n",
            "[572] loss: 0.023\n",
            "[573] loss: 0.024\n",
            "[574] loss: 0.023\n",
            "[575] loss: 0.023\n",
            "[576] loss: 0.023\n",
            "[577] loss: 0.023\n",
            "[578] loss: 0.023\n",
            "[579] loss: 0.023\n",
            "[580] loss: 0.023\n",
            "[581] loss: 0.022\n",
            "[582] loss: 0.023\n",
            "[583] loss: 0.023\n",
            "[584] loss: 0.023\n",
            "[585] loss: 0.022\n",
            "[586] loss: 0.023\n",
            "[587] loss: 0.023\n",
            "[588] loss: 0.023\n",
            "[589] loss: 0.022\n",
            "[590] loss: 0.022\n",
            "[591] loss: 0.023\n",
            "[592] loss: 0.022\n",
            "[593] loss: 0.023\n",
            "[594] loss: 0.023\n",
            "[595] loss: 0.023\n",
            "[596] loss: 0.022\n",
            "[597] loss: 0.023\n",
            "[598] loss: 0.022\n",
            "[599] loss: 0.023\n",
            "[600] loss: 0.023\n",
            "[601] loss: 0.023\n",
            "[602] loss: 0.023\n",
            "[603] loss: 0.022\n",
            "[604] loss: 0.023\n",
            "[605] loss: 0.023\n",
            "[606] loss: 0.023\n",
            "[607] loss: 0.022\n",
            "[608] loss: 0.023\n",
            "[609] loss: 0.023\n",
            "[610] loss: 0.022\n",
            "[611] loss: 0.022\n",
            "[612] loss: 0.023\n",
            "[613] loss: 0.022\n",
            "[614] loss: 0.023\n",
            "[615] loss: 0.023\n",
            "[616] loss: 0.023\n",
            "[617] loss: 0.023\n",
            "[618] loss: 0.023\n",
            "[619] loss: 0.023\n",
            "[620] loss: 0.022\n",
            "[621] loss: 0.023\n",
            "[622] loss: 0.021\n",
            "[623] loss: 0.023\n",
            "[624] loss: 0.022\n",
            "[625] loss: 0.023\n",
            "[626] loss: 0.023\n",
            "[627] loss: 0.022\n",
            "[628] loss: 0.022\n",
            "[629] loss: 0.023\n",
            "[630] loss: 0.023\n",
            "[631] loss: 0.022\n",
            "[632] loss: 0.022\n",
            "[633] loss: 0.022\n",
            "[634] loss: 0.023\n",
            "[635] loss: 0.022\n",
            "[636] loss: 0.022\n",
            "[637] loss: 0.022\n",
            "[638] loss: 0.022\n",
            "[639] loss: 0.021\n",
            "[640] loss: 0.023\n",
            "[641] loss: 0.023\n",
            "[642] loss: 0.023\n",
            "[643] loss: 0.022\n",
            "[644] loss: 0.023\n",
            "[645] loss: 0.023\n",
            "[646] loss: 0.022\n",
            "[647] loss: 0.023\n",
            "[648] loss: 0.022\n",
            "[649] loss: 0.022\n",
            "[650] loss: 0.022\n",
            "[651] loss: 0.022\n",
            "[652] loss: 0.021\n",
            "[653] loss: 0.023\n",
            "[654] loss: 0.022\n",
            "[655] loss: 0.022\n",
            "[656] loss: 0.022\n",
            "[657] loss: 0.023\n",
            "[658] loss: 0.022\n",
            "[659] loss: 0.023\n",
            "[660] loss: 0.022\n",
            "[661] loss: 0.022\n",
            "[662] loss: 0.022\n",
            "[663] loss: 0.023\n",
            "[664] loss: 0.022\n",
            "[665] loss: 0.024\n",
            "[666] loss: 0.022\n",
            "[667] loss: 0.023\n",
            "[668] loss: 0.022\n",
            "[669] loss: 0.022\n",
            "[670] loss: 0.023\n",
            "[671] loss: 0.022\n",
            "[672] loss: 0.023\n",
            "[673] loss: 0.023\n",
            "[674] loss: 0.022\n",
            "[675] loss: 0.022\n",
            "[676] loss: 0.022\n",
            "[677] loss: 0.023\n",
            "[678] loss: 0.022\n",
            "[679] loss: 0.022\n",
            "[680] loss: 0.023\n",
            "[681] loss: 0.023\n",
            "[682] loss: 0.023\n",
            "[683] loss: 0.022\n",
            "[684] loss: 0.022\n",
            "[685] loss: 0.022\n",
            "[686] loss: 0.021\n",
            "[687] loss: 0.022\n",
            "[688] loss: 0.023\n",
            "[689] loss: 0.021\n",
            "[690] loss: 0.022\n",
            "[691] loss: 0.023\n",
            "[692] loss: 0.024\n",
            "[693] loss: 0.022\n",
            "[694] loss: 0.023\n",
            "[695] loss: 0.023\n",
            "[696] loss: 0.022\n",
            "[697] loss: 0.023\n",
            "[698] loss: 0.023\n",
            "[699] loss: 0.023\n",
            "[700] loss: 0.022\n",
            "[701] loss: 0.022\n",
            "[702] loss: 0.023\n",
            "[703] loss: 0.022\n",
            "[704] loss: 0.022\n",
            "[705] loss: 0.023\n",
            "[706] loss: 0.022\n",
            "[707] loss: 0.023\n",
            "[708] loss: 0.022\n",
            "[709] loss: 0.022\n",
            "[710] loss: 0.023\n",
            "[711] loss: 0.021\n",
            "[712] loss: 0.022\n",
            "[713] loss: 0.023\n",
            "[714] loss: 0.022\n",
            "[715] loss: 0.022\n",
            "[716] loss: 0.023\n",
            "[717] loss: 0.022\n",
            "[718] loss: 0.022\n",
            "[719] loss: 0.022\n",
            "[720] loss: 0.022\n",
            "[721] loss: 0.022\n",
            "[722] loss: 0.022\n",
            "[723] loss: 0.022\n",
            "[724] loss: 0.023\n",
            "[725] loss: 0.021\n",
            "[726] loss: 0.023\n",
            "[727] loss: 0.021\n",
            "[728] loss: 0.022\n",
            "[729] loss: 0.022\n",
            "[730] loss: 0.023\n",
            "[731] loss: 0.023\n",
            "[732] loss: 0.023\n",
            "[733] loss: 0.022\n",
            "[734] loss: 0.022\n",
            "[735] loss: 0.023\n",
            "[736] loss: 0.023\n",
            "[737] loss: 0.022\n",
            "[738] loss: 0.023\n",
            "[739] loss: 0.022\n",
            "[740] loss: 0.022\n",
            "[741] loss: 0.022\n",
            "[742] loss: 0.022\n",
            "[743] loss: 0.022\n",
            "[744] loss: 0.022\n",
            "[745] loss: 0.023\n",
            "[746] loss: 0.022\n",
            "[747] loss: 0.022\n",
            "[748] loss: 0.022\n",
            "[749] loss: 0.023\n",
            "[750] loss: 0.023\n",
            "[751] loss: 0.022\n",
            "[752] loss: 0.023\n",
            "[753] loss: 0.023\n",
            "[754] loss: 0.023\n",
            "[755] loss: 0.023\n",
            "[756] loss: 0.022\n",
            "[757] loss: 0.023\n",
            "[758] loss: 0.023\n",
            "[759] loss: 0.023\n",
            "[760] loss: 0.023\n",
            "[761] loss: 0.022\n",
            "[762] loss: 0.022\n",
            "[763] loss: 0.023\n",
            "[764] loss: 0.022\n",
            "[765] loss: 0.022\n",
            "[766] loss: 0.022\n",
            "[767] loss: 0.022\n",
            "[768] loss: 0.023\n",
            "[769] loss: 0.022\n",
            "[770] loss: 0.023\n",
            "[771] loss: 0.023\n",
            "[772] loss: 0.022\n",
            "[773] loss: 0.023\n",
            "[774] loss: 0.023\n",
            "[775] loss: 0.022\n",
            "[776] loss: 0.023\n",
            "[777] loss: 0.024\n",
            "[778] loss: 0.023\n",
            "[779] loss: 0.022\n",
            "[780] loss: 0.022\n",
            "[781] loss: 0.022\n",
            "[782] loss: 0.022\n",
            "[783] loss: 0.021\n",
            "[784] loss: 0.023\n",
            "[785] loss: 0.022\n",
            "[786] loss: 0.023\n",
            "[787] loss: 0.023\n",
            "[788] loss: 0.022\n",
            "[789] loss: 0.021\n",
            "[790] loss: 0.023\n",
            "[791] loss: 0.022\n",
            "[792] loss: 0.023\n",
            "[793] loss: 0.022\n",
            "[794] loss: 0.023\n",
            "[795] loss: 0.021\n",
            "[796] loss: 0.022\n",
            "[797] loss: 0.022\n",
            "[798] loss: 0.022\n",
            "[799] loss: 0.022\n",
            "[800] loss: 0.022\n",
            "[801] loss: 0.022\n",
            "[802] loss: 0.023\n",
            "[803] loss: 0.023\n",
            "[804] loss: 0.022\n",
            "[805] loss: 0.022\n",
            "[806] loss: 0.023\n",
            "[807] loss: 0.022\n",
            "[808] loss: 0.022\n",
            "[809] loss: 0.021\n",
            "[810] loss: 0.021\n",
            "[811] loss: 0.022\n",
            "[812] loss: 0.023\n",
            "[813] loss: 0.022\n",
            "[814] loss: 0.022\n",
            "[815] loss: 0.023\n",
            "[816] loss: 0.022\n",
            "[817] loss: 0.023\n",
            "[818] loss: 0.022\n",
            "[819] loss: 0.022\n",
            "[820] loss: 0.022\n",
            "[821] loss: 0.023\n",
            "[822] loss: 0.022\n",
            "[823] loss: 0.023\n",
            "[824] loss: 0.022\n",
            "[825] loss: 0.023\n",
            "[826] loss: 0.022\n",
            "[827] loss: 0.022\n",
            "[828] loss: 0.022\n",
            "[829] loss: 0.023\n",
            "[830] loss: 0.023\n",
            "[831] loss: 0.023\n",
            "[832] loss: 0.022\n",
            "[833] loss: 0.022\n",
            "[834] loss: 0.022\n",
            "[835] loss: 0.023\n",
            "[836] loss: 0.023\n",
            "[837] loss: 0.022\n",
            "[838] loss: 0.021\n",
            "[839] loss: 0.023\n",
            "[840] loss: 0.023\n",
            "[841] loss: 0.021\n",
            "[842] loss: 0.022\n",
            "[843] loss: 0.022\n",
            "[844] loss: 0.023\n",
            "[845] loss: 0.022\n",
            "[846] loss: 0.022\n",
            "[847] loss: 0.023\n",
            "[848] loss: 0.023\n",
            "[849] loss: 0.023\n",
            "[850] loss: 0.023\n",
            "[851] loss: 0.022\n",
            "[852] loss: 0.022\n",
            "[853] loss: 0.022\n",
            "[854] loss: 0.023\n",
            "[855] loss: 0.021\n",
            "[856] loss: 0.023\n",
            "[857] loss: 0.023\n",
            "[858] loss: 0.022\n",
            "[859] loss: 0.023\n",
            "[860] loss: 0.022\n",
            "[861] loss: 0.022\n",
            "[862] loss: 0.022\n",
            "[863] loss: 0.022\n",
            "[864] loss: 0.023\n",
            "[865] loss: 0.022\n",
            "[866] loss: 0.022\n",
            "[867] loss: 0.022\n",
            "[868] loss: 0.022\n",
            "[869] loss: 0.023\n",
            "[870] loss: 0.023\n",
            "[871] loss: 0.022\n",
            "[872] loss: 0.022\n",
            "[873] loss: 0.023\n",
            "[874] loss: 0.022\n",
            "[875] loss: 0.023\n",
            "[876] loss: 0.023\n",
            "[877] loss: 0.023\n",
            "[878] loss: 0.021\n",
            "[879] loss: 0.023\n",
            "[880] loss: 0.021\n",
            "[881] loss: 0.023\n",
            "[882] loss: 0.023\n",
            "[883] loss: 0.022\n",
            "[884] loss: 0.021\n",
            "[885] loss: 0.023\n",
            "[886] loss: 0.022\n",
            "[887] loss: 0.023\n",
            "[888] loss: 0.023\n",
            "[889] loss: 0.022\n",
            "[890] loss: 0.021\n",
            "[891] loss: 0.022\n",
            "[892] loss: 0.022\n",
            "[893] loss: 0.022\n",
            "[894] loss: 0.023\n",
            "[895] loss: 0.022\n",
            "[896] loss: 0.023\n",
            "[897] loss: 0.022\n",
            "[898] loss: 0.022\n",
            "[899] loss: 0.022\n",
            "[900] loss: 0.022\n",
            "[901] loss: 0.022\n",
            "[902] loss: 0.021\n",
            "[903] loss: 0.021\n",
            "[904] loss: 0.023\n",
            "[905] loss: 0.022\n",
            "[906] loss: 0.022\n",
            "[907] loss: 0.023\n",
            "[908] loss: 0.022\n",
            "[909] loss: 0.023\n",
            "[910] loss: 0.023\n",
            "[911] loss: 0.022\n",
            "[912] loss: 0.022\n",
            "[913] loss: 0.022\n",
            "[914] loss: 0.022\n",
            "[915] loss: 0.022\n",
            "[916] loss: 0.022\n",
            "[917] loss: 0.021\n",
            "[918] loss: 0.022\n",
            "[919] loss: 0.022\n",
            "[920] loss: 0.022\n",
            "[921] loss: 0.023\n",
            "[922] loss: 0.022\n",
            "[923] loss: 0.023\n",
            "[924] loss: 0.022\n",
            "[925] loss: 0.022\n",
            "[926] loss: 0.022\n",
            "[927] loss: 0.022\n",
            "[928] loss: 0.022\n",
            "[929] loss: 0.023\n",
            "[930] loss: 0.022\n",
            "[931] loss: 0.023\n",
            "[932] loss: 0.023\n",
            "[933] loss: 0.022\n",
            "[934] loss: 0.022\n",
            "[935] loss: 0.022\n",
            "[936] loss: 0.022\n",
            "[937] loss: 0.022\n",
            "[938] loss: 0.022\n",
            "[939] loss: 0.022\n",
            "[940] loss: 0.022\n",
            "[941] loss: 0.023\n",
            "[942] loss: 0.023\n",
            "[943] loss: 0.022\n",
            "[944] loss: 0.022\n",
            "[945] loss: 0.022\n",
            "[946] loss: 0.022\n",
            "[947] loss: 0.022\n",
            "[948] loss: 0.021\n",
            "[949] loss: 0.022\n",
            "[950] loss: 0.023\n",
            "[951] loss: 0.023\n",
            "[952] loss: 0.022\n",
            "[953] loss: 0.022\n",
            "[954] loss: 0.022\n",
            "[955] loss: 0.023\n",
            "[956] loss: 0.022\n",
            "[957] loss: 0.022\n",
            "[958] loss: 0.022\n",
            "[959] loss: 0.022\n",
            "[960] loss: 0.023\n",
            "[961] loss: 0.022\n",
            "[962] loss: 0.022\n",
            "[963] loss: 0.022\n",
            "[964] loss: 0.022\n",
            "[965] loss: 0.023\n",
            "[966] loss: 0.021\n",
            "[967] loss: 0.022\n",
            "[968] loss: 0.023\n",
            "[969] loss: 0.022\n",
            "[970] loss: 0.022\n",
            "[971] loss: 0.023\n",
            "[972] loss: 0.021\n",
            "[973] loss: 0.021\n",
            "[974] loss: 0.023\n",
            "[975] loss: 0.022\n",
            "[976] loss: 0.022\n",
            "[977] loss: 0.023\n",
            "[978] loss: 0.022\n",
            "[979] loss: 0.023\n",
            "[980] loss: 0.022\n",
            "[981] loss: 0.022\n",
            "[982] loss: 0.021\n",
            "[983] loss: 0.022\n",
            "[984] loss: 0.021\n",
            "[985] loss: 0.023\n",
            "[986] loss: 0.022\n",
            "[987] loss: 0.022\n",
            "[988] loss: 0.021\n",
            "[989] loss: 0.021\n",
            "[990] loss: 0.022\n",
            "[991] loss: 0.022\n",
            "[992] loss: 0.023\n",
            "[993] loss: 0.022\n",
            "[994] loss: 0.022\n",
            "[995] loss: 0.022\n",
            "[996] loss: 0.022\n",
            "[997] loss: 0.022\n",
            "[998] loss: 0.023\n",
            "[999] loss: 0.023\n",
            "[1000] loss: 0.023\n",
            "Finished Training\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Xqz-DlqmCT",
        "colab_type": "text"
      },
      "source": [
        "Run Deep Teacher model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI5BL9YF2dzp",
        "colab_type": "code",
        "outputId": "807af3f7-7d8a-422c-8cdd-f60ab346d7cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "# Instantiate model and load saved network parameters\n",
        "net = Model().to(device)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# Run model on test set and determine accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "wrong = np.zeros((10,10))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = convert_labels(labels).to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, target = torch.max(target.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        for i, val in enumerate(predicted):\n",
        "          if val != target[i]:\n",
        "            wrong[target[i]][val] += 1\n",
        "\n",
        "# Output model accuracy to user\n",
        "print('Accuracy of the network on the 10000 test images: %d %% (%d wrong out of %d)' % (\n",
        "    100 * correct / total, total - correct, total))\n",
        "#print(\" \"+str([0,1,2,3,4,5,6,7,8,9]))\n",
        "#print(wrong)\n",
        "\n",
        "# Plot confusion matrix\n",
        "df_cm = pd.DataFrame(wrong, index = [i for i in \"0123456789\"],\n",
        "                  columns = [i for i in \"0123456789\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 99 % (64 wrong out of 10000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa11183ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigRhVYyqvL1",
        "colab_type": "text"
      },
      "source": [
        "Train student model to mimic the teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOPgLsQ6D3Lj",
        "colab_type": "code",
        "outputId": "bbdbce18-1d21-49f2-e9e9-27d229044ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Custom student loss: linear combination of 2 cross-entropy losses\n",
        "#     The first one between student output and hard labels\n",
        "#     The second one between student output and soft labels from teacher\n",
        "\n",
        "def student_loss(outputs, labels, teacher_outputs, alpha, temperature):\n",
        "\n",
        "    #loss = torch.mean((weight*(outputA - targetA)**2) + (1-weight)*(outputB-targetB)**2)\n",
        "\n",
        "    #loss = weight*(F.cross_entropy(outputA, targetA)) + (1-weight)*(F.cross_entropy(outputB, targetB))\n",
        "\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "# Setup student model and move it to the GPU\n",
        "student_net = Model(hidden_size = 800)\n",
        "student_net.to(device)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "\n",
        "optimizer = optim.SGD(student_net.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = torch.optim.Adam(student_net.parameters(), lr=0.0001)\n",
        "\n",
        "# Run over 100 epochs (1 epoch = visited all items in dataset)\n",
        "for epoch in range(1000):\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = labels.to(device).long() #convert_labels(labels).to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Set temperature and the weights for losses linear combination\n",
        "        w = 0.7\n",
        "        T = 20\n",
        "\n",
        "        # Compute soft labels using deep teacher model previously trained\n",
        "        outputs_teacher = net(inputs)\n",
        "        #soft_labels = F.softmax(outputs_teacher/T, dim = 1)\n",
        "\n",
        "        # Abomination to obtain hard_labels for custom cross entropy loss\n",
        "        #teacher_hard_labels = torch.from_numpy(np.array([np.argmax(l.cpu().detach().numpy()) for l in soft_labels])).to(device).long()\n",
        "\n",
        "        # Student forward + backward + optimize\n",
        "        outputs_stud = student_net(inputs)\n",
        "        #outputs_stud = F.softmax(output_stud/T, dim = 1)\n",
        "        \n",
        "        loss = student_loss(outputs_stud, target, outputs_teacher, w, T)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += len(data)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # print every epoch\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save model after having finished training\n",
        "STUD_PATH = './mnist_student_100_epoch.pth'\n",
        "torch.save(student_net.state_dict(), STUD_PATH)\n",
        "\n",
        "print('Saved Model')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 1.383\n",
            "[2] loss: 0.998\n",
            "[3] loss: 0.786\n",
            "[4] loss: 0.697\n",
            "[5] loss: 0.631\n",
            "[6] loss: 0.570\n",
            "[7] loss: 0.506\n",
            "[8] loss: 0.450\n",
            "[9] loss: 0.398\n",
            "[10] loss: 0.352\n",
            "[11] loss: 0.318\n",
            "[12] loss: 0.289\n",
            "[13] loss: 0.265\n",
            "[14] loss: 0.243\n",
            "[15] loss: 0.225\n",
            "[16] loss: 0.209\n",
            "[17] loss: 0.194\n",
            "[18] loss: 0.182\n",
            "[19] loss: 0.171\n",
            "[20] loss: 0.161\n",
            "[21] loss: 0.151\n",
            "[22] loss: 0.144\n",
            "[23] loss: 0.138\n",
            "[24] loss: 0.131\n",
            "[25] loss: 0.125\n",
            "[26] loss: 0.121\n",
            "[27] loss: 0.115\n",
            "[28] loss: 0.111\n",
            "[29] loss: 0.108\n",
            "[30] loss: 0.104\n",
            "[31] loss: 0.100\n",
            "[32] loss: 0.098\n",
            "[33] loss: 0.094\n",
            "[34] loss: 0.092\n",
            "[35] loss: 0.089\n",
            "[36] loss: 0.087\n",
            "[37] loss: 0.084\n",
            "[38] loss: 0.083\n",
            "[39] loss: 0.081\n",
            "[40] loss: 0.079\n",
            "[41] loss: 0.077\n",
            "[42] loss: 0.075\n",
            "[43] loss: 0.074\n",
            "[44] loss: 0.072\n",
            "[45] loss: 0.070\n",
            "[46] loss: 0.069\n",
            "[47] loss: 0.068\n",
            "[48] loss: 0.067\n",
            "[49] loss: 0.066\n",
            "[50] loss: 0.064\n",
            "[51] loss: 0.063\n",
            "[52] loss: 0.062\n",
            "[53] loss: 0.061\n",
            "[54] loss: 0.060\n",
            "[55] loss: 0.060\n",
            "[56] loss: 0.058\n",
            "[57] loss: 0.058\n",
            "[58] loss: 0.057\n",
            "[59] loss: 0.056\n",
            "[60] loss: 0.055\n",
            "[61] loss: 0.055\n",
            "[62] loss: 0.054\n",
            "[63] loss: 0.053\n",
            "[64] loss: 0.053\n",
            "[65] loss: 0.052\n",
            "[66] loss: 0.051\n",
            "[67] loss: 0.051\n",
            "[68] loss: 0.050\n",
            "[69] loss: 0.050\n",
            "[70] loss: 0.049\n",
            "[71] loss: 0.048\n",
            "[72] loss: 0.047\n",
            "[73] loss: 0.048\n",
            "[74] loss: 0.047\n",
            "[75] loss: 0.046\n",
            "[76] loss: 0.046\n",
            "[77] loss: 0.045\n",
            "[78] loss: 0.045\n",
            "[79] loss: 0.045\n",
            "[80] loss: 0.044\n",
            "[81] loss: 0.044\n",
            "[82] loss: 0.043\n",
            "[83] loss: 0.043\n",
            "[84] loss: 0.043\n",
            "[85] loss: 0.043\n",
            "[86] loss: 0.042\n",
            "[87] loss: 0.041\n",
            "[88] loss: 0.041\n",
            "[89] loss: 0.041\n",
            "[90] loss: 0.040\n",
            "[91] loss: 0.040\n",
            "[92] loss: 0.040\n",
            "[93] loss: 0.040\n",
            "[94] loss: 0.039\n",
            "[95] loss: 0.039\n",
            "[96] loss: 0.039\n",
            "[97] loss: 0.038\n",
            "[98] loss: 0.038\n",
            "[99] loss: 0.037\n",
            "[100] loss: 0.038\n",
            "[101] loss: 0.037\n",
            "[102] loss: 0.037\n",
            "[103] loss: 0.037\n",
            "[104] loss: 0.036\n",
            "[105] loss: 0.036\n",
            "[106] loss: 0.036\n",
            "[107] loss: 0.035\n",
            "[108] loss: 0.035\n",
            "[109] loss: 0.035\n",
            "[110] loss: 0.035\n",
            "[111] loss: 0.034\n",
            "[112] loss: 0.035\n",
            "[113] loss: 0.034\n",
            "[114] loss: 0.034\n",
            "[115] loss: 0.034\n",
            "[116] loss: 0.034\n",
            "[117] loss: 0.034\n",
            "[118] loss: 0.033\n",
            "[119] loss: 0.033\n",
            "[120] loss: 0.033\n",
            "[121] loss: 0.033\n",
            "[122] loss: 0.032\n",
            "[123] loss: 0.032\n",
            "[124] loss: 0.032\n",
            "[125] loss: 0.032\n",
            "[126] loss: 0.032\n",
            "[127] loss: 0.032\n",
            "[128] loss: 0.032\n",
            "[129] loss: 0.031\n",
            "[130] loss: 0.031\n",
            "[131] loss: 0.031\n",
            "[132] loss: 0.031\n",
            "[133] loss: 0.031\n",
            "[134] loss: 0.030\n",
            "[135] loss: 0.030\n",
            "[136] loss: 0.030\n",
            "[137] loss: 0.030\n",
            "[138] loss: 0.030\n",
            "[139] loss: 0.030\n",
            "[140] loss: 0.029\n",
            "[141] loss: 0.029\n",
            "[142] loss: 0.029\n",
            "[143] loss: 0.029\n",
            "[144] loss: 0.029\n",
            "[145] loss: 0.029\n",
            "[146] loss: 0.029\n",
            "[147] loss: 0.029\n",
            "[148] loss: 0.029\n",
            "[149] loss: 0.028\n",
            "[150] loss: 0.028\n",
            "[151] loss: 0.028\n",
            "[152] loss: 0.028\n",
            "[153] loss: 0.028\n",
            "[154] loss: 0.028\n",
            "[155] loss: 0.028\n",
            "[156] loss: 0.028\n",
            "[157] loss: 0.027\n",
            "[158] loss: 0.027\n",
            "[159] loss: 0.027\n",
            "[160] loss: 0.027\n",
            "[161] loss: 0.027\n",
            "[162] loss: 0.027\n",
            "[163] loss: 0.027\n",
            "[164] loss: 0.027\n",
            "[165] loss: 0.026\n",
            "[166] loss: 0.027\n",
            "[167] loss: 0.026\n",
            "[168] loss: 0.026\n",
            "[169] loss: 0.026\n",
            "[170] loss: 0.026\n",
            "[171] loss: 0.026\n",
            "[172] loss: 0.026\n",
            "[173] loss: 0.025\n",
            "[174] loss: 0.026\n",
            "[175] loss: 0.026\n",
            "[176] loss: 0.025\n",
            "[177] loss: 0.025\n",
            "[178] loss: 0.025\n",
            "[179] loss: 0.025\n",
            "[180] loss: 0.025\n",
            "[181] loss: 0.025\n",
            "[182] loss: 0.025\n",
            "[183] loss: 0.025\n",
            "[184] loss: 0.025\n",
            "[185] loss: 0.025\n",
            "[186] loss: 0.025\n",
            "[187] loss: 0.025\n",
            "[188] loss: 0.024\n",
            "[189] loss: 0.024\n",
            "[190] loss: 0.025\n",
            "[191] loss: 0.024\n",
            "[192] loss: 0.024\n",
            "[193] loss: 0.024\n",
            "[194] loss: 0.024\n",
            "[195] loss: 0.024\n",
            "[196] loss: 0.024\n",
            "[197] loss: 0.024\n",
            "[198] loss: 0.024\n",
            "[199] loss: 0.024\n",
            "[200] loss: 0.023\n",
            "[201] loss: 0.024\n",
            "[202] loss: 0.024\n",
            "[203] loss: 0.023\n",
            "[204] loss: 0.023\n",
            "[205] loss: 0.023\n",
            "[206] loss: 0.023\n",
            "[207] loss: 0.023\n",
            "[208] loss: 0.023\n",
            "[209] loss: 0.023\n",
            "[210] loss: 0.023\n",
            "[211] loss: 0.023\n",
            "[212] loss: 0.023\n",
            "[213] loss: 0.023\n",
            "[214] loss: 0.023\n",
            "[215] loss: 0.023\n",
            "[216] loss: 0.022\n",
            "[217] loss: 0.022\n",
            "[218] loss: 0.022\n",
            "[219] loss: 0.022\n",
            "[220] loss: 0.022\n",
            "[221] loss: 0.022\n",
            "[222] loss: 0.022\n",
            "[223] loss: 0.022\n",
            "[224] loss: 0.022\n",
            "[225] loss: 0.022\n",
            "[226] loss: 0.022\n",
            "[227] loss: 0.022\n",
            "[228] loss: 0.022\n",
            "[229] loss: 0.022\n",
            "[230] loss: 0.022\n",
            "[231] loss: 0.022\n",
            "[232] loss: 0.022\n",
            "[233] loss: 0.022\n",
            "[234] loss: 0.021\n",
            "[235] loss: 0.022\n",
            "[236] loss: 0.021\n",
            "[237] loss: 0.021\n",
            "[238] loss: 0.021\n",
            "[239] loss: 0.021\n",
            "[240] loss: 0.021\n",
            "[241] loss: 0.021\n",
            "[242] loss: 0.021\n",
            "[243] loss: 0.021\n",
            "[244] loss: 0.021\n",
            "[245] loss: 0.021\n",
            "[246] loss: 0.021\n",
            "[247] loss: 0.021\n",
            "[248] loss: 0.021\n",
            "[249] loss: 0.021\n",
            "[250] loss: 0.021\n",
            "[251] loss: 0.021\n",
            "[252] loss: 0.021\n",
            "[253] loss: 0.020\n",
            "[254] loss: 0.021\n",
            "[255] loss: 0.020\n",
            "[256] loss: 0.020\n",
            "[257] loss: 0.020\n",
            "[258] loss: 0.020\n",
            "[259] loss: 0.020\n",
            "[260] loss: 0.020\n",
            "[261] loss: 0.020\n",
            "[262] loss: 0.020\n",
            "[263] loss: 0.020\n",
            "[264] loss: 0.020\n",
            "[265] loss: 0.020\n",
            "[266] loss: 0.020\n",
            "[267] loss: 0.020\n",
            "[268] loss: 0.020\n",
            "[269] loss: 0.020\n",
            "[270] loss: 0.020\n",
            "[271] loss: 0.020\n",
            "[272] loss: 0.020\n",
            "[273] loss: 0.020\n",
            "[274] loss: 0.020\n",
            "[275] loss: 0.020\n",
            "[276] loss: 0.020\n",
            "[277] loss: 0.020\n",
            "[278] loss: 0.019\n",
            "[279] loss: 0.019\n",
            "[280] loss: 0.019\n",
            "[281] loss: 0.019\n",
            "[282] loss: 0.019\n",
            "[283] loss: 0.019\n",
            "[284] loss: 0.019\n",
            "[285] loss: 0.019\n",
            "[286] loss: 0.019\n",
            "[287] loss: 0.019\n",
            "[288] loss: 0.019\n",
            "[289] loss: 0.019\n",
            "[290] loss: 0.019\n",
            "[291] loss: 0.019\n",
            "[292] loss: 0.019\n",
            "[293] loss: 0.019\n",
            "[294] loss: 0.019\n",
            "[295] loss: 0.019\n",
            "[296] loss: 0.019\n",
            "[297] loss: 0.019\n",
            "[298] loss: 0.019\n",
            "[299] loss: 0.019\n",
            "[300] loss: 0.019\n",
            "[301] loss: 0.019\n",
            "[302] loss: 0.019\n",
            "[303] loss: 0.019\n",
            "[304] loss: 0.018\n",
            "[305] loss: 0.018\n",
            "[306] loss: 0.018\n",
            "[307] loss: 0.018\n",
            "[308] loss: 0.018\n",
            "[309] loss: 0.018\n",
            "[310] loss: 0.018\n",
            "[311] loss: 0.018\n",
            "[312] loss: 0.018\n",
            "[313] loss: 0.018\n",
            "[314] loss: 0.018\n",
            "[315] loss: 0.018\n",
            "[316] loss: 0.018\n",
            "[317] loss: 0.018\n",
            "[318] loss: 0.018\n",
            "[319] loss: 0.018\n",
            "[320] loss: 0.018\n",
            "[321] loss: 0.018\n",
            "[322] loss: 0.018\n",
            "[323] loss: 0.018\n",
            "[324] loss: 0.018\n",
            "[325] loss: 0.018\n",
            "[326] loss: 0.018\n",
            "[327] loss: 0.018\n",
            "[328] loss: 0.018\n",
            "[329] loss: 0.018\n",
            "[330] loss: 0.018\n",
            "[331] loss: 0.018\n",
            "[332] loss: 0.018\n",
            "[333] loss: 0.018\n",
            "[334] loss: 0.018\n",
            "[335] loss: 0.018\n",
            "[336] loss: 0.018\n",
            "[337] loss: 0.017\n",
            "[338] loss: 0.017\n",
            "[339] loss: 0.017\n",
            "[340] loss: 0.018\n",
            "[341] loss: 0.017\n",
            "[342] loss: 0.017\n",
            "[343] loss: 0.017\n",
            "[344] loss: 0.017\n",
            "[345] loss: 0.017\n",
            "[346] loss: 0.017\n",
            "[347] loss: 0.017\n",
            "[348] loss: 0.017\n",
            "[349] loss: 0.017\n",
            "[350] loss: 0.017\n",
            "[351] loss: 0.017\n",
            "[352] loss: 0.017\n",
            "[353] loss: 0.017\n",
            "[354] loss: 0.017\n",
            "[355] loss: 0.017\n",
            "[356] loss: 0.017\n",
            "[357] loss: 0.017\n",
            "[358] loss: 0.017\n",
            "[359] loss: 0.017\n",
            "[360] loss: 0.017\n",
            "[361] loss: 0.017\n",
            "[362] loss: 0.017\n",
            "[363] loss: 0.017\n",
            "[364] loss: 0.017\n",
            "[365] loss: 0.017\n",
            "[366] loss: 0.017\n",
            "[367] loss: 0.017\n",
            "[368] loss: 0.017\n",
            "[369] loss: 0.017\n",
            "[370] loss: 0.017\n",
            "[371] loss: 0.017\n",
            "[372] loss: 0.016\n",
            "[373] loss: 0.017\n",
            "[374] loss: 0.017\n",
            "[375] loss: 0.016\n",
            "[376] loss: 0.017\n",
            "[377] loss: 0.017\n",
            "[378] loss: 0.016\n",
            "[379] loss: 0.016\n",
            "[380] loss: 0.016\n",
            "[381] loss: 0.016\n",
            "[382] loss: 0.016\n",
            "[383] loss: 0.016\n",
            "[384] loss: 0.016\n",
            "[385] loss: 0.016\n",
            "[386] loss: 0.016\n",
            "[387] loss: 0.016\n",
            "[388] loss: 0.016\n",
            "[389] loss: 0.016\n",
            "[390] loss: 0.016\n",
            "[391] loss: 0.016\n",
            "[392] loss: 0.016\n",
            "[393] loss: 0.016\n",
            "[394] loss: 0.016\n",
            "[395] loss: 0.016\n",
            "[396] loss: 0.016\n",
            "[397] loss: 0.016\n",
            "[398] loss: 0.016\n",
            "[399] loss: 0.016\n",
            "[400] loss: 0.016\n",
            "[401] loss: 0.016\n",
            "[402] loss: 0.016\n",
            "[403] loss: 0.016\n",
            "[404] loss: 0.016\n",
            "[405] loss: 0.016\n",
            "[406] loss: 0.016\n",
            "[407] loss: 0.016\n",
            "[408] loss: 0.016\n",
            "[409] loss: 0.016\n",
            "[410] loss: 0.016\n",
            "[411] loss: 0.016\n",
            "[412] loss: 0.016\n",
            "[413] loss: 0.016\n",
            "[414] loss: 0.016\n",
            "[415] loss: 0.016\n",
            "[416] loss: 0.016\n",
            "[417] loss: 0.016\n",
            "[418] loss: 0.016\n",
            "[419] loss: 0.016\n",
            "[420] loss: 0.016\n",
            "[421] loss: 0.016\n",
            "[422] loss: 0.015\n",
            "[423] loss: 0.016\n",
            "[424] loss: 0.016\n",
            "[425] loss: 0.016\n",
            "[426] loss: 0.016\n",
            "[427] loss: 0.015\n",
            "[428] loss: 0.015\n",
            "[429] loss: 0.015\n",
            "[430] loss: 0.015\n",
            "[431] loss: 0.015\n",
            "[432] loss: 0.015\n",
            "[433] loss: 0.015\n",
            "[434] loss: 0.015\n",
            "[435] loss: 0.015\n",
            "[436] loss: 0.015\n",
            "[437] loss: 0.015\n",
            "[438] loss: 0.015\n",
            "[439] loss: 0.015\n",
            "[440] loss: 0.015\n",
            "[441] loss: 0.015\n",
            "[442] loss: 0.015\n",
            "[443] loss: 0.015\n",
            "[444] loss: 0.015\n",
            "[445] loss: 0.015\n",
            "[446] loss: 0.015\n",
            "[447] loss: 0.015\n",
            "[448] loss: 0.015\n",
            "[449] loss: 0.015\n",
            "[450] loss: 0.015\n",
            "[451] loss: 0.015\n",
            "[452] loss: 0.015\n",
            "[453] loss: 0.015\n",
            "[454] loss: 0.015\n",
            "[455] loss: 0.015\n",
            "[456] loss: 0.015\n",
            "[457] loss: 0.015\n",
            "[458] loss: 0.015\n",
            "[459] loss: 0.015\n",
            "[460] loss: 0.015\n",
            "[461] loss: 0.015\n",
            "[462] loss: 0.015\n",
            "[463] loss: 0.015\n",
            "[464] loss: 0.015\n",
            "[465] loss: 0.015\n",
            "[466] loss: 0.015\n",
            "[467] loss: 0.015\n",
            "[468] loss: 0.015\n",
            "[469] loss: 0.015\n",
            "[470] loss: 0.015\n",
            "[471] loss: 0.015\n",
            "[472] loss: 0.015\n",
            "[473] loss: 0.015\n",
            "[474] loss: 0.015\n",
            "[475] loss: 0.015\n",
            "[476] loss: 0.015\n",
            "[477] loss: 0.015\n",
            "[478] loss: 0.015\n",
            "[479] loss: 0.015\n",
            "[480] loss: 0.015\n",
            "[481] loss: 0.014\n",
            "[482] loss: 0.015\n",
            "[483] loss: 0.015\n",
            "[484] loss: 0.014\n",
            "[485] loss: 0.014\n",
            "[486] loss: 0.014\n",
            "[487] loss: 0.014\n",
            "[488] loss: 0.014\n",
            "[489] loss: 0.015\n",
            "[490] loss: 0.014\n",
            "[491] loss: 0.014\n",
            "[492] loss: 0.014\n",
            "[493] loss: 0.014\n",
            "[494] loss: 0.014\n",
            "[495] loss: 0.014\n",
            "[496] loss: 0.014\n",
            "[497] loss: 0.014\n",
            "[498] loss: 0.014\n",
            "[499] loss: 0.014\n",
            "[500] loss: 0.014\n",
            "[501] loss: 0.014\n",
            "[502] loss: 0.014\n",
            "[503] loss: 0.014\n",
            "[504] loss: 0.014\n",
            "[505] loss: 0.014\n",
            "[506] loss: 0.014\n",
            "[507] loss: 0.014\n",
            "[508] loss: 0.014\n",
            "[509] loss: 0.014\n",
            "[510] loss: 0.014\n",
            "[511] loss: 0.014\n",
            "[512] loss: 0.014\n",
            "[513] loss: 0.014\n",
            "[514] loss: 0.014\n",
            "[515] loss: 0.014\n",
            "[516] loss: 0.014\n",
            "[517] loss: 0.014\n",
            "[518] loss: 0.014\n",
            "[519] loss: 0.014\n",
            "[520] loss: 0.014\n",
            "[521] loss: 0.014\n",
            "[522] loss: 0.014\n",
            "[523] loss: 0.014\n",
            "[524] loss: 0.014\n",
            "[525] loss: 0.014\n",
            "[526] loss: 0.014\n",
            "[527] loss: 0.014\n",
            "[528] loss: 0.014\n",
            "[529] loss: 0.014\n",
            "[530] loss: 0.014\n",
            "[531] loss: 0.014\n",
            "[532] loss: 0.014\n",
            "[533] loss: 0.014\n",
            "[534] loss: 0.014\n",
            "[535] loss: 0.014\n",
            "[536] loss: 0.014\n",
            "[537] loss: 0.014\n",
            "[538] loss: 0.014\n",
            "[539] loss: 0.014\n",
            "[540] loss: 0.014\n",
            "[541] loss: 0.014\n",
            "[542] loss: 0.014\n",
            "[543] loss: 0.014\n",
            "[544] loss: 0.014\n",
            "[545] loss: 0.014\n",
            "[546] loss: 0.014\n",
            "[547] loss: 0.014\n",
            "[548] loss: 0.014\n",
            "[549] loss: 0.014\n",
            "[550] loss: 0.014\n",
            "[551] loss: 0.013\n",
            "[552] loss: 0.013\n",
            "[553] loss: 0.014\n",
            "[554] loss: 0.013\n",
            "[555] loss: 0.014\n",
            "[556] loss: 0.013\n",
            "[557] loss: 0.013\n",
            "[558] loss: 0.014\n",
            "[559] loss: 0.014\n",
            "[560] loss: 0.013\n",
            "[561] loss: 0.014\n",
            "[562] loss: 0.013\n",
            "[563] loss: 0.013\n",
            "[564] loss: 0.013\n",
            "[565] loss: 0.013\n",
            "[566] loss: 0.013\n",
            "[567] loss: 0.013\n",
            "[568] loss: 0.013\n",
            "[569] loss: 0.013\n",
            "[570] loss: 0.013\n",
            "[571] loss: 0.013\n",
            "[572] loss: 0.013\n",
            "[573] loss: 0.013\n",
            "[574] loss: 0.013\n",
            "[575] loss: 0.013\n",
            "[576] loss: 0.013\n",
            "[577] loss: 0.013\n",
            "[578] loss: 0.013\n",
            "[579] loss: 0.013\n",
            "[580] loss: 0.013\n",
            "[581] loss: 0.013\n",
            "[582] loss: 0.013\n",
            "[583] loss: 0.013\n",
            "[584] loss: 0.013\n",
            "[585] loss: 0.013\n",
            "[586] loss: 0.013\n",
            "[587] loss: 0.013\n",
            "[588] loss: 0.013\n",
            "[589] loss: 0.013\n",
            "[590] loss: 0.013\n",
            "[591] loss: 0.013\n",
            "[592] loss: 0.013\n",
            "[593] loss: 0.013\n",
            "[594] loss: 0.013\n",
            "[595] loss: 0.013\n",
            "[596] loss: 0.013\n",
            "[597] loss: 0.013\n",
            "[598] loss: 0.013\n",
            "[599] loss: 0.013\n",
            "[600] loss: 0.013\n",
            "[601] loss: 0.013\n",
            "[602] loss: 0.013\n",
            "[603] loss: 0.013\n",
            "[604] loss: 0.013\n",
            "[605] loss: 0.013\n",
            "[606] loss: 0.013\n",
            "[607] loss: 0.013\n",
            "[608] loss: 0.013\n",
            "[609] loss: 0.013\n",
            "[610] loss: 0.013\n",
            "[611] loss: 0.013\n",
            "[612] loss: 0.013\n",
            "[613] loss: 0.013\n",
            "[614] loss: 0.013\n",
            "[615] loss: 0.013\n",
            "[616] loss: 0.013\n",
            "[617] loss: 0.013\n",
            "[618] loss: 0.013\n",
            "[619] loss: 0.013\n",
            "[620] loss: 0.013\n",
            "[621] loss: 0.013\n",
            "[622] loss: 0.013\n",
            "[623] loss: 0.013\n",
            "[624] loss: 0.013\n",
            "[625] loss: 0.013\n",
            "[626] loss: 0.013\n",
            "[627] loss: 0.013\n",
            "[628] loss: 0.013\n",
            "[629] loss: 0.013\n",
            "[630] loss: 0.013\n",
            "[631] loss: 0.013\n",
            "[632] loss: 0.013\n",
            "[633] loss: 0.013\n",
            "[634] loss: 0.013\n",
            "[635] loss: 0.013\n",
            "[636] loss: 0.013\n",
            "[637] loss: 0.013\n",
            "[638] loss: 0.013\n",
            "[639] loss: 0.013\n",
            "[640] loss: 0.013\n",
            "[641] loss: 0.013\n",
            "[642] loss: 0.013\n",
            "[643] loss: 0.013\n",
            "[644] loss: 0.013\n",
            "[645] loss: 0.012\n",
            "[646] loss: 0.013\n",
            "[647] loss: 0.012\n",
            "[648] loss: 0.012\n",
            "[649] loss: 0.012\n",
            "[650] loss: 0.012\n",
            "[651] loss: 0.013\n",
            "[652] loss: 0.013\n",
            "[653] loss: 0.012\n",
            "[654] loss: 0.012\n",
            "[655] loss: 0.012\n",
            "[656] loss: 0.012\n",
            "[657] loss: 0.012\n",
            "[658] loss: 0.012\n",
            "[659] loss: 0.013\n",
            "[660] loss: 0.012\n",
            "[661] loss: 0.012\n",
            "[662] loss: 0.012\n",
            "[663] loss: 0.012\n",
            "[664] loss: 0.012\n",
            "[665] loss: 0.012\n",
            "[666] loss: 0.012\n",
            "[667] loss: 0.012\n",
            "[668] loss: 0.012\n",
            "[669] loss: 0.012\n",
            "[670] loss: 0.012\n",
            "[671] loss: 0.012\n",
            "[672] loss: 0.012\n",
            "[673] loss: 0.012\n",
            "[674] loss: 0.012\n",
            "[675] loss: 0.012\n",
            "[676] loss: 0.012\n",
            "[677] loss: 0.012\n",
            "[678] loss: 0.012\n",
            "[679] loss: 0.012\n",
            "[680] loss: 0.012\n",
            "[681] loss: 0.012\n",
            "[682] loss: 0.012\n",
            "[683] loss: 0.012\n",
            "[684] loss: 0.012\n",
            "[685] loss: 0.012\n",
            "[686] loss: 0.012\n",
            "[687] loss: 0.012\n",
            "[688] loss: 0.012\n",
            "[689] loss: 0.012\n",
            "[690] loss: 0.012\n",
            "[691] loss: 0.012\n",
            "[692] loss: 0.012\n",
            "[693] loss: 0.012\n",
            "[694] loss: 0.012\n",
            "[695] loss: 0.012\n",
            "[696] loss: 0.012\n",
            "[697] loss: 0.012\n",
            "[698] loss: 0.012\n",
            "[699] loss: 0.012\n",
            "[700] loss: 0.012\n",
            "[701] loss: 0.012\n",
            "[702] loss: 0.012\n",
            "[703] loss: 0.012\n",
            "[704] loss: 0.012\n",
            "[705] loss: 0.012\n",
            "[706] loss: 0.012\n",
            "[707] loss: 0.012\n",
            "[708] loss: 0.012\n",
            "[709] loss: 0.012\n",
            "[710] loss: 0.012\n",
            "[711] loss: 0.012\n",
            "[712] loss: 0.012\n",
            "[713] loss: 0.012\n",
            "[714] loss: 0.012\n",
            "[715] loss: 0.012\n",
            "[716] loss: 0.012\n",
            "[717] loss: 0.012\n",
            "[718] loss: 0.012\n",
            "[719] loss: 0.012\n",
            "[720] loss: 0.012\n",
            "[721] loss: 0.012\n",
            "[722] loss: 0.012\n",
            "[723] loss: 0.012\n",
            "[724] loss: 0.012\n",
            "[725] loss: 0.012\n",
            "[726] loss: 0.012\n",
            "[727] loss: 0.012\n",
            "[728] loss: 0.012\n",
            "[729] loss: 0.012\n",
            "[730] loss: 0.012\n",
            "[731] loss: 0.012\n",
            "[732] loss: 0.012\n",
            "[733] loss: 0.012\n",
            "[734] loss: 0.012\n",
            "[735] loss: 0.012\n",
            "[736] loss: 0.012\n",
            "[737] loss: 0.012\n",
            "[738] loss: 0.012\n",
            "[739] loss: 0.012\n",
            "[740] loss: 0.012\n",
            "[741] loss: 0.012\n",
            "[742] loss: 0.012\n",
            "[743] loss: 0.012\n",
            "[744] loss: 0.012\n",
            "[745] loss: 0.012\n",
            "[746] loss: 0.012\n",
            "[747] loss: 0.012\n",
            "[748] loss: 0.012\n",
            "[749] loss: 0.012\n",
            "[750] loss: 0.012\n",
            "[751] loss: 0.012\n",
            "[752] loss: 0.012\n",
            "[753] loss: 0.012\n",
            "[754] loss: 0.012\n",
            "[755] loss: 0.012\n",
            "[756] loss: 0.012\n",
            "[757] loss: 0.012\n",
            "[758] loss: 0.012\n",
            "[759] loss: 0.012\n",
            "[760] loss: 0.011\n",
            "[761] loss: 0.011\n",
            "[762] loss: 0.012\n",
            "[763] loss: 0.012\n",
            "[764] loss: 0.011\n",
            "[765] loss: 0.012\n",
            "[766] loss: 0.012\n",
            "[767] loss: 0.012\n",
            "[768] loss: 0.012\n",
            "[769] loss: 0.012\n",
            "[770] loss: 0.011\n",
            "[771] loss: 0.012\n",
            "[772] loss: 0.011\n",
            "[773] loss: 0.011\n",
            "[774] loss: 0.011\n",
            "[775] loss: 0.011\n",
            "[776] loss: 0.011\n",
            "[777] loss: 0.011\n",
            "[778] loss: 0.011\n",
            "[779] loss: 0.011\n",
            "[780] loss: 0.011\n",
            "[781] loss: 0.011\n",
            "[782] loss: 0.011\n",
            "[783] loss: 0.011\n",
            "[784] loss: 0.012\n",
            "[785] loss: 0.012\n",
            "[786] loss: 0.011\n",
            "[787] loss: 0.011\n",
            "[788] loss: 0.011\n",
            "[789] loss: 0.011\n",
            "[790] loss: 0.011\n",
            "[791] loss: 0.011\n",
            "[792] loss: 0.011\n",
            "[793] loss: 0.011\n",
            "[794] loss: 0.011\n",
            "[795] loss: 0.011\n",
            "[796] loss: 0.011\n",
            "[797] loss: 0.011\n",
            "[798] loss: 0.011\n",
            "[799] loss: 0.011\n",
            "[800] loss: 0.011\n",
            "[801] loss: 0.011\n",
            "[802] loss: 0.011\n",
            "[803] loss: 0.011\n",
            "[804] loss: 0.011\n",
            "[805] loss: 0.011\n",
            "[806] loss: 0.011\n",
            "[807] loss: 0.011\n",
            "[808] loss: 0.011\n",
            "[809] loss: 0.011\n",
            "[810] loss: 0.011\n",
            "[811] loss: 0.011\n",
            "[812] loss: 0.011\n",
            "[813] loss: 0.011\n",
            "[814] loss: 0.011\n",
            "[815] loss: 0.011\n",
            "[816] loss: 0.011\n",
            "[817] loss: 0.011\n",
            "[818] loss: 0.011\n",
            "[819] loss: 0.011\n",
            "[820] loss: 0.011\n",
            "[821] loss: 0.011\n",
            "[822] loss: 0.011\n",
            "[823] loss: 0.011\n",
            "[824] loss: 0.011\n",
            "[825] loss: 0.011\n",
            "[826] loss: 0.011\n",
            "[827] loss: 0.011\n",
            "[828] loss: 0.011\n",
            "[829] loss: 0.011\n",
            "[830] loss: 0.011\n",
            "[831] loss: 0.011\n",
            "[832] loss: 0.011\n",
            "[833] loss: 0.011\n",
            "[834] loss: 0.011\n",
            "[835] loss: 0.011\n",
            "[836] loss: 0.011\n",
            "[837] loss: 0.011\n",
            "[838] loss: 0.011\n",
            "[839] loss: 0.011\n",
            "[840] loss: 0.011\n",
            "[841] loss: 0.011\n",
            "[842] loss: 0.011\n",
            "[843] loss: 0.011\n",
            "[844] loss: 0.011\n",
            "[845] loss: 0.011\n",
            "[846] loss: 0.011\n",
            "[847] loss: 0.011\n",
            "[848] loss: 0.011\n",
            "[849] loss: 0.011\n",
            "[850] loss: 0.011\n",
            "[851] loss: 0.011\n",
            "[852] loss: 0.011\n",
            "[853] loss: 0.011\n",
            "[854] loss: 0.011\n",
            "[855] loss: 0.011\n",
            "[856] loss: 0.011\n",
            "[857] loss: 0.011\n",
            "[858] loss: 0.011\n",
            "[859] loss: 0.011\n",
            "[860] loss: 0.011\n",
            "[861] loss: 0.011\n",
            "[862] loss: 0.011\n",
            "[863] loss: 0.011\n",
            "[864] loss: 0.011\n",
            "[865] loss: 0.011\n",
            "[866] loss: 0.011\n",
            "[867] loss: 0.011\n",
            "[868] loss: 0.011\n",
            "[869] loss: 0.011\n",
            "[870] loss: 0.011\n",
            "[871] loss: 0.011\n",
            "[872] loss: 0.011\n",
            "[873] loss: 0.011\n",
            "[874] loss: 0.011\n",
            "[875] loss: 0.011\n",
            "[876] loss: 0.011\n",
            "[877] loss: 0.011\n",
            "[878] loss: 0.011\n",
            "[879] loss: 0.011\n",
            "[880] loss: 0.011\n",
            "[881] loss: 0.011\n",
            "[882] loss: 0.011\n",
            "[883] loss: 0.011\n",
            "[884] loss: 0.011\n",
            "[885] loss: 0.011\n",
            "[886] loss: 0.011\n",
            "[887] loss: 0.011\n",
            "[888] loss: 0.011\n",
            "[889] loss: 0.011\n",
            "[890] loss: 0.011\n",
            "[891] loss: 0.011\n",
            "[892] loss: 0.011\n",
            "[893] loss: 0.011\n",
            "[894] loss: 0.011\n",
            "[895] loss: 0.011\n",
            "[896] loss: 0.011\n",
            "[897] loss: 0.011\n",
            "[898] loss: 0.011\n",
            "[899] loss: 0.011\n",
            "[900] loss: 0.011\n",
            "[901] loss: 0.011\n",
            "[902] loss: 0.011\n",
            "[903] loss: 0.011\n",
            "[904] loss: 0.011\n",
            "[905] loss: 0.011\n",
            "[906] loss: 0.011\n",
            "[907] loss: 0.011\n",
            "[908] loss: 0.011\n",
            "[909] loss: 0.011\n",
            "[910] loss: 0.011\n",
            "[911] loss: 0.011\n",
            "[912] loss: 0.011\n",
            "[913] loss: 0.011\n",
            "[914] loss: 0.011\n",
            "[915] loss: 0.011\n",
            "[916] loss: 0.011\n",
            "[917] loss: 0.011\n",
            "[918] loss: 0.011\n",
            "[919] loss: 0.011\n",
            "[920] loss: 0.011\n",
            "[921] loss: 0.011\n",
            "[922] loss: 0.011\n",
            "[923] loss: 0.011\n",
            "[924] loss: 0.011\n",
            "[925] loss: 0.011\n",
            "[926] loss: 0.010\n",
            "[927] loss: 0.011\n",
            "[928] loss: 0.010\n",
            "[929] loss: 0.011\n",
            "[930] loss: 0.011\n",
            "[931] loss: 0.011\n",
            "[932] loss: 0.011\n",
            "[933] loss: 0.011\n",
            "[934] loss: 0.011\n",
            "[935] loss: 0.010\n",
            "[936] loss: 0.010\n",
            "[937] loss: 0.011\n",
            "[938] loss: 0.011\n",
            "[939] loss: 0.010\n",
            "[940] loss: 0.010\n",
            "[941] loss: 0.011\n",
            "[942] loss: 0.010\n",
            "[943] loss: 0.010\n",
            "[944] loss: 0.010\n",
            "[945] loss: 0.010\n",
            "[946] loss: 0.010\n",
            "[947] loss: 0.010\n",
            "[948] loss: 0.010\n",
            "[949] loss: 0.010\n",
            "[950] loss: 0.010\n",
            "[951] loss: 0.010\n",
            "[952] loss: 0.010\n",
            "[953] loss: 0.010\n",
            "[954] loss: 0.010\n",
            "[955] loss: 0.010\n",
            "[956] loss: 0.010\n",
            "[957] loss: 0.010\n",
            "[958] loss: 0.010\n",
            "[959] loss: 0.010\n",
            "[960] loss: 0.010\n",
            "[961] loss: 0.010\n",
            "[962] loss: 0.010\n",
            "[963] loss: 0.010\n",
            "[964] loss: 0.010\n",
            "[965] loss: 0.010\n",
            "[966] loss: 0.010\n",
            "[967] loss: 0.010\n",
            "[968] loss: 0.010\n",
            "[969] loss: 0.010\n",
            "[970] loss: 0.010\n",
            "[971] loss: 0.010\n",
            "[972] loss: 0.010\n",
            "[973] loss: 0.010\n",
            "[974] loss: 0.010\n",
            "[975] loss: 0.010\n",
            "[976] loss: 0.010\n",
            "[977] loss: 0.010\n",
            "[978] loss: 0.010\n",
            "[979] loss: 0.010\n",
            "[980] loss: 0.010\n",
            "[981] loss: 0.010\n",
            "[982] loss: 0.010\n",
            "[983] loss: 0.010\n",
            "[984] loss: 0.010\n",
            "[985] loss: 0.010\n",
            "[986] loss: 0.010\n",
            "[987] loss: 0.010\n",
            "[988] loss: 0.010\n",
            "[989] loss: 0.010\n",
            "[990] loss: 0.010\n",
            "[991] loss: 0.010\n",
            "[992] loss: 0.010\n",
            "[993] loss: 0.010\n",
            "[994] loss: 0.010\n",
            "[995] loss: 0.010\n",
            "[996] loss: 0.010\n",
            "[997] loss: 0.010\n",
            "[998] loss: 0.010\n",
            "[999] loss: 0.010\n",
            "[1000] loss: 0.010\n",
            "Finished Training\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8FmA6zWq2wW",
        "colab_type": "text"
      },
      "source": [
        "Running student model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1fxqFeILgTp",
        "colab_type": "code",
        "outputId": "633675f9-1d76-4260-c71b-b650455bb7f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "stud_net = Model(hidden_size = 800).to(device)\n",
        "stud_net.load_state_dict(torch.load(STUD_PATH))\n",
        "\n",
        "# Run model on test set and determine accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "wrong = np.zeros((10,10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = convert_labels(labels).to(device)\n",
        "        outputs = stud_net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, target = torch.max(target.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        for i, val in enumerate(predicted):\n",
        "          if val != target[i]:\n",
        "            wrong[target[i]][val] += 1\n",
        "\n",
        "# Output model accuracy to user\n",
        "print('Accuracy of the network on the 10000 test images: %d %% (%d wrong out of %d)' % (\n",
        "    100 * correct / total, total - correct, total))\n",
        "#print(\" \"+str([0,1,2,3,4,5,6,7,8,9]))\n",
        "#print(wrong)\n",
        "\n",
        "# Plot confusion matrix\n",
        "df_cm = pd.DataFrame(wrong, index = [i for i in \"0123456789\"],\n",
        "                  columns = [i for i in \"0123456789\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 99 % (67 wrong out of 10000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa1117ac18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAGfCAYAAAAZLHvQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfXhV5Znv8d+9QwCxCuIbJOEUpjiKHYso6Di0FN+gVkGnzkEd7ZnjNWfoTJ0OtlOkttSeqdpqp2q1U+uFtUhfRNBWBQGLlQrSUykIFCEwDghqEqFypMW3Ykju8wdbTmrJfpG19tp38v30Wlezk72yfj7XSnLz3M9ay9xdAAAAachlHQAAAHRdFBoAACA1FBoAACA1FBoAACA1FBoAACA1FBoAACA1FBoAAKAkZna8ma3tsO02s6sL7sN9NAAAQLnMrEZSs6TT3f2Fzt7HjAYAAHgvzpa0pVCRIUk90k7Ro2c9UyYASnLKUUOzjtDlrd65OesI3cLet5utksdr3fl8Yn9rex79gU9JmtzhUzPcfcYB3nqppNnFvl/qhQYAAIgjX1QcqLDYz8x6Spoo6dpi349CAwCA6NrbKn3E8yStdvcdxd7IGg0AAFCuy1RC20RiRgMAgPi8vWKHMrNDJZ0r6VOlvJ9CAwCA6NorV2i4+xuSjiz1/bROAABAapjRAAAgOK9g66RcFBoAAERXwdZJuWidAACA1DCjAQBAdLROAABAaip/w66S0ToBAACpYUYDAIDoaJ0AAIDUcNVJssaPG6sN65dpU+NyXTP1qqzjlCRa5mh5JTJXQrS802+dpsfWPazZS2ZmHaVkETNHOy+kmJmjCldo5HI53XH7jbpgwhU6afiZuuSSizRs2HFZxyooWuZoeSUyV0K0vJK0YM4iTbl8atYxyhItc8TzImLmYtzbE9uSVrTQMLMTzGyamd2R36aZ2bDEk5TotFEjtGXLNm3d+qJaW1s1d+4jmjhhfFZxShItc7S8EpkrIVpeSVqzYp1273ot6xhliZY54nkRMXNR7e3JbQkrWGiY2TRJ90sySb/ObyZptpl9IfE0JairH6CXmlr2v25qfll1dQOyiFKyaJmj5ZXIXAnR8qIyIp4XETNHVmwx6N9L+qC7t3b8pJndKmmDpJsOtJOZTZY0WZKspq9yuUMTiAoAAA4o8FUn7ZLqJL3wrs8PzH/tgNx9hqQZktSjZ70fTMB3a2nerkENdftfN9QPVEvL9iQPkbhomaPllchcCdHyojIinhcRMxcV+IZdV0t6wswWmdmM/PaYpCckTUk/3p9auWqthg4dosGDB6m2tlaTJl2o+Y8uziJKyaJljpZXInMlRMuLyoh4XkTMHFnBGQ13f8zM/lzSaZLq859ulrTS3TMpn9ra2jTl6ulauOA+1eRyunfWHDU2PpdFlJJFyxwtr0TmSoiWV5Kuv/M6nXrGyerXv6/mr3pAd98yU/NmL8w6VkHRMkc8LyJmLqqKWyfmnmhn408k3ToB0HWdctTQrCN0eat3bs46Qrew9+1mq+Tx9mx4IrG/tb0+eHai2cPdRwMAAMTBLcgBAIiuilsnFBoAAETHs04AAEB3xIwGAADBZXQhaEkoNAAAiK6K12jQOgEAAKlhRgMAgOiqeDEohQYAANFVceuEQgMAgOgCP1QNAADgPWNGAwCA6GidAACA1FTxYlBaJwAAIDXMaHQBER+tzaOqcSDRzouIP3vRMkc7JzJD6wQAAKSG1gkAAOiOmNEAACC6Kp7RoNAAACC4an56K60TAACQGmY0AACIjtYJAABITRVf3krrBAAApIYZDQAAoqN1AgAAUkPrBAAAdEfMaAAAEB2tEwAAkBpaJwAAoCsws35m9qCZbTKzjWZ2RqH3hyw0xo8bqw3rl2lT43JdM/WqrOOUJFrm6bdO02PrHtbsJTOzjlKyaGMsxcscLa8UL3PEn72ImaOdF0W1tye3FXe7pMfc/QRJwyVtLPTmcIVGLpfTHbffqAsmXKGThp+pSy65SMOGHZd1rIIiZl4wZ5GmXD416xglizjG0TJHyyvFzBztZ0+KlznieVFUhQoNM+sraYykeyTJ3d92998V2idcoXHaqBHasmWbtm59Ua2trZo79xFNnDA+61gFRcy8ZsU67d71WtYxShZxjKNljpZXipk52s+eFC9zxPOiksxsspmt6rBN7vDlIZJekTTTzNaY2ffM7NBC3y9coVFXP0AvNbXsf93U/LLq6gZkmKi4iJmjiTjG0TJHyyvFzIz0dcnzwtsT29x9hruP7LDN6HCkHpJOkfRddx8h6Q1JXygU7T0XGmZ2ZYGv7a+G2tvfeK+HAAAApajcGo0mSU3uviL/+kHtKzw6dTAzGv/W2Rc6VkO5XMEZlbK1NG/XoIa6/a8b6geqpWV7osdIWsTM0UQc42iZo+WVYmZG+jgv3jt33y7pJTM7Pv+psyU1FtqnYKFhZus62Z6VdGwyscuzctVaDR06RIMHD1Jtba0mTbpQ8x9dnEWUkkXMHE3EMY6WOVpeKWZmpK9LnhcJtk5K8BlJPzazdZJOlvS1Qm8udsOuYyWNl7TrXZ83Sf+nlDRJa2tr05Srp2vhgvtUk8vp3llz1Nj4XBZRShYx8/V3XqdTzzhZ/fr31fxVD+juW2Zq3uyFWcfqVMQxjpY5Wl4pZuZoP3tSvMwRz4uiKnhnUHdfK2lkqe83d+/8i2b3SJrp7ssP8LX73P1vix2gR8/6zg+ARJxy1NCsI5Rt9c7NWUcADlrEn71oov6u2Pt2s1XyeG89dFNif2sP+esvJJq94IyGu/99ga8VLTIAAEAFVPEtyHnWCQAA0VXxQ9XC3UcDAADEwYwGAADRVfGMBoUGAADRFbiwI2u0TgAAQGqY0QAAIDpaJwAAIDVVXGjQOgEAAKlhRgMAgOi4YRcAAEgNrRMAANAdMaMBAEB0VXwfDQoNAACiq+LWCYVGFxD1McqRRHwcOOdF+r7QXp91hLJNenVp1hHQzVBoAAAQHTMaAAAgNVV8eStXnQAAgNQwowEAQHDezlUnAAAgLVW8RoPWCQAASA0zGgAARFfFi0EpNAAAiK6K12jQOgEAAKlhRgMAgOiqeDEohQYAANFRaAAAgNRU8dNbWaMBAABSw4wGAADRVXHrJOSMxvhxY7Vh/TJtalyua6ZelXWckkTLHC2vFC/z9Fun6bF1D2v2kplZRylZtDGW4mXO9arV2EXX66wnvq5zln5Dw6ZenHWkoqKNsRQzc0HtntyWsHCFRi6X0x2336gLJlyhk4afqUsuuUjDhh2XdayComWOlleKmXnBnEWacvnUrGOULOIYR8zcvqdVT118g5acfa2eOPtaHXvmcB1xytCsY3Uq4hhHzBxZuELjtFEjtGXLNm3d+qJaW1s1d+4jmjhhfNaxCoqWOVpeKWbmNSvWafeu17KOUbKIYxwxsyS1vblHkpSrrVGuR01VL/SLOMYRMxfl7cltCStaaJjZCWZ2tpm9712f/1jiaUpQVz9ALzW17H/d1Pyy6uoGZBGlZNEyR8srxcwcTcQxjphZkpQznfXzr+n89Xdpx7JntWvNlqwTdSriGEfMXFTU1omZ/YukRyR9RtJ6M7uww5e/VmC/yWa2ysxWtbe/kUxSAOgu2l1LzvmiFo34Z/Uf8QEdfkJD1omA96zYVSf/IOlUd3/dzAZLetDMBrv77ZKss53cfYakGZLUo2d9ouVRS/N2DWqo2/+6oX6gWlq2J3mIxEXLHC2vFDNzNBHHOGLmjlp3v6lXftmoY88crt2bmrKOc0ARxzhi5mI88FUnOXd/XZLcfZuksZLOM7NbVaDQSNPKVWs1dOgQDR48SLW1tZo06ULNf3RxFlFKFi1ztLxSzMzRRBzjiJl7HnmYag/vI0nK9a7VMWNO0mubW4rslZ2IYxwxc1FV3DopNqOxw8xOdve1kpSf2bhA0vclnZR4mhK0tbVpytXTtXDBfarJ5XTvrDlqbHwuiygli5Y5Wl4pZubr77xOp55xsvr176v5qx7Q3bfM1LzZC7OO1amIYxwxc+9j+mnkHf8kq8lJOVPzvKe1/fE1WcfqVMQxjpg5MvMCq5nNrEHSXnf/kzklMxvt7r8sdoCkWydAFk45qnovL+zM6p2bs47Q5c3t/9GsI5Rt0qtLs47QLex9u7mis/5v3HBFYn9rD53+o0SzF5zRcPdOm4KlFBkAAKACUmh5JCXcfTQAAEAcPOsEAIDoqviqEwoNAACio3UCAAC6I2Y0AACILoVnlCSFQgMAgOgq2Doxs22SXpPUpn23wBhZ6P0UGgAAoFxnuvvOUt5IoQEAQHCRn3UCAACqXYLPOun4BPb8NvldR3NJi83smQN87U8wowEAAPbr+AT2TnzY3ZvN7BhJj5vZJndf1tmbmdEAACC6Cj691d2b8///W0kPSTqt0PspNAAAiM7bk9sKMLNDzeywdz6WNE7S+kL70DoBAAClOlbSQ2Ym7ash7nP3xwrtQKEBlCDiI9c/U/eRrCOU7dstT2UdoSw35ZqzjlC2iOcFSlCh+2i4+/OShpezD4UGAADBOc86AQAA3REzGgAARFfFMxoUGgAARMedQQEAQHfEjAYAANHROgEAAKmp4kKD1gkAAEgNMxoAAATnXr0zGhQaAABER+sEAAB0R8xoAAAQXRXPaFBoAAAQHM86AQAA3VLIQmP8uLHasH6ZNjUu1zVTr8o6TkmiZY6WVyJz2voNPFKfnv1lTXv8m5q2+N815srzso5UkkhjLEnTb52mx9Y9rNlLZmYdpSQRz4uImYtq9+S2hIVrneRyOd1x+4362McvU1PTy3r6Vws1/9HF2rjxv7KO1qlomaPllchcCe172zTvhh+qacM29Tq0tz43/+v6z6fWacfm5qyjdSraGEvSgjmL9MDMn+p/3/7FrKOUJOJ5ETFzUdX7qJN4MxqnjRqhLVu2aevWF9Xa2qq5cx/RxAnjs45VULTM0fJKZK6E3a/8Tk0btkmS9rzxB+3Y0qy+A/pnG6qIaGMsSWtWrNPuXa9lHaNkEc+LiJkjK1pomNlpZjYq//GJZvY5M/t4+tEOrK5+gF5qatn/uqn5ZdXVDcgqTkmiZY6WVyJzpR3RcLQaThysF9ZuzjpKQZHHOKIo50VHETMfiLd7YlvSCrZOzOwrks6T1MPMHpd0uqRfSPqCmY1w9xs72W+ypMmSZDV9lcsdmmxqAJnp2aeXrvzuZ/XQV2dpz+tvZR0HVSLieRExc6eq+KqTYms0/kbSyZJ6SdouqcHdd5vZNyWtkHTAQsPdZ0iaIUk9etYn+l/f0rxdgxrq9r9uqB+olpbtSR4icdEyR8srkblScj1qdOVdn9MzDy/Xsz9bmXWcoiKOcUTRzgspZuaoirVO9rp7m7u/KWmLu++WJHd/SxktPVm5aq2GDh2iwYMHqba2VpMmXaj5jy7OIkrJomWOllcic6VcevOntGNzs5beszDrKCWJOMYRRTsvpJiZC2pPcEtYsRmNt82sT77QOPWdT5pZ33TiFNfW1qYpV0/XwgX3qSaX072z5qix8bksopQsWuZoeSUyV8KQkcdr1MVj1LLxBX1+4U2SpAXfuF8bn1ybcbLORRtjSbr+zut06hknq1//vpq/6gHdfctMzZtdvX8MI54XETMXU8037LJCT3wzs17uvucAnz9K0kB3f7bYAZJunQAozWfqPpJ1hLJ9u+WprCOU5ZSjhmYdoWyjew7MOkK3cNu2+62Sx9v138cm9rf2iAeeTDR7wRmNAxUZ+c/vlLQzySAAAOA9quL7aIS7YRcAAPhj1dw6CXfDLgAAEAczGgAAREfrBAAApMUpNAAAQGqquNBgjQYAAEgNMxoAAARH6wQAAKSnigsNWicAACA1zGgAABAcrRMAAJCaai40aJ0AAIDUMKMBAEBw1TyjQaEBdFHRHrkuSXP7fzTrCGWZtHNp1hHKtlqbs45QlmjnRGa8ok+lLwutEwAAkBpmNAAACI7WCQAASI230zoBAADdEDMaAAAER+sEAACkxrnqBAAAdEfMaAAAEFylWydmViNplaRmd7+g0HspNAAACC6Dq06mSNoo6fBib6R1AgAASmZmDZLOl/S9Ut5PoQEAQHDuyW1mNtnMVnXYJr/rcN+SdI2kkho2tE4AAAguydaJu8+QNONAXzOzCyT91t2fMbOxpXw/ZjQAAECpRkuaaGbbJN0v6Swz+1GhHSg0AAAIztstsa3gcdyvdfcGdx8s6VJJS9z9ikL7hCw0xo8bqw3rl2lT43JdM/WqrOOUJFrmaHklMldCtLy5XrUau+h6nfXE13XO0m9o2NSLs45UkmjjHC1v1POikCTXaCTNPI3v2kGPnvWJHiCXy2njhqf0sY9fpqaml/X0rxbqik9+Whs3/leSh0lUtMzR8kpkroRK5J3b/6OJfa931PTppbY398h61Oij876i30z/gXat3pzI95706tJEvk9HnBd/LI1zQkr3vJCkT2y/r6LXm24dfm5if2uH/ObxRLOHm9E4bdQIbdmyTVu3vqjW1lbNnfuIJk4Yn3WsgqJljpZXInMlRMv7jrY390iScrU1yvWoSeefbAmKNs7R8r4j2nlRTKVaJ+9F2YWGmf0g8RRlqKsfoJeaWva/bmp+WXV1AzJMVFy0zNHySmSuhGh598uZzvr513T++ru0Y9mz2rVmS9aJCoo2ztHy7hfsvCjG3RLbklbw8lYzm/fuT0k608z67fsP84mJJwKAJLW7lpzzRdUe3kd/OfOzOvyEBu3e1JR1KmSN86Jiit1Ho0FSo/bd/cu1r9AYKemWQjvlb+4xWZKspq9yuUMPPmleS/N2DWqo+/8B6weqpWV7Yt8/DdEyR8srkbkSouV9t9bdb+qVXzbq2DOHV/UflGjjHC3vu0U5L4qp5sfEF2udjJT0jKQvSfq9uz8p6S13X+runa6CcvcZ7j7S3UcmWWRI0spVazV06BANHjxItbW1mjTpQs1/dHGix0hatMzR8kpkroRoeSWp55GHqfbwPpKkXO9aHTPmJL22uaXIXtmKNs7R8koxz4ti2t0S25JWcEbD3dsl3WZmD+T/f0exfdLW1tamKVdP18IF96kml9O9s+aosfG5LCMVFS1ztLwSmSshWl5J6n1MP428459kNTkpZ2qe97S2P74m61gFRRvnaHmlmOdFZGVd3mpm50sa7e5fLHWfpC9vBdB1pXUpY1rSuLwVfyzaOfGOSl/e+p8nnJfY39rjNy1KNHtZsxPuvkDSgiQDAACAg5PBY+JLFu4+GgAAIA6e3goAQHDVfL8xCg0AAIKjdQIAALolZjQAAAgujftfJIVCAwCA4NJ4RklSaJ0AAIDUMKMBAEBwXHUCAABSU81rNGidAACA1DCjAQBAcNW8GJRCAwCA4Kp5jQatEwAAkJrUZzROOWpo2odI1Oqdm7OO0C08/6ETso5Qlj9btynrCN3CTbnmrCOUJdrvt4huUqxz4h2fqPDxqnkxKK0TAACCq+Y1GrROAABAapjRAAAgOFonAAAgNVV80QmFBgAA0VXzjAZrNAAAQGqY0QAAILhqvuqEQgMAgODasw5QAK0TAACQGmY0AAAIzkXrBAAApKS9iq9vpXUCAABSw4wGAADBtdM6AQAAaanmNRrhWifTb52mx9Y9rNlLZmYdpSzjx43VhvXLtKlxua6ZelXWcYqKlne/XE4DfnyXjr7txqyTlCTaOEfLG/H3BZnTFy1vdOEKjQVzFmnK5VOzjlGWXC6nO26/URdMuEInDT9Tl1xykYYNOy7rWJ2Klrejwy77hFq3vph1jJJEG+doeaWYvy/InL5oeUvRnuCWtHCFxpoV67R712tZxyjLaaNGaMuWbdq69UW1trZq7txHNHHC+KxjdSpa3nfUHHOUDhl9ul5/eGHWUUoSbZyj5ZVi/r4gc/qi5S2FyxLbklZWoWFmHzazz5nZuMSTdGF19QP0UlPL/tdNzS+rrm5AhokKi5b3HUf861XadccMyav4Oq8Ooo1ztLwAqkPBQsPMft3h43+Q9B+SDpP0FTP7QsrZgJL1/vBfqu3VXWrd9F9ZRwGAiqvm1kmxq05qO3w8WdK57v6KmX1T0tOSbjrQTmY2Of9+vb/vcTqmz8AksobV0rxdgxrq9r9uqB+olpbtGSYqLFpeSeo1/IM6ZMxf6ZDRp8t69pS9r4+O/Oq1+r/XfT3raJ2KNs7R8gLdSeRnneTM7AgzO1KSufsrkuTub0ja29lO7j7D3Ue6+8juXmRI0spVazV06BANHjxItbW1mjTpQs1/dHHWsToVLa8k/f4796jl/EvVMvFy7fzSDdqzcm1VFxlSvHGOlhdAdShWaPSV9IykVZL6m9lASTKz90nZXLR7/Z3X6Z75d+r9H/hvmr/qAU287ONZxChLW1ubplw9XQsX3Kf1657Ugw/OV2Pjc1nH6lS0vFFFG+doeaWYvy/InL5oeUtRzYtBzd/Dwjkz6yPpWHffWuy9p9V9NMbKvLzVOzdnHaFbeP5DJ2QdoSx/tm5T1hG6hVOOGpp1BCARv25ZWtF/jM8fcFlif2snbJ+daPb3dGdQd39TUtEiAwAAdG/cghwAgOAq9awTM+staZmkXtpXQzzo7l8ptA+FBgAAwVVwjcIeSWe5++tmVitpuZktcvenO9uBQgMAAJTE9y3sfD3/sja/Faxzwt2CHAAA/LEkb9hlZpPNbFWHbXLHY5lZjZmtlfRbSY+7+4pC2ZjRAAAguHZLbo2Gu8+QNKPA19sknWxm/SQ9ZGZ/4e7rO3s/MxoAAKBs7v47Sb+Q9LFC76PQAAAgOE9wK8TMjs7PZMjMDpF0rqSCNxqidQIAQHAVfNbJQEmzzKxG+yYr5rr7o4V2oNAAAAAlcfd1kkaUsw+FBgAAwbVn8vSx0lBoAAAQXKXuDPpesBgUAACkhhkNAACCq+bHpKdeaHyhvT7tQyTqqbqBWUco27dbnso6QtmiPXadx5dXxuqdm7OOAIRUzWs0aJ0AAIDU0DoBACC4Ct5Ho2wUGgAABFfNazRonQAAgNQwowEAQHDVvBiUQgMAgOCqeY0GrRMAAJAaZjQAAAiummc0KDQAAAjOq3iNBq0TAACQGmY0AAAIjtYJAABITTUXGrROAABAapjRAAAguGq+BTmFBgAAwVXznUHDtU5yvWo1dtH1OuuJr+ucpd/QsKkXZx2pqH4Dj9SnZ39Z0x7/pqYt/neNufK8rCMVNX7cWG1Yv0ybGpfrmqlXZR2nJNEyT791mh5b97BmL5mZdZSSRcwc7byQ4mWOlleKmTmqcIVG+55WPXXxDVpy9rV64uxrdeyZw3XEKUOzjlVQ+942zbvhh7r53M/rW3/9ZY3+5DgdO7Q+61idyuVyuuP2G3XBhCt00vAzdcklF2nYsOOyjlVQxMwL5izSlMunZh2jLNEyRzwvomWOlleKmbmY9gS3pBUsNMzsdDM7PP/xIWb2b2Y238xuNrO+KeQpSdubeyRJudoa5XrUSF7N3Slp9yu/U9OGbZKkPW/8QTu2NKvvgP7ZhirgtFEjtGXLNm3d+qJaW1s1d+4jmjhhfNaxCoqYec2Kddq967WsY5QlWuaI50W0zNHySjEzFxO20JD0fUlv5j++XVJfSTfnP5fd3GnOdNbPv6bz19+lHcue1a41WzKLUq4jGo5Ww4mD9cLazVlH6VRd/QC91NSy/3VT88uqqxuQYaLiImZG+iKeF9EyR8srxcwcWbFCI+fue/Mfj3T3q919ubv/m6Q/62wnM5tsZqvMbNXiN1P4g9ruWnLOF7VoxD+r/4gP6PATGpI/Rgp69umlK7/7WT301Vna8/pbWccBAHQRnuCWtGKFxnozuzL/8W/MbKQkmdmfS2rtbCd3n+HuI9195Lg+6a2faN39pl75ZaOOPXN4asdISq5Hja6863N65uHlevZnK7OOU1BL83YNaqjb/7qhfqBaWrZnmKi4iJmRvojnRbTM0fJKMTMX027JbUkrVmj8L0kfNbMtkk6U9Csze17S3fmvVVzPIw9T7eF9JEm53rU6ZsxJem1zS5G9snfpzZ/Sjs3NWnrPwqyjFLVy1VoNHTpEgwcPUm1trSZNulDzH12cdayCImZG+iKeF9EyR8srxcxcTDWv0Sh4Hw13/72k/5lfEDok//4md9+RQpaS9D6mn0be8U+ympyUMzXPe1rbH1+TVZySDBl5vEZdPEYtG1/Q5xfeJEla8I37tfHJtRknO7C2tjZNuXq6Fi64TzW5nO6dNUeNjc9lHaugiJmvv/M6nXrGyerXv6/mr3pAd98yU/NmV3chGi1zxPMiWuZoeaWYmSMzT/mKjZ8O+NvqviTkXZ7qXc13jD+wb7c8lXWELu+Uo6r7EuquYvXO6l0kDZRj79vNFb2F1tfff0Vif2uvfeFHiWbnzqAAAATXXsU3IQ93wy4AABAHMxoAAARXzU1/Cg0AAIKr3sYJrRMAAJAiZjQAAAiO1gkAAEhNGnf0TAqtEwAAkBpmNAAACK6a76NBoQEAQHDVW2bQOgEAACliRgMAgOC46gQAAKSmW6/RmPTq0rQPkaiIT+l8/kMnZB2hbH/TsjfrCGXhqaLoKqL9jhvdc2DWEXCQmNEAACC46p3PoNAAACC8al6jwVUnAACgJGY2yMx+YWaNZrbBzKYU24cZDQAAgqvgYtC9kv7V3Veb2WGSnjGzx929sbMdKDQAAAiuUmWGu78s6eX8x6+Z2UZJ9ZI6LTRonQAAgP3MbLKZreqwTe7kfYMljZC0otD3Y0YDAIDgklwM6u4zJM0o9B4ze5+kn0i62t13F3ovhQYAAMF5BS9wNbNa7SsyfuzuPy32flonAACgJGZmku6RtNHdby1lHwoNAACCa09wK2K0pE9KOsvM1ua3jxfagdYJAADBVeryVndfLsnK2YcZDQAAkBpmNAAACI5nnQAAgNRU82PiaZ0AAIDUhCw0xo8bqw3rl2lT43JdM/WqrOMUNf3WaXps3cOavWRm1lHKk8tpwI/v0tG33Zh1kqKijnG0czlaXonMlRDt56/fwCP16dlf1rTHv6lpi/9dY648L+tIB62CV52ULVyhkcvldMftN+qCCVfopOFn6pJLLrL7buIAABFlSURBVNKwYcdlHaugBXMWacrlU7OOUbbDLvuEWre+mHWMkkQc42jncrS8EpkrJdrPX/veNs274Ye6+dzP61t//WWN/uQ4HTu0PutYB8UT/F/SChYaZvYvZjYo8aMehNNGjdCWLdu0deuLam1t1dy5j2jihPFZxypozYp12r3rtaxjlKXmmKN0yOjT9frDC7OOUpKIYxztXI6WVyJzpUT7+dv9yu/UtGGbJGnPG3/Qji3N6jugf7ahurBiMxrXS1phZk+Z2afN7OhKhCqkrn6AXmpq2f+6qfll1dUNyDBR13TEv16lXXfMkLx6FxhFF+1cjpZXIjOKO6LhaDWcOFgvrN2cdZSDErl18rykBu0rOE6V1Ghmj5nZ3+WfQ39AHZ/81t7+RoJxUQm9P/yXant1l1o3/VfWUQAgNT379NKV3/2sHvrqLO15/a2s4xyUam6dFLu81d29XdJiSYvzD1I5T9Jlkr4p6YAzHB2f/NajZ32iqVuat2tQQ93+1w31A9XSsj3JQ3R7vYZ/UIeM+SsdMvp0Wc+esvf10ZFfvVb/97qvZx2tS4l2LkfLK5EZncv1qNGVd31Ozzy8XM/+bGXWcbq0YjMaf3SbUXdvdfd57n6ZpPenF6tzK1et1dChQzR48CDV1tZq0qQLNf/RxVlE6bJ+/5171HL+pWqZeLl2fukG7Vm5liIjBdHO5Wh5JTKjc5fe/Cnt2NyspffEWIdWTDW3TorNaFzS2Rfc/c2Es5Skra1NU66eroUL7lNNLqd7Z81RY+NzWUQp2fV3XqdTzzhZ/fr31fxVD+juW2Zq3uyucXJXi4hjHO1cjpZXInOlRPv5GzLyeI26eIxaNr6gzy+8SZK04Bv3a+OTazNO9t61V/F6OvOUwyXdOknbKUcNzTpC2R6si3eD179p2Zt1hLKs3hl7oRjwjmi/40b3HJh1hPfktm33l/XgsYP1yfd/IrG/tT984aeJZo/3FwoAAPyRav4XPYUGAADB8awTAADQLTGjAQBAcGnc/yIpFBoAAASXxmWpSaF1AgAAUsOMBgAAwVXzYlAKDQAAgqvmNRq0TgAAQGqY0QAAILhqXgxKoQEAQHBpP07kYNA6AQAAqWFGAwCA4LjqJJCIT+n8G8V6GiMqI9pTOqV4P3+fqftI1hHK9u2Wp7KOUJbRdTGf3lpprNEAAACp4fJWAADQLTGjAQBAcKzRAAAAqeHyVgAA0C0xowEAQHBcdQIAAFLDVScAAKBbYkYDAIDguOoEAACkhqtOAABAt8SMBgAAwdE6AQAAqeGqEwAA0C0xowEAQHDtLAZN1vhxY7Vh/TJtalyua6ZelXWckkTLPP3WaXps3cOavWRm1lFKEi3vOzgv0hdtjPsNPFKfnv1lTXv8m5q2+N815srzso5UFGOcPU9wS1q4QiOXy+mO22/UBROu0EnDz9Qll1ykYcOOyzpWQREzL5izSFMun5p1jJJFyytxXlRCxDFu39umeTf8UDef+3l966+/rNGfHKdjh9ZnHatTjDGKKVhomFlPM/sfZnZO/vXfmtl/mNlVZlZbmYh/7LRRI7RlyzZt3fqiWltbNXfuI5o4YXwWUUoWMfOaFeu0e9drWccoWbS8EudFJUQc492v/E5NG7ZJkva88Qft2NKsvgP6ZxuqAMa4OrTLE9uSVmxGY6ak8yVNMbMfSvrvklZIGiXpe4mnKUFd/QC91NSy/3VT88uqqxuQRZSSRcyM9HFepC/6GB/RcLQaThysF9ZuzjpKpxjj6lDJQsPMvm9mvzWz9aVkK7YY9CR3/5CZ9ZDULKnO3dvM7EeSflMgxGRJkyXJavoqlzu0lCwAgLyefXrpyu9+Vg99dZb2vP5W1nG6JMb4PbtX0n9I+kEpby5WaOTMrKekQyX1kdRX0quSeknqtHXi7jMkzZCkHj3rE52HaWnerkENdftfN9QPVEvL9iQPkbiImZE+zov0RR3jXI8aXXnX5/TMw8v17M9WZh2nIMa4OlTyFuTuvszMBpf6/mKtk3skbZK0VtKXJD1gZndLWinp/veY8aCsXLVWQ4cO0eDBg1RbW6tJky7U/EcXZxGlZBEzI32cF+mLOsaX3vwp7djcrKX3LMw6SlGMcXVIsnViZpPNbFWHbfLBZCs4o+Hut5nZnPzHLWb2A0nnSLrb3X99MAd+r9ra2jTl6ulauOA+1eRyunfWHDU2PpdFlJJFzHz9ndfp1DNOVr/+fTV/1QO6+5aZmje7en8go+WVOC8qIeIYDxl5vEZdPEYtG1/Q5xfeJEla8I37tfHJtRknOzDGuOvp2JVIgqU93ZJ06wR/6pSjhmYdoctbvTPeQrGI50W0cf5M3UeyjlC2b7c8lXWEskQcY0m6bdv9Vsnjjaobk9jf2pUty4pmz7dOHnX3vyj2Xu4MCgBAcDwmHgAAdAlmNlvSryQdb2ZNZvb3hd7PjAYAAMFV8jHx7n5ZOe+n0AAAIDhaJwAAoFtiRgMAgOAq2TopF4UGAADBeRUXGrROAABAapjRAAAguPYqXgxKoQEAQHC0TgAAQLfEjAYAAMHROgEAAKmp5tYJhca78MTLyog2zm8Fe+KlJI3+0JVZR+jyPvKHeN3nb2cdoEzRnjb7jtuyDlBFKDQAAAiO1gkAAEhNNbdO4s37AQCAMJjRAAAgOFonAAAgNbROAABAt8SMBgAAwbm3Zx2hUxQaAAAE107rBAAAdEfMaAAAEJxz1QkAAEgLrRMAANAtMaMBAEBwtE4AAEBqqvnOoLROAABAakIWGuPHjdWG9cu0qXG5rpl6VdZxipp+6zQ9tu5hzV4yM+soJWOM07f1hSZd/HdX7d9OP/cT+uGch7KOVVDEcY52Lud61Wrsout11hNf1zlLv6FhUy/OOlJR0cZYipm5EE/wf0kLV2jkcjndcfuNumDCFTpp+Jm65JKLNGzYcVnHKmjBnEWacvnUrGOUjDGujCHvb9BPZn1HP5n1Hc39/h3q3bu3zv7oX2Udq6Bo4xzxXG7f06qnLr5BS86+Vk+cfa2OPXO4jjhlaNaxOhVxjCNmLsbdE9uSVrTQMLM/M7PPm9ntZnarmf2jmR2eeJISnTZqhLZs2aatW19Ua2ur5s59RBMnjM8qTknWrFin3bteyzpGyRjjynt61VoNqh+ougHHZh2loGjjHPFclqS2N/dIknK1Ncr1qJGquP8ecYwjZi6mXZ7YlrSChYaZ/YukuyT1ljRKUi9JgyQ9bWZjE09Tgrr6AXqpqWX/66bml1VXNyCLKF0WY1x5i55Yqo+f89GsY3Q5Yc/lnOmsn39N56+/SzuWPatda7ZknahTEcc4YubIis1o/IOk89z9BknnSPqgu39J0sck3dbZTmY22cxWmdmq9vY3kksLdEGtra16cvkKjTvrI1lHQbVody0554taNOKf1X/EB3T4CQ1ZJ0KVC9060f+/BLaXpPfl/4NelFTb2Q7uPsPdR7r7yFzu0INP2UFL83YNaqjb/7qhfqBaWrYneozujjGurKeeXqVhf/4BHdX/iKyjdDnRz+XW3W/qlV826tgzh2cdpVMRxzhi5mLa3RPbklas0PiepJVmdrekX0n6jiSZ2dGSXk08TQlWrlqroUOHaPDgQaqtrdWkSRdq/qOLs4jSZTHGlbXw8Sf18XPHZh2jS4p4Lvc88jDVHt5HkpTrXatjxpyk1za3FNkrOxHHOGLmyAresMvdbzezn0saJukWd9+U//wrksZUIN+faGtr05Srp2vhgvtUk8vp3llz1Nj4XBZRSnb9ndfp1DNOVr/+fTV/1QO6+5aZmjd7YdaxOsUYV86bb/1Bv1q5Rl+55l+yjlKSaOMc8VzufUw/jbzjn2Q1OSlnap73tLY/vibrWJ2KOMYRMxdTzXcGtbTD9ehZX73/9QdwylHVexlZZ1bv3Jx1hLJFG+dfrotz34h3jP7QlVlHKFu0c3lu/3gLeCe9ujTrCN3C3rebrZLH6/u+DyT2t/b3r29JNHu4+2gAAIA4eNYJAADBVXPrhEIDAIDgeKgaAADolpjRAAAguDQehpYUCg0AAIKjdQIAALolZjQAAAiOq04AAEBqqnmNBq0TAACQGmY0AAAIrppbJ8xoAAAQnLsnthVjZh8zs/80s81m9oVi76fQAAAAJTGzGknfkXSepBMlXWZmJxbah0IDAIDgPMGtiNMkbXb35939bUn3S7qw0A6pr9FI81G5ZjbZ3Wek9f2TFi2vFC9ztLxSepl/3ZLe48CjjXO0vFJ6mfcm/Q07iDbO0fIWkuTfWjObLGlyh0/N6DBO9ZJe6vC1JkmnF/p+0Wc0Jhd/S1WJlleKlzlaXonMlRAtr0TmSoiWtyLcfYa7j+ywHVQxFr3QAAAAldMsaVCH1w35z3WKQgMAAJRqpaTjzGyImfWUdKmkeYV2iH4fjWi9tWh5pXiZo+WVyFwJ0fJKZK6EaHkz5+57zeyfJf1MUo2k77v7hkL7WDXf5AMAAMRG6wQAAKSGQgMAAKQmZKFR7u1Ps2Zm3zez35rZ+qyzlMLMBpnZL8ys0cw2mNmUrDMVY2a9zezXZvabfOZ/yzpTKcysxszWmNmjWWcphZltM7NnzWytma3KOk8pzKyfmT1oZpvMbKOZnZF1pkLM7Pj8+L6z7Tazq7POVYiZfTb/c7fezGabWe+sMxVjZlPyeTdU+/hGF26NRv72p89JOlf7bhSyUtJl7t6YabACzGyMpNcl/cDd/yLrPMWY2UBJA919tZkdJukZSRdV+RibpEPd/XUzq5W0XNIUd38642gFmdnnJI2UdLi7X5B1nmLMbJukke6+M+sspTKzWZKecvfv5VfJ93H332WdqxT533fNkk539xeyznMgZlavfT9vJ7r7W2Y2V9JCd78322SdM7O/0L47Wp4m6W1Jj0n6R3ffnGmwLirijEbZtz/Nmrsvk/Rq1jlK5e4vu/vq/MevSdqofXeDq1q+z+v5l7X5raqraDNrkHS+pO9lnaWrMrO+ksZIukeS3P3tKEVG3tmStlRrkdFBD0mHmFkPSX0ktWScp5hhkla4+5vuvlfSUkmfyDhTlxWx0DjQ7U+r+o9gZGY2WNIISSuyTVJcvg2xVtJvJT3u7tWe+VuSrpHUnnWQMrikxWb2TP42xdVuiKRXJM3Mt6i+Z2aHZh2qDJdKmp11iELcvVnSNyW9KOllSb9398XZpipqvaSPmNmRZtZH0sf1xzehQoIiFhqoEDN7n6SfSLra3XdnnacYd29z95O17051p+WnR6uSmV0g6bfu/kzWWcr0YXc/Rfue3HhVvi1YzXpIOkXSd919hKQ3JFX9ui5Jyrd5Jkp6IOsshZjZEdo3qzxEUp2kQ83simxTFebuGyXdLGmx9rVN1kpqyzRUFxax0Cj79qcoX36dw08k/djdf5p1nnLkp8Z/IeljWWcpYLSkifk1D/dLOsvMfpRtpOLy/3qVu/9W0kPa18qsZk2SmjrMbj2ofYVHBOdJWu3uO7IOUsQ5kra6+yvu3irpp5L+KuNMRbn7Pe5+qruPkbRL+9b+IQURC42yb3+K8uQXVt4jaaO735p1nlKY2dFm1i//8SHat1h4U7apOufu17p7g7sP1r5zeIm7V/W/As3s0PziYOXbD+O0bwq6arn7dkkvmdnx+U+dLalqFzW/y2Wq8rZJ3ouS/tLM+uR/d5ytfeu6qpqZHZP///+mfesz7ss2UdcV7hbk7+X2p1kzs9mSxko6ysyaJH3F3e/JNlVBoyV9UtKz+TUPkvRFd1+YYaZiBkqalV+ln5M0191DXDIayLGSHtr3t0Q9JN3n7o9lG6kkn5H04/w/TJ6XdGXGeYrKF3LnSvpU1lmKcfcVZvagpNXa9xT6NYpxa++fmNmRklolXRVskXAo4S5vBQAAcURsnQAAgCAoNAAAQGooNAAAQGooNAAAQGooNAAAQGooNAAAQGooNAAAQGr+H5Szl6O8fXEtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTd-QxTTPBh5",
        "colab_type": "text"
      },
      "source": [
        "Training student model on test set without a digit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpFdPs4qPAvC",
        "colab_type": "code",
        "outputId": "8261b66a-0e9b-4048-d148-ff0e836c6d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Setup student model and move it to the GPU\n",
        "student_net = Model(hidden_size = 800)\n",
        "student_net.to(device)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = optim.SGD(student_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Run over 100 epochs (1 epoch = visited all items in dataset)\n",
        "for epoch in range(1000):\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader_noDigit, 0):\n",
        "      \n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = labels.to(device).long()\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Set temperature and the weights for losses linear combination\n",
        "        w = 0.7\n",
        "        T = 20\n",
        "\n",
        "        # Compute soft labels using deep teacher model previously trained\n",
        "        outputs_teacher = net(inputs)\n",
        "\n",
        "        # Student forward + backward + optimize\n",
        "        outputs_stud = student_net(inputs)\n",
        "        \n",
        "        loss = student_loss(outputs_stud, target, outputs_teacher, w, T)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += len(data)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # print every epoch\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save model after having finished training\n",
        "STUD_PATH = './mnist_student_no_digit.pth'\n",
        "torch.save(student_net.state_dict(), STUD_PATH)\n",
        "\n",
        "print('Saved Model')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 1.311\n",
            "[2] loss: 0.963\n",
            "[3] loss: 0.752\n",
            "[4] loss: 0.665\n",
            "[5] loss: 0.609\n",
            "[6] loss: 0.550\n",
            "[7] loss: 0.495\n",
            "[8] loss: 0.445\n",
            "[9] loss: 0.399\n",
            "[10] loss: 0.361\n",
            "[11] loss: 0.330\n",
            "[12] loss: 0.302\n",
            "[13] loss: 0.276\n",
            "[14] loss: 0.257\n",
            "[15] loss: 0.237\n",
            "[16] loss: 0.218\n",
            "[17] loss: 0.204\n",
            "[18] loss: 0.191\n",
            "[19] loss: 0.179\n",
            "[20] loss: 0.167\n",
            "[21] loss: 0.157\n",
            "[22] loss: 0.149\n",
            "[23] loss: 0.142\n",
            "[24] loss: 0.135\n",
            "[25] loss: 0.129\n",
            "[26] loss: 0.123\n",
            "[27] loss: 0.119\n",
            "[28] loss: 0.114\n",
            "[29] loss: 0.110\n",
            "[30] loss: 0.106\n",
            "[31] loss: 0.103\n",
            "[32] loss: 0.099\n",
            "[33] loss: 0.096\n",
            "[34] loss: 0.093\n",
            "[35] loss: 0.091\n",
            "[36] loss: 0.089\n",
            "[37] loss: 0.087\n",
            "[38] loss: 0.084\n",
            "[39] loss: 0.083\n",
            "[40] loss: 0.080\n",
            "[41] loss: 0.079\n",
            "[42] loss: 0.078\n",
            "[43] loss: 0.076\n",
            "[44] loss: 0.074\n",
            "[45] loss: 0.073\n",
            "[46] loss: 0.071\n",
            "[47] loss: 0.070\n",
            "[48] loss: 0.069\n",
            "[49] loss: 0.067\n",
            "[50] loss: 0.066\n",
            "[51] loss: 0.066\n",
            "[52] loss: 0.064\n",
            "[53] loss: 0.063\n",
            "[54] loss: 0.062\n",
            "[55] loss: 0.062\n",
            "[56] loss: 0.061\n",
            "[57] loss: 0.060\n",
            "[58] loss: 0.059\n",
            "[59] loss: 0.058\n",
            "[60] loss: 0.058\n",
            "[61] loss: 0.057\n",
            "[62] loss: 0.056\n",
            "[63] loss: 0.055\n",
            "[64] loss: 0.055\n",
            "[65] loss: 0.054\n",
            "[66] loss: 0.054\n",
            "[67] loss: 0.053\n",
            "[68] loss: 0.053\n",
            "[69] loss: 0.051\n",
            "[70] loss: 0.051\n",
            "[71] loss: 0.051\n",
            "[72] loss: 0.050\n",
            "[73] loss: 0.050\n",
            "[74] loss: 0.049\n",
            "[75] loss: 0.049\n",
            "[76] loss: 0.048\n",
            "[77] loss: 0.048\n",
            "[78] loss: 0.047\n",
            "[79] loss: 0.046\n",
            "[80] loss: 0.047\n",
            "[81] loss: 0.046\n",
            "[82] loss: 0.046\n",
            "[83] loss: 0.045\n",
            "[84] loss: 0.044\n",
            "[85] loss: 0.044\n",
            "[86] loss: 0.044\n",
            "[87] loss: 0.044\n",
            "[88] loss: 0.043\n",
            "[89] loss: 0.043\n",
            "[90] loss: 0.042\n",
            "[91] loss: 0.042\n",
            "[92] loss: 0.042\n",
            "[93] loss: 0.042\n",
            "[94] loss: 0.041\n",
            "[95] loss: 0.041\n",
            "[96] loss: 0.041\n",
            "[97] loss: 0.040\n",
            "[98] loss: 0.040\n",
            "[99] loss: 0.040\n",
            "[100] loss: 0.039\n",
            "[101] loss: 0.039\n",
            "[102] loss: 0.039\n",
            "[103] loss: 0.039\n",
            "[104] loss: 0.038\n",
            "[105] loss: 0.038\n",
            "[106] loss: 0.038\n",
            "[107] loss: 0.037\n",
            "[108] loss: 0.037\n",
            "[109] loss: 0.037\n",
            "[110] loss: 0.037\n",
            "[111] loss: 0.037\n",
            "[112] loss: 0.036\n",
            "[113] loss: 0.036\n",
            "[114] loss: 0.036\n",
            "[115] loss: 0.036\n",
            "[116] loss: 0.035\n",
            "[117] loss: 0.035\n",
            "[118] loss: 0.035\n",
            "[119] loss: 0.035\n",
            "[120] loss: 0.035\n",
            "[121] loss: 0.034\n",
            "[122] loss: 0.034\n",
            "[123] loss: 0.034\n",
            "[124] loss: 0.034\n",
            "[125] loss: 0.034\n",
            "[126] loss: 0.034\n",
            "[127] loss: 0.033\n",
            "[128] loss: 0.033\n",
            "[129] loss: 0.033\n",
            "[130] loss: 0.033\n",
            "[131] loss: 0.032\n",
            "[132] loss: 0.032\n",
            "[133] loss: 0.032\n",
            "[134] loss: 0.032\n",
            "[135] loss: 0.032\n",
            "[136] loss: 0.032\n",
            "[137] loss: 0.032\n",
            "[138] loss: 0.031\n",
            "[139] loss: 0.031\n",
            "[140] loss: 0.031\n",
            "[141] loss: 0.031\n",
            "[142] loss: 0.031\n",
            "[143] loss: 0.031\n",
            "[144] loss: 0.030\n",
            "[145] loss: 0.030\n",
            "[146] loss: 0.030\n",
            "[147] loss: 0.030\n",
            "[148] loss: 0.030\n",
            "[149] loss: 0.030\n",
            "[150] loss: 0.030\n",
            "[151] loss: 0.029\n",
            "[152] loss: 0.029\n",
            "[153] loss: 0.029\n",
            "[154] loss: 0.029\n",
            "[155] loss: 0.029\n",
            "[156] loss: 0.029\n",
            "[157] loss: 0.029\n",
            "[158] loss: 0.029\n",
            "[159] loss: 0.029\n",
            "[160] loss: 0.028\n",
            "[161] loss: 0.028\n",
            "[162] loss: 0.028\n",
            "[163] loss: 0.028\n",
            "[164] loss: 0.028\n",
            "[165] loss: 0.028\n",
            "[166] loss: 0.028\n",
            "[167] loss: 0.028\n",
            "[168] loss: 0.028\n",
            "[169] loss: 0.027\n",
            "[170] loss: 0.027\n",
            "[171] loss: 0.027\n",
            "[172] loss: 0.027\n",
            "[173] loss: 0.027\n",
            "[174] loss: 0.027\n",
            "[175] loss: 0.027\n",
            "[176] loss: 0.027\n",
            "[177] loss: 0.027\n",
            "[178] loss: 0.027\n",
            "[179] loss: 0.026\n",
            "[180] loss: 0.026\n",
            "[181] loss: 0.026\n",
            "[182] loss: 0.026\n",
            "[183] loss: 0.026\n",
            "[184] loss: 0.026\n",
            "[185] loss: 0.026\n",
            "[186] loss: 0.026\n",
            "[187] loss: 0.026\n",
            "[188] loss: 0.025\n",
            "[189] loss: 0.025\n",
            "[190] loss: 0.026\n",
            "[191] loss: 0.025\n",
            "[192] loss: 0.025\n",
            "[193] loss: 0.025\n",
            "[194] loss: 0.025\n",
            "[195] loss: 0.025\n",
            "[196] loss: 0.025\n",
            "[197] loss: 0.025\n",
            "[198] loss: 0.025\n",
            "[199] loss: 0.025\n",
            "[200] loss: 0.025\n",
            "[201] loss: 0.024\n",
            "[202] loss: 0.024\n",
            "[203] loss: 0.024\n",
            "[204] loss: 0.024\n",
            "[205] loss: 0.024\n",
            "[206] loss: 0.024\n",
            "[207] loss: 0.024\n",
            "[208] loss: 0.024\n",
            "[209] loss: 0.024\n",
            "[210] loss: 0.024\n",
            "[211] loss: 0.024\n",
            "[212] loss: 0.024\n",
            "[213] loss: 0.024\n",
            "[214] loss: 0.024\n",
            "[215] loss: 0.024\n",
            "[216] loss: 0.023\n",
            "[217] loss: 0.023\n",
            "[218] loss: 0.024\n",
            "[219] loss: 0.023\n",
            "[220] loss: 0.023\n",
            "[221] loss: 0.023\n",
            "[222] loss: 0.023\n",
            "[223] loss: 0.023\n",
            "[224] loss: 0.023\n",
            "[225] loss: 0.023\n",
            "[226] loss: 0.023\n",
            "[227] loss: 0.023\n",
            "[228] loss: 0.023\n",
            "[229] loss: 0.022\n",
            "[230] loss: 0.023\n",
            "[231] loss: 0.023\n",
            "[232] loss: 0.022\n",
            "[233] loss: 0.022\n",
            "[234] loss: 0.022\n",
            "[235] loss: 0.022\n",
            "[236] loss: 0.022\n",
            "[237] loss: 0.022\n",
            "[238] loss: 0.022\n",
            "[239] loss: 0.022\n",
            "[240] loss: 0.022\n",
            "[241] loss: 0.022\n",
            "[242] loss: 0.022\n",
            "[243] loss: 0.022\n",
            "[244] loss: 0.022\n",
            "[245] loss: 0.022\n",
            "[246] loss: 0.022\n",
            "[247] loss: 0.021\n",
            "[248] loss: 0.022\n",
            "[249] loss: 0.021\n",
            "[250] loss: 0.021\n",
            "[251] loss: 0.021\n",
            "[252] loss: 0.021\n",
            "[253] loss: 0.021\n",
            "[254] loss: 0.021\n",
            "[255] loss: 0.021\n",
            "[256] loss: 0.021\n",
            "[257] loss: 0.021\n",
            "[258] loss: 0.021\n",
            "[259] loss: 0.021\n",
            "[260] loss: 0.021\n",
            "[261] loss: 0.021\n",
            "[262] loss: 0.021\n",
            "[263] loss: 0.021\n",
            "[264] loss: 0.021\n",
            "[265] loss: 0.021\n",
            "[266] loss: 0.021\n",
            "[267] loss: 0.021\n",
            "[268] loss: 0.021\n",
            "[269] loss: 0.021\n",
            "[270] loss: 0.021\n",
            "[271] loss: 0.020\n",
            "[272] loss: 0.020\n",
            "[273] loss: 0.020\n",
            "[274] loss: 0.020\n",
            "[275] loss: 0.020\n",
            "[276] loss: 0.020\n",
            "[277] loss: 0.020\n",
            "[278] loss: 0.020\n",
            "[279] loss: 0.020\n",
            "[280] loss: 0.020\n",
            "[281] loss: 0.020\n",
            "[282] loss: 0.020\n",
            "[283] loss: 0.020\n",
            "[284] loss: 0.020\n",
            "[285] loss: 0.020\n",
            "[286] loss: 0.020\n",
            "[287] loss: 0.020\n",
            "[288] loss: 0.020\n",
            "[289] loss: 0.020\n",
            "[290] loss: 0.019\n",
            "[291] loss: 0.020\n",
            "[292] loss: 0.019\n",
            "[293] loss: 0.019\n",
            "[294] loss: 0.019\n",
            "[295] loss: 0.019\n",
            "[296] loss: 0.020\n",
            "[297] loss: 0.019\n",
            "[298] loss: 0.019\n",
            "[299] loss: 0.019\n",
            "[300] loss: 0.019\n",
            "[301] loss: 0.019\n",
            "[302] loss: 0.019\n",
            "[303] loss: 0.019\n",
            "[304] loss: 0.019\n",
            "[305] loss: 0.019\n",
            "[306] loss: 0.019\n",
            "[307] loss: 0.019\n",
            "[308] loss: 0.019\n",
            "[309] loss: 0.019\n",
            "[310] loss: 0.019\n",
            "[311] loss: 0.019\n",
            "[312] loss: 0.019\n",
            "[313] loss: 0.019\n",
            "[314] loss: 0.018\n",
            "[315] loss: 0.019\n",
            "[316] loss: 0.019\n",
            "[317] loss: 0.019\n",
            "[318] loss: 0.018\n",
            "[319] loss: 0.018\n",
            "[320] loss: 0.018\n",
            "[321] loss: 0.018\n",
            "[322] loss: 0.018\n",
            "[323] loss: 0.018\n",
            "[324] loss: 0.018\n",
            "[325] loss: 0.018\n",
            "[326] loss: 0.018\n",
            "[327] loss: 0.018\n",
            "[328] loss: 0.018\n",
            "[329] loss: 0.018\n",
            "[330] loss: 0.018\n",
            "[331] loss: 0.018\n",
            "[332] loss: 0.018\n",
            "[333] loss: 0.018\n",
            "[334] loss: 0.018\n",
            "[335] loss: 0.018\n",
            "[336] loss: 0.018\n",
            "[337] loss: 0.018\n",
            "[338] loss: 0.018\n",
            "[339] loss: 0.018\n",
            "[340] loss: 0.018\n",
            "[341] loss: 0.018\n",
            "[342] loss: 0.018\n",
            "[343] loss: 0.018\n",
            "[344] loss: 0.018\n",
            "[345] loss: 0.018\n",
            "[346] loss: 0.018\n",
            "[347] loss: 0.018\n",
            "[348] loss: 0.018\n",
            "[349] loss: 0.018\n",
            "[350] loss: 0.017\n",
            "[351] loss: 0.018\n",
            "[352] loss: 0.017\n",
            "[353] loss: 0.017\n",
            "[354] loss: 0.017\n",
            "[355] loss: 0.018\n",
            "[356] loss: 0.017\n",
            "[357] loss: 0.017\n",
            "[358] loss: 0.017\n",
            "[359] loss: 0.017\n",
            "[360] loss: 0.017\n",
            "[361] loss: 0.017\n",
            "[362] loss: 0.017\n",
            "[363] loss: 0.017\n",
            "[364] loss: 0.017\n",
            "[365] loss: 0.017\n",
            "[366] loss: 0.017\n",
            "[367] loss: 0.017\n",
            "[368] loss: 0.017\n",
            "[369] loss: 0.017\n",
            "[370] loss: 0.017\n",
            "[371] loss: 0.017\n",
            "[372] loss: 0.017\n",
            "[373] loss: 0.017\n",
            "[374] loss: 0.017\n",
            "[375] loss: 0.017\n",
            "[376] loss: 0.017\n",
            "[377] loss: 0.017\n",
            "[378] loss: 0.017\n",
            "[379] loss: 0.017\n",
            "[380] loss: 0.017\n",
            "[381] loss: 0.017\n",
            "[382] loss: 0.017\n",
            "[383] loss: 0.017\n",
            "[384] loss: 0.017\n",
            "[385] loss: 0.017\n",
            "[386] loss: 0.017\n",
            "[387] loss: 0.016\n",
            "[388] loss: 0.017\n",
            "[389] loss: 0.016\n",
            "[390] loss: 0.016\n",
            "[391] loss: 0.016\n",
            "[392] loss: 0.016\n",
            "[393] loss: 0.016\n",
            "[394] loss: 0.016\n",
            "[395] loss: 0.016\n",
            "[396] loss: 0.016\n",
            "[397] loss: 0.016\n",
            "[398] loss: 0.016\n",
            "[399] loss: 0.016\n",
            "[400] loss: 0.016\n",
            "[401] loss: 0.016\n",
            "[402] loss: 0.016\n",
            "[403] loss: 0.016\n",
            "[404] loss: 0.016\n",
            "[405] loss: 0.016\n",
            "[406] loss: 0.016\n",
            "[407] loss: 0.016\n",
            "[408] loss: 0.016\n",
            "[409] loss: 0.016\n",
            "[410] loss: 0.016\n",
            "[411] loss: 0.016\n",
            "[412] loss: 0.016\n",
            "[413] loss: 0.016\n",
            "[414] loss: 0.016\n",
            "[415] loss: 0.016\n",
            "[416] loss: 0.016\n",
            "[417] loss: 0.016\n",
            "[418] loss: 0.016\n",
            "[419] loss: 0.016\n",
            "[420] loss: 0.016\n",
            "[421] loss: 0.016\n",
            "[422] loss: 0.016\n",
            "[423] loss: 0.016\n",
            "[424] loss: 0.016\n",
            "[425] loss: 0.016\n",
            "[426] loss: 0.016\n",
            "[427] loss: 0.016\n",
            "[428] loss: 0.016\n",
            "[429] loss: 0.015\n",
            "[430] loss: 0.016\n",
            "[431] loss: 0.016\n",
            "[432] loss: 0.016\n",
            "[433] loss: 0.016\n",
            "[434] loss: 0.015\n",
            "[435] loss: 0.016\n",
            "[436] loss: 0.015\n",
            "[437] loss: 0.016\n",
            "[438] loss: 0.015\n",
            "[439] loss: 0.015\n",
            "[440] loss: 0.015\n",
            "[441] loss: 0.015\n",
            "[442] loss: 0.015\n",
            "[443] loss: 0.015\n",
            "[444] loss: 0.015\n",
            "[445] loss: 0.015\n",
            "[446] loss: 0.015\n",
            "[447] loss: 0.015\n",
            "[448] loss: 0.015\n",
            "[449] loss: 0.015\n",
            "[450] loss: 0.015\n",
            "[451] loss: 0.015\n",
            "[452] loss: 0.015\n",
            "[453] loss: 0.015\n",
            "[454] loss: 0.015\n",
            "[455] loss: 0.015\n",
            "[456] loss: 0.015\n",
            "[457] loss: 0.015\n",
            "[458] loss: 0.015\n",
            "[459] loss: 0.015\n",
            "[460] loss: 0.015\n",
            "[461] loss: 0.015\n",
            "[462] loss: 0.015\n",
            "[463] loss: 0.015\n",
            "[464] loss: 0.015\n",
            "[465] loss: 0.015\n",
            "[466] loss: 0.015\n",
            "[467] loss: 0.015\n",
            "[468] loss: 0.015\n",
            "[469] loss: 0.015\n",
            "[470] loss: 0.015\n",
            "[471] loss: 0.015\n",
            "[472] loss: 0.015\n",
            "[473] loss: 0.015\n",
            "[474] loss: 0.015\n",
            "[475] loss: 0.015\n",
            "[476] loss: 0.015\n",
            "[477] loss: 0.015\n",
            "[478] loss: 0.015\n",
            "[479] loss: 0.015\n",
            "[480] loss: 0.015\n",
            "[481] loss: 0.015\n",
            "[482] loss: 0.015\n",
            "[483] loss: 0.015\n",
            "[484] loss: 0.015\n",
            "[485] loss: 0.015\n",
            "[486] loss: 0.015\n",
            "[487] loss: 0.015\n",
            "[488] loss: 0.015\n",
            "[489] loss: 0.014\n",
            "[490] loss: 0.015\n",
            "[491] loss: 0.014\n",
            "[492] loss: 0.015\n",
            "[493] loss: 0.014\n",
            "[494] loss: 0.014\n",
            "[495] loss: 0.014\n",
            "[496] loss: 0.014\n",
            "[497] loss: 0.014\n",
            "[498] loss: 0.014\n",
            "[499] loss: 0.014\n",
            "[500] loss: 0.014\n",
            "[501] loss: 0.014\n",
            "[502] loss: 0.014\n",
            "[503] loss: 0.014\n",
            "[504] loss: 0.014\n",
            "[505] loss: 0.014\n",
            "[506] loss: 0.014\n",
            "[507] loss: 0.014\n",
            "[508] loss: 0.014\n",
            "[509] loss: 0.014\n",
            "[510] loss: 0.014\n",
            "[511] loss: 0.014\n",
            "[512] loss: 0.014\n",
            "[513] loss: 0.014\n",
            "[514] loss: 0.014\n",
            "[515] loss: 0.014\n",
            "[516] loss: 0.014\n",
            "[517] loss: 0.014\n",
            "[518] loss: 0.014\n",
            "[519] loss: 0.014\n",
            "[520] loss: 0.014\n",
            "[521] loss: 0.014\n",
            "[522] loss: 0.014\n",
            "[523] loss: 0.014\n",
            "[524] loss: 0.014\n",
            "[525] loss: 0.014\n",
            "[526] loss: 0.014\n",
            "[527] loss: 0.014\n",
            "[528] loss: 0.014\n",
            "[529] loss: 0.014\n",
            "[530] loss: 0.014\n",
            "[531] loss: 0.014\n",
            "[532] loss: 0.014\n",
            "[533] loss: 0.014\n",
            "[534] loss: 0.014\n",
            "[535] loss: 0.014\n",
            "[536] loss: 0.014\n",
            "[537] loss: 0.014\n",
            "[538] loss: 0.014\n",
            "[539] loss: 0.014\n",
            "[540] loss: 0.014\n",
            "[541] loss: 0.014\n",
            "[542] loss: 0.014\n",
            "[543] loss: 0.014\n",
            "[544] loss: 0.014\n",
            "[545] loss: 0.014\n",
            "[546] loss: 0.014\n",
            "[547] loss: 0.014\n",
            "[548] loss: 0.014\n",
            "[549] loss: 0.014\n",
            "[550] loss: 0.014\n",
            "[551] loss: 0.014\n",
            "[552] loss: 0.014\n",
            "[553] loss: 0.013\n",
            "[554] loss: 0.014\n",
            "[555] loss: 0.014\n",
            "[556] loss: 0.014\n",
            "[557] loss: 0.013\n",
            "[558] loss: 0.013\n",
            "[559] loss: 0.014\n",
            "[560] loss: 0.014\n",
            "[561] loss: 0.014\n",
            "[562] loss: 0.014\n",
            "[563] loss: 0.013\n",
            "[564] loss: 0.014\n",
            "[565] loss: 0.013\n",
            "[566] loss: 0.013\n",
            "[567] loss: 0.013\n",
            "[568] loss: 0.014\n",
            "[569] loss: 0.013\n",
            "[570] loss: 0.013\n",
            "[571] loss: 0.014\n",
            "[572] loss: 0.013\n",
            "[573] loss: 0.013\n",
            "[574] loss: 0.013\n",
            "[575] loss: 0.013\n",
            "[576] loss: 0.013\n",
            "[577] loss: 0.013\n",
            "[578] loss: 0.013\n",
            "[579] loss: 0.013\n",
            "[580] loss: 0.013\n",
            "[581] loss: 0.013\n",
            "[582] loss: 0.013\n",
            "[583] loss: 0.013\n",
            "[584] loss: 0.013\n",
            "[585] loss: 0.013\n",
            "[586] loss: 0.013\n",
            "[587] loss: 0.013\n",
            "[588] loss: 0.013\n",
            "[589] loss: 0.013\n",
            "[590] loss: 0.013\n",
            "[591] loss: 0.013\n",
            "[592] loss: 0.013\n",
            "[593] loss: 0.013\n",
            "[594] loss: 0.013\n",
            "[595] loss: 0.013\n",
            "[596] loss: 0.013\n",
            "[597] loss: 0.013\n",
            "[598] loss: 0.013\n",
            "[599] loss: 0.013\n",
            "[600] loss: 0.013\n",
            "[601] loss: 0.013\n",
            "[602] loss: 0.013\n",
            "[603] loss: 0.013\n",
            "[604] loss: 0.013\n",
            "[605] loss: 0.013\n",
            "[606] loss: 0.013\n",
            "[607] loss: 0.013\n",
            "[608] loss: 0.013\n",
            "[609] loss: 0.013\n",
            "[610] loss: 0.013\n",
            "[611] loss: 0.013\n",
            "[612] loss: 0.013\n",
            "[613] loss: 0.013\n",
            "[614] loss: 0.013\n",
            "[615] loss: 0.013\n",
            "[616] loss: 0.013\n",
            "[617] loss: 0.013\n",
            "[618] loss: 0.013\n",
            "[619] loss: 0.013\n",
            "[620] loss: 0.013\n",
            "[621] loss: 0.013\n",
            "[622] loss: 0.013\n",
            "[623] loss: 0.013\n",
            "[624] loss: 0.013\n",
            "[625] loss: 0.013\n",
            "[626] loss: 0.013\n",
            "[627] loss: 0.013\n",
            "[628] loss: 0.013\n",
            "[629] loss: 0.013\n",
            "[630] loss: 0.013\n",
            "[631] loss: 0.013\n",
            "[632] loss: 0.013\n",
            "[633] loss: 0.013\n",
            "[634] loss: 0.013\n",
            "[635] loss: 0.013\n",
            "[636] loss: 0.013\n",
            "[637] loss: 0.013\n",
            "[638] loss: 0.013\n",
            "[639] loss: 0.013\n",
            "[640] loss: 0.013\n",
            "[641] loss: 0.013\n",
            "[642] loss: 0.013\n",
            "[643] loss: 0.013\n",
            "[644] loss: 0.013\n",
            "[645] loss: 0.013\n",
            "[646] loss: 0.013\n",
            "[647] loss: 0.013\n",
            "[648] loss: 0.012\n",
            "[649] loss: 0.013\n",
            "[650] loss: 0.013\n",
            "[651] loss: 0.012\n",
            "[652] loss: 0.012\n",
            "[653] loss: 0.012\n",
            "[654] loss: 0.012\n",
            "[655] loss: 0.012\n",
            "[656] loss: 0.013\n",
            "[657] loss: 0.012\n",
            "[658] loss: 0.013\n",
            "[659] loss: 0.012\n",
            "[660] loss: 0.012\n",
            "[661] loss: 0.012\n",
            "[662] loss: 0.012\n",
            "[663] loss: 0.012\n",
            "[664] loss: 0.012\n",
            "[665] loss: 0.012\n",
            "[666] loss: 0.012\n",
            "[667] loss: 0.012\n",
            "[668] loss: 0.013\n",
            "[669] loss: 0.012\n",
            "[670] loss: 0.012\n",
            "[671] loss: 0.012\n",
            "[672] loss: 0.012\n",
            "[673] loss: 0.012\n",
            "[674] loss: 0.012\n",
            "[675] loss: 0.012\n",
            "[676] loss: 0.012\n",
            "[677] loss: 0.012\n",
            "[678] loss: 0.012\n",
            "[679] loss: 0.012\n",
            "[680] loss: 0.012\n",
            "[681] loss: 0.012\n",
            "[682] loss: 0.012\n",
            "[683] loss: 0.012\n",
            "[684] loss: 0.012\n",
            "[685] loss: 0.012\n",
            "[686] loss: 0.012\n",
            "[687] loss: 0.012\n",
            "[688] loss: 0.012\n",
            "[689] loss: 0.012\n",
            "[690] loss: 0.012\n",
            "[691] loss: 0.012\n",
            "[692] loss: 0.012\n",
            "[693] loss: 0.012\n",
            "[694] loss: 0.012\n",
            "[695] loss: 0.012\n",
            "[696] loss: 0.012\n",
            "[697] loss: 0.012\n",
            "[698] loss: 0.012\n",
            "[699] loss: 0.012\n",
            "[700] loss: 0.012\n",
            "[701] loss: 0.012\n",
            "[702] loss: 0.012\n",
            "[703] loss: 0.012\n",
            "[704] loss: 0.012\n",
            "[705] loss: 0.012\n",
            "[706] loss: 0.012\n",
            "[707] loss: 0.012\n",
            "[708] loss: 0.012\n",
            "[709] loss: 0.012\n",
            "[710] loss: 0.012\n",
            "[711] loss: 0.012\n",
            "[712] loss: 0.012\n",
            "[713] loss: 0.012\n",
            "[714] loss: 0.012\n",
            "[715] loss: 0.012\n",
            "[716] loss: 0.012\n",
            "[717] loss: 0.012\n",
            "[718] loss: 0.012\n",
            "[719] loss: 0.012\n",
            "[720] loss: 0.012\n",
            "[721] loss: 0.012\n",
            "[722] loss: 0.012\n",
            "[723] loss: 0.012\n",
            "[724] loss: 0.012\n",
            "[725] loss: 0.012\n",
            "[726] loss: 0.012\n",
            "[727] loss: 0.012\n",
            "[728] loss: 0.012\n",
            "[729] loss: 0.012\n",
            "[730] loss: 0.012\n",
            "[731] loss: 0.012\n",
            "[732] loss: 0.012\n",
            "[733] loss: 0.012\n",
            "[734] loss: 0.012\n",
            "[735] loss: 0.012\n",
            "[736] loss: 0.012\n",
            "[737] loss: 0.012\n",
            "[738] loss: 0.012\n",
            "[739] loss: 0.012\n",
            "[740] loss: 0.012\n",
            "[741] loss: 0.012\n",
            "[742] loss: 0.012\n",
            "[743] loss: 0.012\n",
            "[744] loss: 0.012\n",
            "[745] loss: 0.012\n",
            "[746] loss: 0.012\n",
            "[747] loss: 0.012\n",
            "[748] loss: 0.012\n",
            "[749] loss: 0.012\n",
            "[750] loss: 0.012\n",
            "[751] loss: 0.012\n",
            "[752] loss: 0.012\n",
            "[753] loss: 0.012\n",
            "[754] loss: 0.012\n",
            "[755] loss: 0.012\n",
            "[756] loss: 0.012\n",
            "[757] loss: 0.012\n",
            "[758] loss: 0.012\n",
            "[759] loss: 0.012\n",
            "[760] loss: 0.012\n",
            "[761] loss: 0.012\n",
            "[762] loss: 0.012\n",
            "[763] loss: 0.012\n",
            "[764] loss: 0.012\n",
            "[765] loss: 0.011\n",
            "[766] loss: 0.011\n",
            "[767] loss: 0.012\n",
            "[768] loss: 0.012\n",
            "[769] loss: 0.012\n",
            "[770] loss: 0.012\n",
            "[771] loss: 0.011\n",
            "[772] loss: 0.011\n",
            "[773] loss: 0.011\n",
            "[774] loss: 0.012\n",
            "[775] loss: 0.011\n",
            "[776] loss: 0.011\n",
            "[777] loss: 0.011\n",
            "[778] loss: 0.011\n",
            "[779] loss: 0.011\n",
            "[780] loss: 0.011\n",
            "[781] loss: 0.011\n",
            "[782] loss: 0.012\n",
            "[783] loss: 0.011\n",
            "[784] loss: 0.011\n",
            "[785] loss: 0.011\n",
            "[786] loss: 0.011\n",
            "[787] loss: 0.011\n",
            "[788] loss: 0.011\n",
            "[789] loss: 0.011\n",
            "[790] loss: 0.011\n",
            "[791] loss: 0.011\n",
            "[792] loss: 0.011\n",
            "[793] loss: 0.011\n",
            "[794] loss: 0.011\n",
            "[795] loss: 0.011\n",
            "[796] loss: 0.011\n",
            "[797] loss: 0.011\n",
            "[798] loss: 0.011\n",
            "[799] loss: 0.011\n",
            "[800] loss: 0.011\n",
            "[801] loss: 0.011\n",
            "[802] loss: 0.011\n",
            "[803] loss: 0.011\n",
            "[804] loss: 0.011\n",
            "[805] loss: 0.011\n",
            "[806] loss: 0.011\n",
            "[807] loss: 0.011\n",
            "[808] loss: 0.011\n",
            "[809] loss: 0.011\n",
            "[810] loss: 0.011\n",
            "[811] loss: 0.011\n",
            "[812] loss: 0.011\n",
            "[813] loss: 0.011\n",
            "[814] loss: 0.011\n",
            "[815] loss: 0.011\n",
            "[816] loss: 0.011\n",
            "[817] loss: 0.011\n",
            "[818] loss: 0.011\n",
            "[819] loss: 0.011\n",
            "[820] loss: 0.011\n",
            "[821] loss: 0.011\n",
            "[822] loss: 0.011\n",
            "[823] loss: 0.011\n",
            "[824] loss: 0.011\n",
            "[825] loss: 0.011\n",
            "[826] loss: 0.011\n",
            "[827] loss: 0.011\n",
            "[828] loss: 0.011\n",
            "[829] loss: 0.011\n",
            "[830] loss: 0.011\n",
            "[831] loss: 0.011\n",
            "[832] loss: 0.011\n",
            "[833] loss: 0.011\n",
            "[834] loss: 0.011\n",
            "[835] loss: 0.011\n",
            "[836] loss: 0.011\n",
            "[837] loss: 0.011\n",
            "[838] loss: 0.011\n",
            "[839] loss: 0.011\n",
            "[840] loss: 0.011\n",
            "[841] loss: 0.011\n",
            "[842] loss: 0.011\n",
            "[843] loss: 0.011\n",
            "[844] loss: 0.011\n",
            "[845] loss: 0.011\n",
            "[846] loss: 0.011\n",
            "[847] loss: 0.011\n",
            "[848] loss: 0.011\n",
            "[849] loss: 0.011\n",
            "[850] loss: 0.011\n",
            "[851] loss: 0.011\n",
            "[852] loss: 0.011\n",
            "[853] loss: 0.011\n",
            "[854] loss: 0.011\n",
            "[855] loss: 0.011\n",
            "[856] loss: 0.011\n",
            "[857] loss: 0.011\n",
            "[858] loss: 0.011\n",
            "[859] loss: 0.011\n",
            "[860] loss: 0.011\n",
            "[861] loss: 0.011\n",
            "[862] loss: 0.011\n",
            "[863] loss: 0.011\n",
            "[864] loss: 0.011\n",
            "[865] loss: 0.011\n",
            "[866] loss: 0.011\n",
            "[867] loss: 0.011\n",
            "[868] loss: 0.011\n",
            "[869] loss: 0.011\n",
            "[870] loss: 0.011\n",
            "[871] loss: 0.011\n",
            "[872] loss: 0.011\n",
            "[873] loss: 0.011\n",
            "[874] loss: 0.011\n",
            "[875] loss: 0.011\n",
            "[876] loss: 0.011\n",
            "[877] loss: 0.011\n",
            "[878] loss: 0.011\n",
            "[879] loss: 0.011\n",
            "[880] loss: 0.011\n",
            "[881] loss: 0.011\n",
            "[882] loss: 0.011\n",
            "[883] loss: 0.011\n",
            "[884] loss: 0.011\n",
            "[885] loss: 0.011\n",
            "[886] loss: 0.011\n",
            "[887] loss: 0.011\n",
            "[888] loss: 0.011\n",
            "[889] loss: 0.011\n",
            "[890] loss: 0.011\n",
            "[891] loss: 0.011\n",
            "[892] loss: 0.011\n",
            "[893] loss: 0.011\n",
            "[894] loss: 0.011\n",
            "[895] loss: 0.011\n",
            "[896] loss: 0.011\n",
            "[897] loss: 0.011\n",
            "[898] loss: 0.011\n",
            "[899] loss: 0.011\n",
            "[900] loss: 0.011\n",
            "[901] loss: 0.011\n",
            "[902] loss: 0.011\n",
            "[903] loss: 0.011\n",
            "[904] loss: 0.010\n",
            "[905] loss: 0.011\n",
            "[906] loss: 0.011\n",
            "[907] loss: 0.010\n",
            "[908] loss: 0.011\n",
            "[909] loss: 0.011\n",
            "[910] loss: 0.011\n",
            "[911] loss: 0.011\n",
            "[912] loss: 0.010\n",
            "[913] loss: 0.011\n",
            "[914] loss: 0.011\n",
            "[915] loss: 0.011\n",
            "[916] loss: 0.011\n",
            "[917] loss: 0.011\n",
            "[918] loss: 0.010\n",
            "[919] loss: 0.010\n",
            "[920] loss: 0.011\n",
            "[921] loss: 0.010\n",
            "[922] loss: 0.010\n",
            "[923] loss: 0.011\n",
            "[924] loss: 0.010\n",
            "[925] loss: 0.011\n",
            "[926] loss: 0.010\n",
            "[927] loss: 0.011\n",
            "[928] loss: 0.011\n",
            "[929] loss: 0.010\n",
            "[930] loss: 0.010\n",
            "[931] loss: 0.010\n",
            "[932] loss: 0.010\n",
            "[933] loss: 0.011\n",
            "[934] loss: 0.010\n",
            "[935] loss: 0.010\n",
            "[936] loss: 0.010\n",
            "[937] loss: 0.010\n",
            "[938] loss: 0.010\n",
            "[939] loss: 0.010\n",
            "[940] loss: 0.010\n",
            "[941] loss: 0.010\n",
            "[942] loss: 0.010\n",
            "[943] loss: 0.010\n",
            "[944] loss: 0.011\n",
            "[945] loss: 0.010\n",
            "[946] loss: 0.010\n",
            "[947] loss: 0.010\n",
            "[948] loss: 0.010\n",
            "[949] loss: 0.010\n",
            "[950] loss: 0.010\n",
            "[951] loss: 0.011\n",
            "[952] loss: 0.010\n",
            "[953] loss: 0.010\n",
            "[954] loss: 0.010\n",
            "[955] loss: 0.010\n",
            "[956] loss: 0.010\n",
            "[957] loss: 0.010\n",
            "[958] loss: 0.010\n",
            "[959] loss: 0.010\n",
            "[960] loss: 0.010\n",
            "[961] loss: 0.010\n",
            "[962] loss: 0.010\n",
            "[963] loss: 0.010\n",
            "[964] loss: 0.010\n",
            "[965] loss: 0.010\n",
            "[966] loss: 0.010\n",
            "[967] loss: 0.010\n",
            "[968] loss: 0.010\n",
            "[969] loss: 0.010\n",
            "[970] loss: 0.010\n",
            "[971] loss: 0.010\n",
            "[972] loss: 0.010\n",
            "[973] loss: 0.010\n",
            "[974] loss: 0.010\n",
            "[975] loss: 0.010\n",
            "[976] loss: 0.010\n",
            "[977] loss: 0.010\n",
            "[978] loss: 0.010\n",
            "[979] loss: 0.010\n",
            "[980] loss: 0.010\n",
            "[981] loss: 0.010\n",
            "[982] loss: 0.010\n",
            "[983] loss: 0.010\n",
            "[984] loss: 0.010\n",
            "[985] loss: 0.010\n",
            "[986] loss: 0.010\n",
            "[987] loss: 0.010\n",
            "[988] loss: 0.010\n",
            "[989] loss: 0.010\n",
            "[990] loss: 0.010\n",
            "[991] loss: 0.010\n",
            "[992] loss: 0.010\n",
            "[993] loss: 0.010\n",
            "[994] loss: 0.010\n",
            "[995] loss: 0.010\n",
            "[996] loss: 0.010\n",
            "[997] loss: 0.010\n",
            "[998] loss: 0.010\n",
            "[999] loss: 0.010\n",
            "[1000] loss: 0.010\n",
            "Finished Training\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-AQ-tphe38",
        "colab_type": "text"
      },
      "source": [
        "Test student traint on digitless training set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDKsq9f6hjuk",
        "colab_type": "code",
        "outputId": "f18b6832-c914-40ec-96ea-ddb0ee3315cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "stud_net = Model(hidden_size = 800).to(device)\n",
        "stud_net.load_state_dict(torch.load(STUD_PATH))\n",
        "\n",
        "# Run model on test set and determine accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "wrong = np.zeros((10,10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = convert_labels(labels).to(device)\n",
        "        outputs = stud_net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, target = torch.max(target.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        for i, val in enumerate(predicted):\n",
        "          if val != target[i]:\n",
        "            wrong[target[i]][val] += 1\n",
        "\n",
        "# Output model accuracy to user\n",
        "print('Accuracy of the network on the 10000 test images: %d %% (%d wrong out of %d)' % (\n",
        "    100 * correct / total, total - correct, total))\n",
        "#print(\" \"+str([0,1,2,3,4,5,6,7,8,9]))\n",
        "#print(wrong)\n",
        "\n",
        "# Plot confusion matrix\n",
        "df_cm = pd.DataFrame(wrong, index = [i for i in \"0123456789\"],\n",
        "                  columns = [i for i in \"0123456789\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 98 % (161 wrong out of 10000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa19bc5198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}