{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distillation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E20iEpB1qB_U",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXEF8f3moEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pytorch basic functions/classes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import torchvision functions/classes for MNIST import and data loaders\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set device on which code is run\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl08sOn-p-WI",
        "colab_type": "text"
      },
      "source": [
        "Defining support functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZ1UPkh2iN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define support function used to convert label to one-hot encoded tensor\n",
        "def convert_labels(labels):\n",
        "    target = torch.zeros([len(labels), 10], dtype=torch.float32)\n",
        "    for i, l in enumerate(labels):\n",
        "      target[i][l] = 1.0\n",
        "    return target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MepCPdkTqMD3",
        "colab_type": "text"
      },
      "source": [
        "Define our network model (the hidden layers size is specified through the constructor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFrlhvun-U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define MLP model and its layers\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=1200, dropout=0.0, hidden_dropout=0.0):\n",
        "        super(Model, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden1 = nn.Linear(784, hidden_size, bias=True)\n",
        "        self.hidden1_dropout = nn.Dropout(hidden_dropout)\n",
        "        self.hidden2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.hidden2_dropout = nn.Dropout(hidden_dropout)\n",
        "        self.hidden3 = nn.Linear(hidden_size, 10, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = self.hidden1_dropout(x)\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.hidden2_dropout(x)\n",
        "        x = self.hidden3(x)\n",
        "        return x#, F.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwzMAeT7qZCW",
        "colab_type": "text"
      },
      "source": [
        "Downloading MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONsjY_Fmpe0t",
        "colab_type": "code",
        "outputId": "775aaf19-490e-4a90-80c6-930b9ca6649b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Define transform from PIL image to tensor and normalize to 1x768 pixels\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomAffine(0, (1/14, 1/14)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Set batch size for data loaders\n",
        "batch_size = 128\n",
        "\n",
        "# (Down)load training set\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# (Down)load test set\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "'''examples = enumerate(testloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "#fig'''"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'examples = enumerate(testloader)\\nbatch_idx, (example_data, example_targets) = next(examples)\\n\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure()\\nfor i in range(6):\\n  plt.subplot(2,3,i+1)\\n  plt.tight_layout()\\n  plt.imshow(example_data[i][0], cmap=\\'gray\\', interpolation=\\'none\\')\\n  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\\n  plt.xticks([])\\n  plt.yticks([])\\n#fig'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdksm3J5qfzu",
        "colab_type": "text"
      },
      "source": [
        "Training the Deep Teacher Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rOCDtvFrTa4",
        "colab_type": "code",
        "outputId": "1b5850cb-cb4c-4d68-b398-d13e2055c8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Setup model and move it to the GPU\n",
        "net = Model(dropout=0.2, hidden_dropout=0.5)\n",
        "net.to(device)\n",
        "\n",
        "# Set up loss function and optimizer: \n",
        "#     using cross entropy loss because it's better for classification task\n",
        "\n",
        "learning_rate = 0.01\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum=0.9)\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.00001)\n",
        "\n",
        "# Run over 100 epochs (1 epoch = visited all items in dataset)\n",
        "for epoch in range(2000):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "\n",
        "    if(epoch%200 == 0 and epoch != 0):\n",
        "\n",
        "      learning_rate = learning_rate - (0.001) # or maybe decrease by (learning_rate * 0.1)\n",
        "      optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum=0.9)\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        \n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "\n",
        "        # This for not cross entropy\n",
        "        #target = convert_labels(labels).to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        target = labels.to(device).long()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += len(data)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # print every epoch\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save model after having finished training\n",
        "PATH = './mnist_dropout_100_epoch.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Saved Model')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 0.515\n",
            "[2] loss: 0.265\n",
            "[3] loss: 0.202\n",
            "[4] loss: 0.174\n",
            "[5] loss: 0.153\n",
            "[6] loss: 0.140\n",
            "[7] loss: 0.131\n",
            "[8] loss: 0.122\n",
            "[9] loss: 0.116\n",
            "[10] loss: 0.110\n",
            "[11] loss: 0.107\n",
            "[12] loss: 0.103\n",
            "[13] loss: 0.099\n",
            "[14] loss: 0.094\n",
            "[15] loss: 0.093\n",
            "[16] loss: 0.091\n",
            "[17] loss: 0.086\n",
            "[18] loss: 0.087\n",
            "[19] loss: 0.083\n",
            "[20] loss: 0.080\n",
            "[21] loss: 0.082\n",
            "[22] loss: 0.078\n",
            "[23] loss: 0.076\n",
            "[24] loss: 0.076\n",
            "[25] loss: 0.075\n",
            "[26] loss: 0.072\n",
            "[27] loss: 0.075\n",
            "[28] loss: 0.070\n",
            "[29] loss: 0.069\n",
            "[30] loss: 0.067\n",
            "[31] loss: 0.068\n",
            "[32] loss: 0.068\n",
            "[33] loss: 0.065\n",
            "[34] loss: 0.065\n",
            "[35] loss: 0.064\n",
            "[36] loss: 0.067\n",
            "[37] loss: 0.064\n",
            "[38] loss: 0.062\n",
            "[39] loss: 0.062\n",
            "[40] loss: 0.062\n",
            "[41] loss: 0.061\n",
            "[42] loss: 0.062\n",
            "[43] loss: 0.060\n",
            "[44] loss: 0.060\n",
            "[45] loss: 0.058\n",
            "[46] loss: 0.058\n",
            "[47] loss: 0.058\n",
            "[48] loss: 0.057\n",
            "[49] loss: 0.057\n",
            "[50] loss: 0.055\n",
            "[51] loss: 0.055\n",
            "[52] loss: 0.056\n",
            "[53] loss: 0.054\n",
            "[54] loss: 0.056\n",
            "[55] loss: 0.055\n",
            "[56] loss: 0.053\n",
            "[57] loss: 0.054\n",
            "[58] loss: 0.053\n",
            "[59] loss: 0.052\n",
            "[60] loss: 0.052\n",
            "[61] loss: 0.051\n",
            "[62] loss: 0.052\n",
            "[63] loss: 0.051\n",
            "[64] loss: 0.050\n",
            "[65] loss: 0.051\n",
            "[66] loss: 0.052\n",
            "[67] loss: 0.050\n",
            "[68] loss: 0.050\n",
            "[69] loss: 0.050\n",
            "[70] loss: 0.049\n",
            "[71] loss: 0.050\n",
            "[72] loss: 0.048\n",
            "[73] loss: 0.049\n",
            "[74] loss: 0.049\n",
            "[75] loss: 0.047\n",
            "[76] loss: 0.047\n",
            "[77] loss: 0.047\n",
            "[78] loss: 0.046\n",
            "[79] loss: 0.047\n",
            "[80] loss: 0.046\n",
            "[81] loss: 0.045\n",
            "[82] loss: 0.046\n",
            "[83] loss: 0.046\n",
            "[84] loss: 0.045\n",
            "[85] loss: 0.046\n",
            "[86] loss: 0.045\n",
            "[87] loss: 0.046\n",
            "[88] loss: 0.043\n",
            "[89] loss: 0.046\n",
            "[90] loss: 0.045\n",
            "[91] loss: 0.045\n",
            "[92] loss: 0.045\n",
            "[93] loss: 0.043\n",
            "[94] loss: 0.044\n",
            "[95] loss: 0.045\n",
            "[96] loss: 0.043\n",
            "[97] loss: 0.044\n",
            "[98] loss: 0.044\n",
            "[99] loss: 0.043\n",
            "[100] loss: 0.044\n",
            "[101] loss: 0.044\n",
            "[102] loss: 0.042\n",
            "[103] loss: 0.043\n",
            "[104] loss: 0.041\n",
            "[105] loss: 0.041\n",
            "[106] loss: 0.041\n",
            "[107] loss: 0.041\n",
            "[108] loss: 0.041\n",
            "[109] loss: 0.041\n",
            "[110] loss: 0.041\n",
            "[111] loss: 0.043\n",
            "[112] loss: 0.041\n",
            "[113] loss: 0.040\n",
            "[114] loss: 0.042\n",
            "[115] loss: 0.043\n",
            "[116] loss: 0.040\n",
            "[117] loss: 0.040\n",
            "[118] loss: 0.040\n",
            "[119] loss: 0.041\n",
            "[120] loss: 0.040\n",
            "[121] loss: 0.041\n",
            "[122] loss: 0.040\n",
            "[123] loss: 0.041\n",
            "[124] loss: 0.039\n",
            "[125] loss: 0.040\n",
            "[126] loss: 0.039\n",
            "[127] loss: 0.039\n",
            "[128] loss: 0.040\n",
            "[129] loss: 0.039\n",
            "[130] loss: 0.038\n",
            "[131] loss: 0.038\n",
            "[132] loss: 0.038\n",
            "[133] loss: 0.039\n",
            "[134] loss: 0.038\n",
            "[135] loss: 0.040\n",
            "[136] loss: 0.038\n",
            "[137] loss: 0.038\n",
            "[138] loss: 0.038\n",
            "[139] loss: 0.038\n",
            "[140] loss: 0.038\n",
            "[141] loss: 0.037\n",
            "[142] loss: 0.039\n",
            "[143] loss: 0.037\n",
            "[144] loss: 0.037\n",
            "[145] loss: 0.036\n",
            "[146] loss: 0.039\n",
            "[147] loss: 0.036\n",
            "[148] loss: 0.036\n",
            "[149] loss: 0.037\n",
            "[150] loss: 0.035\n",
            "[151] loss: 0.036\n",
            "[152] loss: 0.038\n",
            "[153] loss: 0.037\n",
            "[154] loss: 0.035\n",
            "[155] loss: 0.036\n",
            "[156] loss: 0.037\n",
            "[157] loss: 0.037\n",
            "[158] loss: 0.035\n",
            "[159] loss: 0.037\n",
            "[160] loss: 0.036\n",
            "[161] loss: 0.036\n",
            "[162] loss: 0.037\n",
            "[163] loss: 0.035\n",
            "[164] loss: 0.036\n",
            "[165] loss: 0.035\n",
            "[166] loss: 0.036\n",
            "[167] loss: 0.035\n",
            "[168] loss: 0.036\n",
            "[169] loss: 0.036\n",
            "[170] loss: 0.035\n",
            "[171] loss: 0.035\n",
            "[172] loss: 0.035\n",
            "[173] loss: 0.034\n",
            "[174] loss: 0.036\n",
            "[175] loss: 0.034\n",
            "[176] loss: 0.033\n",
            "[177] loss: 0.034\n",
            "[178] loss: 0.036\n",
            "[179] loss: 0.034\n",
            "[180] loss: 0.035\n",
            "[181] loss: 0.034\n",
            "[182] loss: 0.034\n",
            "[183] loss: 0.033\n",
            "[184] loss: 0.033\n",
            "[185] loss: 0.035\n",
            "[186] loss: 0.035\n",
            "[187] loss: 0.035\n",
            "[188] loss: 0.034\n",
            "[189] loss: 0.034\n",
            "[190] loss: 0.034\n",
            "[191] loss: 0.033\n",
            "[192] loss: 0.034\n",
            "[193] loss: 0.032\n",
            "[194] loss: 0.034\n",
            "[195] loss: 0.033\n",
            "[196] loss: 0.033\n",
            "[197] loss: 0.035\n",
            "[198] loss: 0.033\n",
            "[199] loss: 0.033\n",
            "[200] loss: 0.033\n",
            "[201] loss: 0.032\n",
            "[202] loss: 0.033\n",
            "[203] loss: 0.032\n",
            "[204] loss: 0.033\n",
            "[205] loss: 0.033\n",
            "[206] loss: 0.033\n",
            "[207] loss: 0.033\n",
            "[208] loss: 0.034\n",
            "[209] loss: 0.032\n",
            "[210] loss: 0.032\n",
            "[211] loss: 0.032\n",
            "[212] loss: 0.032\n",
            "[213] loss: 0.031\n",
            "[214] loss: 0.030\n",
            "[215] loss: 0.033\n",
            "[216] loss: 0.031\n",
            "[217] loss: 0.031\n",
            "[218] loss: 0.032\n",
            "[219] loss: 0.030\n",
            "[220] loss: 0.031\n",
            "[221] loss: 0.030\n",
            "[222] loss: 0.031\n",
            "[223] loss: 0.031\n",
            "[224] loss: 0.030\n",
            "[225] loss: 0.032\n",
            "[226] loss: 0.031\n",
            "[227] loss: 0.030\n",
            "[228] loss: 0.030\n",
            "[229] loss: 0.031\n",
            "[230] loss: 0.031\n",
            "[231] loss: 0.030\n",
            "[232] loss: 0.029\n",
            "[233] loss: 0.030\n",
            "[234] loss: 0.030\n",
            "[235] loss: 0.031\n",
            "[236] loss: 0.030\n",
            "[237] loss: 0.030\n",
            "[238] loss: 0.030\n",
            "[239] loss: 0.029\n",
            "[240] loss: 0.030\n",
            "[241] loss: 0.030\n",
            "[242] loss: 0.030\n",
            "[243] loss: 0.031\n",
            "[244] loss: 0.031\n",
            "[245] loss: 0.032\n",
            "[246] loss: 0.029\n",
            "[247] loss: 0.029\n",
            "[248] loss: 0.030\n",
            "[249] loss: 0.029\n",
            "[250] loss: 0.030\n",
            "[251] loss: 0.029\n",
            "[252] loss: 0.029\n",
            "[253] loss: 0.029\n",
            "[254] loss: 0.029\n",
            "[255] loss: 0.030\n",
            "[256] loss: 0.028\n",
            "[257] loss: 0.028\n",
            "[258] loss: 0.030\n",
            "[259] loss: 0.030\n",
            "[260] loss: 0.030\n",
            "[261] loss: 0.028\n",
            "[262] loss: 0.029\n",
            "[263] loss: 0.030\n",
            "[264] loss: 0.029\n",
            "[265] loss: 0.028\n",
            "[266] loss: 0.030\n",
            "[267] loss: 0.030\n",
            "[268] loss: 0.030\n",
            "[269] loss: 0.029\n",
            "[270] loss: 0.029\n",
            "[271] loss: 0.030\n",
            "[272] loss: 0.030\n",
            "[273] loss: 0.029\n",
            "[274] loss: 0.030\n",
            "[275] loss: 0.029\n",
            "[276] loss: 0.029\n",
            "[277] loss: 0.028\n",
            "[278] loss: 0.029\n",
            "[279] loss: 0.028\n",
            "[280] loss: 0.030\n",
            "[281] loss: 0.029\n",
            "[282] loss: 0.029\n",
            "[283] loss: 0.029\n",
            "[284] loss: 0.029\n",
            "[285] loss: 0.028\n",
            "[286] loss: 0.027\n",
            "[287] loss: 0.028\n",
            "[288] loss: 0.028\n",
            "[289] loss: 0.028\n",
            "[290] loss: 0.029\n",
            "[291] loss: 0.028\n",
            "[292] loss: 0.029\n",
            "[293] loss: 0.027\n",
            "[294] loss: 0.029\n",
            "[295] loss: 0.029\n",
            "[296] loss: 0.029\n",
            "[297] loss: 0.028\n",
            "[298] loss: 0.027\n",
            "[299] loss: 0.029\n",
            "[300] loss: 0.029\n",
            "[301] loss: 0.030\n",
            "[302] loss: 0.029\n",
            "[303] loss: 0.029\n",
            "[304] loss: 0.028\n",
            "[305] loss: 0.027\n",
            "[306] loss: 0.027\n",
            "[307] loss: 0.028\n",
            "[308] loss: 0.028\n",
            "[309] loss: 0.028\n",
            "[310] loss: 0.029\n",
            "[311] loss: 0.027\n",
            "[312] loss: 0.027\n",
            "[313] loss: 0.029\n",
            "[314] loss: 0.028\n",
            "[315] loss: 0.028\n",
            "[316] loss: 0.027\n",
            "[317] loss: 0.028\n",
            "[318] loss: 0.027\n",
            "[319] loss: 0.028\n",
            "[320] loss: 0.028\n",
            "[321] loss: 0.027\n",
            "[322] loss: 0.027\n",
            "[323] loss: 0.028\n",
            "[324] loss: 0.027\n",
            "[325] loss: 0.027\n",
            "[326] loss: 0.027\n",
            "[327] loss: 0.028\n",
            "[328] loss: 0.027\n",
            "[329] loss: 0.028\n",
            "[330] loss: 0.026\n",
            "[331] loss: 0.026\n",
            "[332] loss: 0.026\n",
            "[333] loss: 0.026\n",
            "[334] loss: 0.026\n",
            "[335] loss: 0.027\n",
            "[336] loss: 0.028\n",
            "[337] loss: 0.026\n",
            "[338] loss: 0.027\n",
            "[339] loss: 0.028\n",
            "[340] loss: 0.027\n",
            "[341] loss: 0.027\n",
            "[342] loss: 0.026\n",
            "[343] loss: 0.028\n",
            "[344] loss: 0.027\n",
            "[345] loss: 0.027\n",
            "[346] loss: 0.027\n",
            "[347] loss: 0.027\n",
            "[348] loss: 0.027\n",
            "[349] loss: 0.027\n",
            "[350] loss: 0.025\n",
            "[351] loss: 0.026\n",
            "[352] loss: 0.027\n",
            "[353] loss: 0.027\n",
            "[354] loss: 0.028\n",
            "[355] loss: 0.027\n",
            "[356] loss: 0.026\n",
            "[357] loss: 0.027\n",
            "[358] loss: 0.026\n",
            "[359] loss: 0.025\n",
            "[360] loss: 0.025\n",
            "[361] loss: 0.026\n",
            "[362] loss: 0.027\n",
            "[363] loss: 0.026\n",
            "[364] loss: 0.026\n",
            "[365] loss: 0.027\n",
            "[366] loss: 0.027\n",
            "[367] loss: 0.025\n",
            "[368] loss: 0.026\n",
            "[369] loss: 0.026\n",
            "[370] loss: 0.027\n",
            "[371] loss: 0.026\n",
            "[372] loss: 0.026\n",
            "[373] loss: 0.026\n",
            "[374] loss: 0.025\n",
            "[375] loss: 0.026\n",
            "[376] loss: 0.026\n",
            "[377] loss: 0.026\n",
            "[378] loss: 0.025\n",
            "[379] loss: 0.026\n",
            "[380] loss: 0.027\n",
            "[381] loss: 0.026\n",
            "[382] loss: 0.025\n",
            "[383] loss: 0.026\n",
            "[384] loss: 0.027\n",
            "[385] loss: 0.027\n",
            "[386] loss: 0.027\n",
            "[387] loss: 0.025\n",
            "[388] loss: 0.026\n",
            "[389] loss: 0.026\n",
            "[390] loss: 0.026\n",
            "[391] loss: 0.025\n",
            "[392] loss: 0.026\n",
            "[393] loss: 0.027\n",
            "[394] loss: 0.026\n",
            "[395] loss: 0.024\n",
            "[396] loss: 0.025\n",
            "[397] loss: 0.026\n",
            "[398] loss: 0.025\n",
            "[399] loss: 0.026\n",
            "[400] loss: 0.024\n",
            "[401] loss: 0.025\n",
            "[402] loss: 0.025\n",
            "[403] loss: 0.024\n",
            "[404] loss: 0.025\n",
            "[405] loss: 0.026\n",
            "[406] loss: 0.026\n",
            "[407] loss: 0.024\n",
            "[408] loss: 0.025\n",
            "[409] loss: 0.025\n",
            "[410] loss: 0.025\n",
            "[411] loss: 0.025\n",
            "[412] loss: 0.025\n",
            "[413] loss: 0.024\n",
            "[414] loss: 0.025\n",
            "[415] loss: 0.024\n",
            "[416] loss: 0.024\n",
            "[417] loss: 0.025\n",
            "[418] loss: 0.024\n",
            "[419] loss: 0.025\n",
            "[420] loss: 0.026\n",
            "[421] loss: 0.025\n",
            "[422] loss: 0.024\n",
            "[423] loss: 0.025\n",
            "[424] loss: 0.025\n",
            "[425] loss: 0.023\n",
            "[426] loss: 0.025\n",
            "[427] loss: 0.024\n",
            "[428] loss: 0.024\n",
            "[429] loss: 0.024\n",
            "[430] loss: 0.024\n",
            "[431] loss: 0.024\n",
            "[432] loss: 0.024\n",
            "[433] loss: 0.024\n",
            "[434] loss: 0.024\n",
            "[435] loss: 0.024\n",
            "[436] loss: 0.024\n",
            "[437] loss: 0.023\n",
            "[438] loss: 0.023\n",
            "[439] loss: 0.024\n",
            "[440] loss: 0.023\n",
            "[441] loss: 0.024\n",
            "[442] loss: 0.023\n",
            "[443] loss: 0.024\n",
            "[444] loss: 0.024\n",
            "[445] loss: 0.024\n",
            "[446] loss: 0.023\n",
            "[447] loss: 0.023\n",
            "[448] loss: 0.024\n",
            "[449] loss: 0.024\n",
            "[450] loss: 0.023\n",
            "[451] loss: 0.024\n",
            "[452] loss: 0.024\n",
            "[453] loss: 0.023\n",
            "[454] loss: 0.024\n",
            "[455] loss: 0.024\n",
            "[456] loss: 0.023\n",
            "[457] loss: 0.023\n",
            "[458] loss: 0.023\n",
            "[459] loss: 0.023\n",
            "[460] loss: 0.024\n",
            "[461] loss: 0.024\n",
            "[462] loss: 0.023\n",
            "[463] loss: 0.025\n",
            "[464] loss: 0.024\n",
            "[465] loss: 0.023\n",
            "[466] loss: 0.024\n",
            "[467] loss: 0.024\n",
            "[468] loss: 0.023\n",
            "[469] loss: 0.024\n",
            "[470] loss: 0.024\n",
            "[471] loss: 0.024\n",
            "[472] loss: 0.023\n",
            "[473] loss: 0.025\n",
            "[474] loss: 0.024\n",
            "[475] loss: 0.023\n",
            "[476] loss: 0.023\n",
            "[477] loss: 0.022\n",
            "[478] loss: 0.024\n",
            "[479] loss: 0.024\n",
            "[480] loss: 0.022\n",
            "[481] loss: 0.022\n",
            "[482] loss: 0.023\n",
            "[483] loss: 0.024\n",
            "[484] loss: 0.024\n",
            "[485] loss: 0.023\n",
            "[486] loss: 0.024\n",
            "[487] loss: 0.022\n",
            "[488] loss: 0.023\n",
            "[489] loss: 0.024\n",
            "[490] loss: 0.024\n",
            "[491] loss: 0.023\n",
            "[492] loss: 0.023\n",
            "[493] loss: 0.023\n",
            "[494] loss: 0.023\n",
            "[495] loss: 0.022\n",
            "[496] loss: 0.023\n",
            "[497] loss: 0.022\n",
            "[498] loss: 0.023\n",
            "[499] loss: 0.024\n",
            "[500] loss: 0.023\n",
            "[501] loss: 0.023\n",
            "[502] loss: 0.023\n",
            "[503] loss: 0.023\n",
            "[504] loss: 0.021\n",
            "[505] loss: 0.024\n",
            "[506] loss: 0.023\n",
            "[507] loss: 0.022\n",
            "[508] loss: 0.023\n",
            "[509] loss: 0.023\n",
            "[510] loss: 0.023\n",
            "[511] loss: 0.023\n",
            "[512] loss: 0.023\n",
            "[513] loss: 0.022\n",
            "[514] loss: 0.023\n",
            "[515] loss: 0.023\n",
            "[516] loss: 0.022\n",
            "[517] loss: 0.022\n",
            "[518] loss: 0.024\n",
            "[519] loss: 0.023\n",
            "[520] loss: 0.023\n",
            "[521] loss: 0.023\n",
            "[522] loss: 0.022\n",
            "[523] loss: 0.023\n",
            "[524] loss: 0.023\n",
            "[525] loss: 0.023\n",
            "[526] loss: 0.023\n",
            "[527] loss: 0.021\n",
            "[528] loss: 0.024\n",
            "[529] loss: 0.022\n",
            "[530] loss: 0.022\n",
            "[531] loss: 0.024\n",
            "[532] loss: 0.023\n",
            "[533] loss: 0.024\n",
            "[534] loss: 0.024\n",
            "[535] loss: 0.022\n",
            "[536] loss: 0.021\n",
            "[537] loss: 0.022\n",
            "[538] loss: 0.022\n",
            "[539] loss: 0.022\n",
            "[540] loss: 0.023\n",
            "[541] loss: 0.023\n",
            "[542] loss: 0.022\n",
            "[543] loss: 0.023\n",
            "[544] loss: 0.022\n",
            "[545] loss: 0.022\n",
            "[546] loss: 0.022\n",
            "[547] loss: 0.023\n",
            "[548] loss: 0.022\n",
            "[549] loss: 0.023\n",
            "[550] loss: 0.023\n",
            "[551] loss: 0.023\n",
            "[552] loss: 0.022\n",
            "[553] loss: 0.022\n",
            "[554] loss: 0.023\n",
            "[555] loss: 0.022\n",
            "[556] loss: 0.022\n",
            "[557] loss: 0.022\n",
            "[558] loss: 0.022\n",
            "[559] loss: 0.023\n",
            "[560] loss: 0.022\n",
            "[561] loss: 0.022\n",
            "[562] loss: 0.023\n",
            "[563] loss: 0.022\n",
            "[564] loss: 0.022\n",
            "[565] loss: 0.022\n",
            "[566] loss: 0.022\n",
            "[567] loss: 0.022\n",
            "[568] loss: 0.023\n",
            "[569] loss: 0.023\n",
            "[570] loss: 0.021\n",
            "[571] loss: 0.022\n",
            "[572] loss: 0.022\n",
            "[573] loss: 0.022\n",
            "[574] loss: 0.022\n",
            "[575] loss: 0.022\n",
            "[576] loss: 0.022\n",
            "[577] loss: 0.022\n",
            "[578] loss: 0.022\n",
            "[579] loss: 0.021\n",
            "[580] loss: 0.021\n",
            "[581] loss: 0.021\n",
            "[582] loss: 0.023\n",
            "[583] loss: 0.021\n",
            "[584] loss: 0.022\n",
            "[585] loss: 0.022\n",
            "[586] loss: 0.022\n",
            "[587] loss: 0.023\n",
            "[588] loss: 0.022\n",
            "[589] loss: 0.022\n",
            "[590] loss: 0.021\n",
            "[591] loss: 0.023\n",
            "[592] loss: 0.022\n",
            "[593] loss: 0.022\n",
            "[594] loss: 0.021\n",
            "[595] loss: 0.023\n",
            "[596] loss: 0.022\n",
            "[597] loss: 0.021\n",
            "[598] loss: 0.022\n",
            "[599] loss: 0.022\n",
            "[600] loss: 0.021\n",
            "[601] loss: 0.022\n",
            "[602] loss: 0.022\n",
            "[603] loss: 0.022\n",
            "[604] loss: 0.021\n",
            "[605] loss: 0.021\n",
            "[606] loss: 0.021\n",
            "[607] loss: 0.020\n",
            "[608] loss: 0.021\n",
            "[609] loss: 0.022\n",
            "[610] loss: 0.023\n",
            "[611] loss: 0.022\n",
            "[612] loss: 0.021\n",
            "[613] loss: 0.022\n",
            "[614] loss: 0.022\n",
            "[615] loss: 0.021\n",
            "[616] loss: 0.021\n",
            "[617] loss: 0.021\n",
            "[618] loss: 0.022\n",
            "[619] loss: 0.020\n",
            "[620] loss: 0.021\n",
            "[621] loss: 0.020\n",
            "[622] loss: 0.021\n",
            "[623] loss: 0.023\n",
            "[624] loss: 0.021\n",
            "[625] loss: 0.021\n",
            "[626] loss: 0.020\n",
            "[627] loss: 0.021\n",
            "[628] loss: 0.021\n",
            "[629] loss: 0.020\n",
            "[630] loss: 0.021\n",
            "[631] loss: 0.021\n",
            "[632] loss: 0.020\n",
            "[633] loss: 0.021\n",
            "[634] loss: 0.020\n",
            "[635] loss: 0.021\n",
            "[636] loss: 0.020\n",
            "[637] loss: 0.021\n",
            "[638] loss: 0.020\n",
            "[639] loss: 0.021\n",
            "[640] loss: 0.020\n",
            "[641] loss: 0.022\n",
            "[642] loss: 0.021\n",
            "[643] loss: 0.020\n",
            "[644] loss: 0.021\n",
            "[645] loss: 0.020\n",
            "[646] loss: 0.020\n",
            "[647] loss: 0.021\n",
            "[648] loss: 0.021\n",
            "[649] loss: 0.020\n",
            "[650] loss: 0.021\n",
            "[651] loss: 0.020\n",
            "[652] loss: 0.020\n",
            "[653] loss: 0.020\n",
            "[654] loss: 0.021\n",
            "[655] loss: 0.020\n",
            "[656] loss: 0.020\n",
            "[657] loss: 0.021\n",
            "[658] loss: 0.020\n",
            "[659] loss: 0.021\n",
            "[660] loss: 0.019\n",
            "[661] loss: 0.021\n",
            "[662] loss: 0.020\n",
            "[663] loss: 0.020\n",
            "[664] loss: 0.021\n",
            "[665] loss: 0.020\n",
            "[666] loss: 0.020\n",
            "[667] loss: 0.020\n",
            "[668] loss: 0.020\n",
            "[669] loss: 0.020\n",
            "[670] loss: 0.020\n",
            "[671] loss: 0.020\n",
            "[672] loss: 0.021\n",
            "[673] loss: 0.021\n",
            "[674] loss: 0.021\n",
            "[675] loss: 0.021\n",
            "[676] loss: 0.021\n",
            "[677] loss: 0.020\n",
            "[678] loss: 0.019\n",
            "[679] loss: 0.019\n",
            "[680] loss: 0.020\n",
            "[681] loss: 0.021\n",
            "[682] loss: 0.021\n",
            "[683] loss: 0.020\n",
            "[684] loss: 0.021\n",
            "[685] loss: 0.021\n",
            "[686] loss: 0.021\n",
            "[687] loss: 0.019\n",
            "[688] loss: 0.021\n",
            "[689] loss: 0.019\n",
            "[690] loss: 0.020\n",
            "[691] loss: 0.019\n",
            "[692] loss: 0.021\n",
            "[693] loss: 0.020\n",
            "[694] loss: 0.021\n",
            "[695] loss: 0.018\n",
            "[696] loss: 0.020\n",
            "[697] loss: 0.020\n",
            "[698] loss: 0.020\n",
            "[699] loss: 0.020\n",
            "[700] loss: 0.019\n",
            "[701] loss: 0.020\n",
            "[702] loss: 0.020\n",
            "[703] loss: 0.020\n",
            "[704] loss: 0.019\n",
            "[705] loss: 0.019\n",
            "[706] loss: 0.019\n",
            "[707] loss: 0.020\n",
            "[708] loss: 0.020\n",
            "[709] loss: 0.020\n",
            "[710] loss: 0.018\n",
            "[711] loss: 0.021\n",
            "[712] loss: 0.019\n",
            "[713] loss: 0.020\n",
            "[714] loss: 0.020\n",
            "[715] loss: 0.020\n",
            "[716] loss: 0.020\n",
            "[717] loss: 0.020\n",
            "[718] loss: 0.020\n",
            "[719] loss: 0.019\n",
            "[720] loss: 0.020\n",
            "[721] loss: 0.021\n",
            "[722] loss: 0.020\n",
            "[723] loss: 0.020\n",
            "[724] loss: 0.021\n",
            "[725] loss: 0.020\n",
            "[726] loss: 0.020\n",
            "[727] loss: 0.020\n",
            "[728] loss: 0.020\n",
            "[729] loss: 0.020\n",
            "[730] loss: 0.019\n",
            "[731] loss: 0.020\n",
            "[732] loss: 0.020\n",
            "[733] loss: 0.020\n",
            "[734] loss: 0.020\n",
            "[735] loss: 0.020\n",
            "[736] loss: 0.020\n",
            "[737] loss: 0.019\n",
            "[738] loss: 0.019\n",
            "[739] loss: 0.019\n",
            "[740] loss: 0.021\n",
            "[741] loss: 0.019\n",
            "[742] loss: 0.019\n",
            "[743] loss: 0.021\n",
            "[744] loss: 0.021\n",
            "[745] loss: 0.018\n",
            "[746] loss: 0.020\n",
            "[747] loss: 0.019\n",
            "[748] loss: 0.020\n",
            "[749] loss: 0.020\n",
            "[750] loss: 0.020\n",
            "[751] loss: 0.020\n",
            "[752] loss: 0.019\n",
            "[753] loss: 0.021\n",
            "[754] loss: 0.020\n",
            "[755] loss: 0.019\n",
            "[756] loss: 0.019\n",
            "[757] loss: 0.019\n",
            "[758] loss: 0.019\n",
            "[759] loss: 0.020\n",
            "[760] loss: 0.019\n",
            "[761] loss: 0.020\n",
            "[762] loss: 0.020\n",
            "[763] loss: 0.019\n",
            "[764] loss: 0.018\n",
            "[765] loss: 0.019\n",
            "[766] loss: 0.019\n",
            "[767] loss: 0.019\n",
            "[768] loss: 0.019\n",
            "[769] loss: 0.020\n",
            "[770] loss: 0.020\n",
            "[771] loss: 0.020\n",
            "[772] loss: 0.019\n",
            "[773] loss: 0.019\n",
            "[774] loss: 0.019\n",
            "[775] loss: 0.018\n",
            "[776] loss: 0.018\n",
            "[777] loss: 0.019\n",
            "[778] loss: 0.018\n",
            "[779] loss: 0.019\n",
            "[780] loss: 0.019\n",
            "[781] loss: 0.019\n",
            "[782] loss: 0.018\n",
            "[783] loss: 0.018\n",
            "[784] loss: 0.020\n",
            "[785] loss: 0.018\n",
            "[786] loss: 0.021\n",
            "[787] loss: 0.019\n",
            "[788] loss: 0.020\n",
            "[789] loss: 0.020\n",
            "[790] loss: 0.020\n",
            "[791] loss: 0.019\n",
            "[792] loss: 0.019\n",
            "[793] loss: 0.019\n",
            "[794] loss: 0.018\n",
            "[795] loss: 0.019\n",
            "[796] loss: 0.018\n",
            "[797] loss: 0.020\n",
            "[798] loss: 0.020\n",
            "[799] loss: 0.019\n",
            "[800] loss: 0.019\n",
            "[801] loss: 0.020\n",
            "[802] loss: 0.018\n",
            "[803] loss: 0.018\n",
            "[804] loss: 0.019\n",
            "[805] loss: 0.019\n",
            "[806] loss: 0.017\n",
            "[807] loss: 0.018\n",
            "[808] loss: 0.018\n",
            "[809] loss: 0.018\n",
            "[810] loss: 0.018\n",
            "[811] loss: 0.019\n",
            "[812] loss: 0.019\n",
            "[813] loss: 0.019\n",
            "[814] loss: 0.019\n",
            "[815] loss: 0.018\n",
            "[816] loss: 0.018\n",
            "[817] loss: 0.019\n",
            "[818] loss: 0.019\n",
            "[819] loss: 0.018\n",
            "[820] loss: 0.019\n",
            "[821] loss: 0.019\n",
            "[822] loss: 0.018\n",
            "[823] loss: 0.019\n",
            "[824] loss: 0.018\n",
            "[825] loss: 0.018\n",
            "[826] loss: 0.018\n",
            "[827] loss: 0.020\n",
            "[828] loss: 0.019\n",
            "[829] loss: 0.018\n",
            "[830] loss: 0.019\n",
            "[831] loss: 0.017\n",
            "[832] loss: 0.018\n",
            "[833] loss: 0.018\n",
            "[834] loss: 0.018\n",
            "[835] loss: 0.017\n",
            "[836] loss: 0.018\n",
            "[837] loss: 0.018\n",
            "[838] loss: 0.019\n",
            "[839] loss: 0.018\n",
            "[840] loss: 0.019\n",
            "[841] loss: 0.018\n",
            "[842] loss: 0.018\n",
            "[843] loss: 0.018\n",
            "[844] loss: 0.018\n",
            "[845] loss: 0.019\n",
            "[846] loss: 0.019\n",
            "[847] loss: 0.018\n",
            "[848] loss: 0.018\n",
            "[849] loss: 0.019\n",
            "[850] loss: 0.018\n",
            "[851] loss: 0.017\n",
            "[852] loss: 0.019\n",
            "[853] loss: 0.018\n",
            "[854] loss: 0.018\n",
            "[855] loss: 0.018\n",
            "[856] loss: 0.019\n",
            "[857] loss: 0.019\n",
            "[858] loss: 0.018\n",
            "[859] loss: 0.018\n",
            "[860] loss: 0.018\n",
            "[861] loss: 0.017\n",
            "[862] loss: 0.019\n",
            "[863] loss: 0.018\n",
            "[864] loss: 0.019\n",
            "[865] loss: 0.018\n",
            "[866] loss: 0.018\n",
            "[867] loss: 0.018\n",
            "[868] loss: 0.018\n",
            "[869] loss: 0.017\n",
            "[870] loss: 0.018\n",
            "[871] loss: 0.018\n",
            "[872] loss: 0.018\n",
            "[873] loss: 0.018\n",
            "[874] loss: 0.018\n",
            "[875] loss: 0.018\n",
            "[876] loss: 0.019\n",
            "[877] loss: 0.018\n",
            "[878] loss: 0.018\n",
            "[879] loss: 0.018\n",
            "[880] loss: 0.017\n",
            "[881] loss: 0.017\n",
            "[882] loss: 0.019\n",
            "[883] loss: 0.019\n",
            "[884] loss: 0.018\n",
            "[885] loss: 0.018\n",
            "[886] loss: 0.017\n",
            "[887] loss: 0.018\n",
            "[888] loss: 0.017\n",
            "[889] loss: 0.018\n",
            "[890] loss: 0.018\n",
            "[891] loss: 0.018\n",
            "[892] loss: 0.018\n",
            "[893] loss: 0.017\n",
            "[894] loss: 0.018\n",
            "[895] loss: 0.018\n",
            "[896] loss: 0.018\n",
            "[897] loss: 0.018\n",
            "[898] loss: 0.018\n",
            "[899] loss: 0.018\n",
            "[900] loss: 0.018\n",
            "[901] loss: 0.018\n",
            "[902] loss: 0.018\n",
            "[903] loss: 0.018\n",
            "[904] loss: 0.018\n",
            "[905] loss: 0.018\n",
            "[906] loss: 0.018\n",
            "[907] loss: 0.018\n",
            "[908] loss: 0.018\n",
            "[909] loss: 0.017\n",
            "[910] loss: 0.019\n",
            "[911] loss: 0.018\n",
            "[912] loss: 0.018\n",
            "[913] loss: 0.017\n",
            "[914] loss: 0.017\n",
            "[915] loss: 0.018\n",
            "[916] loss: 0.019\n",
            "[917] loss: 0.018\n",
            "[918] loss: 0.017\n",
            "[919] loss: 0.018\n",
            "[920] loss: 0.018\n",
            "[921] loss: 0.018\n",
            "[922] loss: 0.018\n",
            "[923] loss: 0.018\n",
            "[924] loss: 0.017\n",
            "[925] loss: 0.018\n",
            "[926] loss: 0.017\n",
            "[927] loss: 0.017\n",
            "[928] loss: 0.017\n",
            "[929] loss: 0.017\n",
            "[930] loss: 0.018\n",
            "[931] loss: 0.018\n",
            "[932] loss: 0.018\n",
            "[933] loss: 0.018\n",
            "[934] loss: 0.017\n",
            "[935] loss: 0.018\n",
            "[936] loss: 0.018\n",
            "[937] loss: 0.018\n",
            "[938] loss: 0.017\n",
            "[939] loss: 0.019\n",
            "[940] loss: 0.018\n",
            "[941] loss: 0.018\n",
            "[942] loss: 0.017\n",
            "[943] loss: 0.017\n",
            "[944] loss: 0.017\n",
            "[945] loss: 0.018\n",
            "[946] loss: 0.017\n",
            "[947] loss: 0.018\n",
            "[948] loss: 0.018\n",
            "[949] loss: 0.017\n",
            "[950] loss: 0.017\n",
            "[951] loss: 0.018\n",
            "[952] loss: 0.017\n",
            "[953] loss: 0.016\n",
            "[954] loss: 0.018\n",
            "[955] loss: 0.018\n",
            "[956] loss: 0.018\n",
            "[957] loss: 0.018\n",
            "[958] loss: 0.018\n",
            "[959] loss: 0.018\n",
            "[960] loss: 0.017\n",
            "[961] loss: 0.018\n",
            "[962] loss: 0.017\n",
            "[963] loss: 0.018\n",
            "[964] loss: 0.017\n",
            "[965] loss: 0.018\n",
            "[966] loss: 0.019\n",
            "[967] loss: 0.017\n",
            "[968] loss: 0.019\n",
            "[969] loss: 0.018\n",
            "[970] loss: 0.018\n",
            "[971] loss: 0.019\n",
            "[972] loss: 0.018\n",
            "[973] loss: 0.018\n",
            "[974] loss: 0.018\n",
            "[975] loss: 0.017\n",
            "[976] loss: 0.018\n",
            "[977] loss: 0.017\n",
            "[978] loss: 0.017\n",
            "[979] loss: 0.017\n",
            "[980] loss: 0.018\n",
            "[981] loss: 0.018\n",
            "[982] loss: 0.017\n",
            "[983] loss: 0.017\n",
            "[984] loss: 0.018\n",
            "[985] loss: 0.018\n",
            "[986] loss: 0.017\n",
            "[987] loss: 0.018\n",
            "[988] loss: 0.017\n",
            "[989] loss: 0.017\n",
            "[990] loss: 0.017\n",
            "[991] loss: 0.017\n",
            "[992] loss: 0.018\n",
            "[993] loss: 0.018\n",
            "[994] loss: 0.017\n",
            "[995] loss: 0.018\n",
            "[996] loss: 0.017\n",
            "[997] loss: 0.017\n",
            "[998] loss: 0.016\n",
            "[999] loss: 0.017\n",
            "[1000] loss: 0.016\n",
            "[1001] loss: 0.018\n",
            "[1002] loss: 0.017\n",
            "[1003] loss: 0.017\n",
            "[1004] loss: 0.017\n",
            "[1005] loss: 0.016\n",
            "[1006] loss: 0.017\n",
            "[1007] loss: 0.017\n",
            "[1008] loss: 0.017\n",
            "[1009] loss: 0.016\n",
            "[1010] loss: 0.017\n",
            "[1011] loss: 0.017\n",
            "[1012] loss: 0.016\n",
            "[1013] loss: 0.017\n",
            "[1014] loss: 0.017\n",
            "[1015] loss: 0.017\n",
            "[1016] loss: 0.017\n",
            "[1017] loss: 0.017\n",
            "[1018] loss: 0.018\n",
            "[1019] loss: 0.017\n",
            "[1020] loss: 0.017\n",
            "[1021] loss: 0.016\n",
            "[1022] loss: 0.017\n",
            "[1023] loss: 0.016\n",
            "[1024] loss: 0.018\n",
            "[1025] loss: 0.016\n",
            "[1026] loss: 0.016\n",
            "[1027] loss: 0.016\n",
            "[1028] loss: 0.017\n",
            "[1029] loss: 0.017\n",
            "[1030] loss: 0.016\n",
            "[1031] loss: 0.017\n",
            "[1032] loss: 0.016\n",
            "[1033] loss: 0.016\n",
            "[1034] loss: 0.017\n",
            "[1035] loss: 0.017\n",
            "[1036] loss: 0.016\n",
            "[1037] loss: 0.017\n",
            "[1038] loss: 0.017\n",
            "[1039] loss: 0.017\n",
            "[1040] loss: 0.017\n",
            "[1041] loss: 0.016\n",
            "[1042] loss: 0.016\n",
            "[1043] loss: 0.017\n",
            "[1044] loss: 0.017\n",
            "[1045] loss: 0.015\n",
            "[1046] loss: 0.017\n",
            "[1047] loss: 0.017\n",
            "[1048] loss: 0.016\n",
            "[1049] loss: 0.015\n",
            "[1050] loss: 0.016\n",
            "[1051] loss: 0.017\n",
            "[1052] loss: 0.017\n",
            "[1053] loss: 0.017\n",
            "[1054] loss: 0.016\n",
            "[1055] loss: 0.017\n",
            "[1056] loss: 0.016\n",
            "[1057] loss: 0.017\n",
            "[1058] loss: 0.016\n",
            "[1059] loss: 0.016\n",
            "[1060] loss: 0.016\n",
            "[1061] loss: 0.016\n",
            "[1062] loss: 0.017\n",
            "[1063] loss: 0.017\n",
            "[1064] loss: 0.016\n",
            "[1065] loss: 0.017\n",
            "[1066] loss: 0.016\n",
            "[1067] loss: 0.017\n",
            "[1068] loss: 0.017\n",
            "[1069] loss: 0.016\n",
            "[1070] loss: 0.016\n",
            "[1071] loss: 0.016\n",
            "[1072] loss: 0.015\n",
            "[1073] loss: 0.016\n",
            "[1074] loss: 0.016\n",
            "[1075] loss: 0.016\n",
            "[1076] loss: 0.017\n",
            "[1077] loss: 0.016\n",
            "[1078] loss: 0.016\n",
            "[1079] loss: 0.016\n",
            "[1080] loss: 0.016\n",
            "[1081] loss: 0.016\n",
            "[1082] loss: 0.017\n",
            "[1083] loss: 0.016\n",
            "[1084] loss: 0.016\n",
            "[1085] loss: 0.015\n",
            "[1086] loss: 0.017\n",
            "[1087] loss: 0.017\n",
            "[1088] loss: 0.016\n",
            "[1089] loss: 0.016\n",
            "[1090] loss: 0.016\n",
            "[1091] loss: 0.017\n",
            "[1092] loss: 0.017\n",
            "[1093] loss: 0.016\n",
            "[1094] loss: 0.016\n",
            "[1095] loss: 0.017\n",
            "[1096] loss: 0.016\n",
            "[1097] loss: 0.015\n",
            "[1098] loss: 0.016\n",
            "[1099] loss: 0.016\n",
            "[1100] loss: 0.016\n",
            "[1101] loss: 0.016\n",
            "[1102] loss: 0.016\n",
            "[1103] loss: 0.017\n",
            "[1104] loss: 0.016\n",
            "[1105] loss: 0.016\n",
            "[1106] loss: 0.017\n",
            "[1107] loss: 0.015\n",
            "[1108] loss: 0.016\n",
            "[1109] loss: 0.016\n",
            "[1110] loss: 0.016\n",
            "[1111] loss: 0.016\n",
            "[1112] loss: 0.016\n",
            "[1113] loss: 0.016\n",
            "[1114] loss: 0.016\n",
            "[1115] loss: 0.016\n",
            "[1116] loss: 0.016\n",
            "[1117] loss: 0.016\n",
            "[1118] loss: 0.016\n",
            "[1119] loss: 0.017\n",
            "[1120] loss: 0.017\n",
            "[1121] loss: 0.017\n",
            "[1122] loss: 0.017\n",
            "[1123] loss: 0.016\n",
            "[1124] loss: 0.016\n",
            "[1125] loss: 0.016\n",
            "[1126] loss: 0.016\n",
            "[1127] loss: 0.015\n",
            "[1128] loss: 0.015\n",
            "[1129] loss: 0.016\n",
            "[1130] loss: 0.017\n",
            "[1131] loss: 0.016\n",
            "[1132] loss: 0.016\n",
            "[1133] loss: 0.016\n",
            "[1134] loss: 0.016\n",
            "[1135] loss: 0.016\n",
            "[1136] loss: 0.016\n",
            "[1137] loss: 0.016\n",
            "[1138] loss: 0.016\n",
            "[1139] loss: 0.015\n",
            "[1140] loss: 0.016\n",
            "[1141] loss: 0.017\n",
            "[1142] loss: 0.016\n",
            "[1143] loss: 0.016\n",
            "[1144] loss: 0.017\n",
            "[1145] loss: 0.016\n",
            "[1146] loss: 0.016\n",
            "[1147] loss: 0.015\n",
            "[1148] loss: 0.016\n",
            "[1149] loss: 0.016\n",
            "[1150] loss: 0.015\n",
            "[1151] loss: 0.016\n",
            "[1152] loss: 0.016\n",
            "[1153] loss: 0.016\n",
            "[1154] loss: 0.017\n",
            "[1155] loss: 0.016\n",
            "[1156] loss: 0.015\n",
            "[1157] loss: 0.016\n",
            "[1158] loss: 0.016\n",
            "[1159] loss: 0.017\n",
            "[1160] loss: 0.016\n",
            "[1161] loss: 0.016\n",
            "[1162] loss: 0.016\n",
            "[1163] loss: 0.016\n",
            "[1164] loss: 0.016\n",
            "[1165] loss: 0.016\n",
            "[1166] loss: 0.016\n",
            "[1167] loss: 0.016\n",
            "[1168] loss: 0.015\n",
            "[1169] loss: 0.016\n",
            "[1170] loss: 0.017\n",
            "[1171] loss: 0.017\n",
            "[1172] loss: 0.015\n",
            "[1173] loss: 0.016\n",
            "[1174] loss: 0.016\n",
            "[1175] loss: 0.016\n",
            "[1176] loss: 0.017\n",
            "[1177] loss: 0.017\n",
            "[1178] loss: 0.016\n",
            "[1179] loss: 0.016\n",
            "[1180] loss: 0.016\n",
            "[1181] loss: 0.015\n",
            "[1182] loss: 0.016\n",
            "[1183] loss: 0.016\n",
            "[1184] loss: 0.017\n",
            "[1185] loss: 0.016\n",
            "[1186] loss: 0.015\n",
            "[1187] loss: 0.016\n",
            "[1188] loss: 0.016\n",
            "[1189] loss: 0.016\n",
            "[1190] loss: 0.015\n",
            "[1191] loss: 0.016\n",
            "[1192] loss: 0.016\n",
            "[1193] loss: 0.015\n",
            "[1194] loss: 0.015\n",
            "[1195] loss: 0.015\n",
            "[1196] loss: 0.016\n",
            "[1197] loss: 0.016\n",
            "[1198] loss: 0.016\n",
            "[1199] loss: 0.016\n",
            "[1200] loss: 0.015\n",
            "[1201] loss: 0.014\n",
            "[1202] loss: 0.015\n",
            "[1203] loss: 0.016\n",
            "[1204] loss: 0.015\n",
            "[1205] loss: 0.015\n",
            "[1206] loss: 0.016\n",
            "[1207] loss: 0.014\n",
            "[1208] loss: 0.015\n",
            "[1209] loss: 0.016\n",
            "[1210] loss: 0.016\n",
            "[1211] loss: 0.015\n",
            "[1212] loss: 0.015\n",
            "[1213] loss: 0.015\n",
            "[1214] loss: 0.015\n",
            "[1215] loss: 0.015\n",
            "[1216] loss: 0.015\n",
            "[1217] loss: 0.017\n",
            "[1218] loss: 0.015\n",
            "[1219] loss: 0.016\n",
            "[1220] loss: 0.015\n",
            "[1221] loss: 0.015\n",
            "[1222] loss: 0.016\n",
            "[1223] loss: 0.015\n",
            "[1224] loss: 0.016\n",
            "[1225] loss: 0.016\n",
            "[1226] loss: 0.016\n",
            "[1227] loss: 0.014\n",
            "[1228] loss: 0.014\n",
            "[1229] loss: 0.015\n",
            "[1230] loss: 0.015\n",
            "[1231] loss: 0.015\n",
            "[1232] loss: 0.016\n",
            "[1233] loss: 0.015\n",
            "[1234] loss: 0.016\n",
            "[1235] loss: 0.015\n",
            "[1236] loss: 0.015\n",
            "[1237] loss: 0.015\n",
            "[1238] loss: 0.016\n",
            "[1239] loss: 0.016\n",
            "[1240] loss: 0.015\n",
            "[1241] loss: 0.015\n",
            "[1242] loss: 0.015\n",
            "[1243] loss: 0.015\n",
            "[1244] loss: 0.015\n",
            "[1245] loss: 0.015\n",
            "[1246] loss: 0.015\n",
            "[1247] loss: 0.015\n",
            "[1248] loss: 0.015\n",
            "[1249] loss: 0.015\n",
            "[1250] loss: 0.014\n",
            "[1251] loss: 0.016\n",
            "[1252] loss: 0.015\n",
            "[1253] loss: 0.015\n",
            "[1254] loss: 0.015\n",
            "[1255] loss: 0.015\n",
            "[1256] loss: 0.015\n",
            "[1257] loss: 0.015\n",
            "[1258] loss: 0.015\n",
            "[1259] loss: 0.016\n",
            "[1260] loss: 0.015\n",
            "[1261] loss: 0.015\n",
            "[1262] loss: 0.015\n",
            "[1263] loss: 0.016\n",
            "[1264] loss: 0.015\n",
            "[1265] loss: 0.015\n",
            "[1266] loss: 0.015\n",
            "[1267] loss: 0.015\n",
            "[1268] loss: 0.015\n",
            "[1269] loss: 0.014\n",
            "[1270] loss: 0.015\n",
            "[1271] loss: 0.014\n",
            "[1272] loss: 0.014\n",
            "[1273] loss: 0.015\n",
            "[1274] loss: 0.015\n",
            "[1275] loss: 0.015\n",
            "[1276] loss: 0.015\n",
            "[1277] loss: 0.014\n",
            "[1278] loss: 0.014\n",
            "[1279] loss: 0.016\n",
            "[1280] loss: 0.015\n",
            "[1281] loss: 0.015\n",
            "[1282] loss: 0.016\n",
            "[1283] loss: 0.015\n",
            "[1284] loss: 0.015\n",
            "[1285] loss: 0.015\n",
            "[1286] loss: 0.015\n",
            "[1287] loss: 0.014\n",
            "[1288] loss: 0.015\n",
            "[1289] loss: 0.015\n",
            "[1290] loss: 0.015\n",
            "[1291] loss: 0.015\n",
            "[1292] loss: 0.016\n",
            "[1293] loss: 0.014\n",
            "[1294] loss: 0.015\n",
            "[1295] loss: 0.015\n",
            "[1296] loss: 0.016\n",
            "[1297] loss: 0.016\n",
            "[1298] loss: 0.015\n",
            "[1299] loss: 0.016\n",
            "[1300] loss: 0.015\n",
            "[1301] loss: 0.016\n",
            "[1302] loss: 0.015\n",
            "[1303] loss: 0.015\n",
            "[1304] loss: 0.016\n",
            "[1305] loss: 0.015\n",
            "[1306] loss: 0.015\n",
            "[1307] loss: 0.014\n",
            "[1308] loss: 0.014\n",
            "[1309] loss: 0.015\n",
            "[1310] loss: 0.015\n",
            "[1311] loss: 0.015\n",
            "[1312] loss: 0.015\n",
            "[1313] loss: 0.015\n",
            "[1314] loss: 0.015\n",
            "[1315] loss: 0.015\n",
            "[1316] loss: 0.014\n",
            "[1317] loss: 0.014\n",
            "[1318] loss: 0.015\n",
            "[1319] loss: 0.015\n",
            "[1320] loss: 0.015\n",
            "[1321] loss: 0.015\n",
            "[1322] loss: 0.015\n",
            "[1323] loss: 0.014\n",
            "[1324] loss: 0.014\n",
            "[1325] loss: 0.016\n",
            "[1326] loss: 0.015\n",
            "[1327] loss: 0.015\n",
            "[1328] loss: 0.015\n",
            "[1329] loss: 0.015\n",
            "[1330] loss: 0.015\n",
            "[1331] loss: 0.015\n",
            "[1332] loss: 0.014\n",
            "[1333] loss: 0.015\n",
            "[1334] loss: 0.015\n",
            "[1335] loss: 0.015\n",
            "[1336] loss: 0.014\n",
            "[1337] loss: 0.015\n",
            "[1338] loss: 0.015\n",
            "[1339] loss: 0.014\n",
            "[1340] loss: 0.014\n",
            "[1341] loss: 0.015\n",
            "[1342] loss: 0.015\n",
            "[1343] loss: 0.014\n",
            "[1344] loss: 0.015\n",
            "[1345] loss: 0.014\n",
            "[1346] loss: 0.014\n",
            "[1347] loss: 0.015\n",
            "[1348] loss: 0.014\n",
            "[1349] loss: 0.014\n",
            "[1350] loss: 0.015\n",
            "[1351] loss: 0.015\n",
            "[1352] loss: 0.014\n",
            "[1353] loss: 0.014\n",
            "[1354] loss: 0.015\n",
            "[1355] loss: 0.014\n",
            "[1356] loss: 0.015\n",
            "[1357] loss: 0.015\n",
            "[1358] loss: 0.015\n",
            "[1359] loss: 0.015\n",
            "[1360] loss: 0.015\n",
            "[1361] loss: 0.014\n",
            "[1362] loss: 0.014\n",
            "[1363] loss: 0.015\n",
            "[1364] loss: 0.015\n",
            "[1365] loss: 0.015\n",
            "[1366] loss: 0.014\n",
            "[1367] loss: 0.015\n",
            "[1368] loss: 0.015\n",
            "[1369] loss: 0.014\n",
            "[1370] loss: 0.015\n",
            "[1371] loss: 0.016\n",
            "[1372] loss: 0.015\n",
            "[1373] loss: 0.015\n",
            "[1374] loss: 0.015\n",
            "[1375] loss: 0.015\n",
            "[1376] loss: 0.014\n",
            "[1377] loss: 0.015\n",
            "[1378] loss: 0.014\n",
            "[1379] loss: 0.016\n",
            "[1380] loss: 0.014\n",
            "[1381] loss: 0.014\n",
            "[1382] loss: 0.015\n",
            "[1383] loss: 0.015\n",
            "[1384] loss: 0.015\n",
            "[1385] loss: 0.015\n",
            "[1386] loss: 0.015\n",
            "[1387] loss: 0.014\n",
            "[1388] loss: 0.015\n",
            "[1389] loss: 0.015\n",
            "[1390] loss: 0.014\n",
            "[1391] loss: 0.016\n",
            "[1392] loss: 0.015\n",
            "[1393] loss: 0.015\n",
            "[1394] loss: 0.014\n",
            "[1395] loss: 0.015\n",
            "[1396] loss: 0.015\n",
            "[1397] loss: 0.015\n",
            "[1398] loss: 0.015\n",
            "[1399] loss: 0.015\n",
            "[1400] loss: 0.015\n",
            "[1401] loss: 0.015\n",
            "[1402] loss: 0.015\n",
            "[1403] loss: 0.015\n",
            "[1404] loss: 0.015\n",
            "[1405] loss: 0.014\n",
            "[1406] loss: 0.014\n",
            "[1407] loss: 0.015\n",
            "[1408] loss: 0.014\n",
            "[1409] loss: 0.014\n",
            "[1410] loss: 0.015\n",
            "[1411] loss: 0.014\n",
            "[1412] loss: 0.015\n",
            "[1413] loss: 0.014\n",
            "[1414] loss: 0.015\n",
            "[1415] loss: 0.014\n",
            "[1416] loss: 0.013\n",
            "[1417] loss: 0.014\n",
            "[1418] loss: 0.015\n",
            "[1419] loss: 0.015\n",
            "[1420] loss: 0.013\n",
            "[1421] loss: 0.013\n",
            "[1422] loss: 0.014\n",
            "[1423] loss: 0.014\n",
            "[1424] loss: 0.014\n",
            "[1425] loss: 0.015\n",
            "[1426] loss: 0.014\n",
            "[1427] loss: 0.013\n",
            "[1428] loss: 0.013\n",
            "[1429] loss: 0.014\n",
            "[1430] loss: 0.015\n",
            "[1431] loss: 0.014\n",
            "[1432] loss: 0.014\n",
            "[1433] loss: 0.014\n",
            "[1434] loss: 0.014\n",
            "[1435] loss: 0.014\n",
            "[1436] loss: 0.014\n",
            "[1437] loss: 0.014\n",
            "[1438] loss: 0.014\n",
            "[1439] loss: 0.014\n",
            "[1440] loss: 0.015\n",
            "[1441] loss: 0.014\n",
            "[1442] loss: 0.014\n",
            "[1443] loss: 0.014\n",
            "[1444] loss: 0.014\n",
            "[1445] loss: 0.014\n",
            "[1446] loss: 0.014\n",
            "[1447] loss: 0.014\n",
            "[1448] loss: 0.014\n",
            "[1449] loss: 0.014\n",
            "[1450] loss: 0.014\n",
            "[1451] loss: 0.015\n",
            "[1452] loss: 0.014\n",
            "[1453] loss: 0.014\n",
            "[1454] loss: 0.014\n",
            "[1455] loss: 0.014\n",
            "[1456] loss: 0.013\n",
            "[1457] loss: 0.014\n",
            "[1458] loss: 0.014\n",
            "[1459] loss: 0.014\n",
            "[1460] loss: 0.014\n",
            "[1461] loss: 0.014\n",
            "[1462] loss: 0.015\n",
            "[1463] loss: 0.013\n",
            "[1464] loss: 0.014\n",
            "[1465] loss: 0.014\n",
            "[1466] loss: 0.014\n",
            "[1467] loss: 0.014\n",
            "[1468] loss: 0.014\n",
            "[1469] loss: 0.014\n",
            "[1470] loss: 0.015\n",
            "[1471] loss: 0.014\n",
            "[1472] loss: 0.013\n",
            "[1473] loss: 0.015\n",
            "[1474] loss: 0.014\n",
            "[1475] loss: 0.015\n",
            "[1476] loss: 0.014\n",
            "[1477] loss: 0.014\n",
            "[1478] loss: 0.013\n",
            "[1479] loss: 0.015\n",
            "[1480] loss: 0.013\n",
            "[1481] loss: 0.013\n",
            "[1482] loss: 0.014\n",
            "[1483] loss: 0.014\n",
            "[1484] loss: 0.014\n",
            "[1485] loss: 0.015\n",
            "[1486] loss: 0.014\n",
            "[1487] loss: 0.014\n",
            "[1488] loss: 0.014\n",
            "[1489] loss: 0.014\n",
            "[1490] loss: 0.014\n",
            "[1491] loss: 0.014\n",
            "[1492] loss: 0.013\n",
            "[1493] loss: 0.014\n",
            "[1494] loss: 0.013\n",
            "[1495] loss: 0.014\n",
            "[1496] loss: 0.015\n",
            "[1497] loss: 0.015\n",
            "[1498] loss: 0.013\n",
            "[1499] loss: 0.013\n",
            "[1500] loss: 0.014\n",
            "[1501] loss: 0.014\n",
            "[1502] loss: 0.014\n",
            "[1503] loss: 0.014\n",
            "[1504] loss: 0.014\n",
            "[1505] loss: 0.014\n",
            "[1506] loss: 0.014\n",
            "[1507] loss: 0.014\n",
            "[1508] loss: 0.014\n",
            "[1509] loss: 0.013\n",
            "[1510] loss: 0.014\n",
            "[1511] loss: 0.014\n",
            "[1512] loss: 0.014\n",
            "[1513] loss: 0.014\n",
            "[1514] loss: 0.014\n",
            "[1515] loss: 0.014\n",
            "[1516] loss: 0.014\n",
            "[1517] loss: 0.013\n",
            "[1518] loss: 0.013\n",
            "[1519] loss: 0.014\n",
            "[1520] loss: 0.015\n",
            "[1521] loss: 0.015\n",
            "[1522] loss: 0.014\n",
            "[1523] loss: 0.015\n",
            "[1524] loss: 0.014\n",
            "[1525] loss: 0.014\n",
            "[1526] loss: 0.014\n",
            "[1527] loss: 0.014\n",
            "[1528] loss: 0.014\n",
            "[1529] loss: 0.013\n",
            "[1530] loss: 0.014\n",
            "[1531] loss: 0.013\n",
            "[1532] loss: 0.014\n",
            "[1533] loss: 0.014\n",
            "[1534] loss: 0.014\n",
            "[1535] loss: 0.014\n",
            "[1536] loss: 0.013\n",
            "[1537] loss: 0.015\n",
            "[1538] loss: 0.014\n",
            "[1539] loss: 0.014\n",
            "[1540] loss: 0.013\n",
            "[1541] loss: 0.013\n",
            "[1542] loss: 0.014\n",
            "[1543] loss: 0.013\n",
            "[1544] loss: 0.014\n",
            "[1545] loss: 0.013\n",
            "[1546] loss: 0.014\n",
            "[1547] loss: 0.014\n",
            "[1548] loss: 0.013\n",
            "[1549] loss: 0.014\n",
            "[1550] loss: 0.015\n",
            "[1551] loss: 0.013\n",
            "[1552] loss: 0.013\n",
            "[1553] loss: 0.013\n",
            "[1554] loss: 0.015\n",
            "[1555] loss: 0.014\n",
            "[1556] loss: 0.014\n",
            "[1557] loss: 0.013\n",
            "[1558] loss: 0.014\n",
            "[1559] loss: 0.014\n",
            "[1560] loss: 0.014\n",
            "[1561] loss: 0.014\n",
            "[1562] loss: 0.014\n",
            "[1563] loss: 0.014\n",
            "[1564] loss: 0.013\n",
            "[1565] loss: 0.014\n",
            "[1566] loss: 0.013\n",
            "[1567] loss: 0.015\n",
            "[1568] loss: 0.014\n",
            "[1569] loss: 0.012\n",
            "[1570] loss: 0.015\n",
            "[1571] loss: 0.014\n",
            "[1572] loss: 0.014\n",
            "[1573] loss: 0.014\n",
            "[1574] loss: 0.014\n",
            "[1575] loss: 0.014\n",
            "[1576] loss: 0.014\n",
            "[1577] loss: 0.014\n",
            "[1578] loss: 0.014\n",
            "[1579] loss: 0.014\n",
            "[1580] loss: 0.013\n",
            "[1581] loss: 0.013\n",
            "[1582] loss: 0.014\n",
            "[1583] loss: 0.013\n",
            "[1584] loss: 0.013\n",
            "[1585] loss: 0.013\n",
            "[1586] loss: 0.013\n",
            "[1587] loss: 0.015\n",
            "[1588] loss: 0.014\n",
            "[1589] loss: 0.013\n",
            "[1590] loss: 0.014\n",
            "[1591] loss: 0.014\n",
            "[1592] loss: 0.013\n",
            "[1593] loss: 0.014\n",
            "[1594] loss: 0.014\n",
            "[1595] loss: 0.014\n",
            "[1596] loss: 0.013\n",
            "[1597] loss: 0.014\n",
            "[1598] loss: 0.013\n",
            "[1599] loss: 0.014\n",
            "[1600] loss: 0.014\n",
            "[1601] loss: 0.013\n",
            "[1602] loss: 0.013\n",
            "[1603] loss: 0.013\n",
            "[1604] loss: 0.014\n",
            "[1605] loss: 0.013\n",
            "[1606] loss: 0.013\n",
            "[1607] loss: 0.012\n",
            "[1608] loss: 0.014\n",
            "[1609] loss: 0.013\n",
            "[1610] loss: 0.013\n",
            "[1611] loss: 0.013\n",
            "[1612] loss: 0.014\n",
            "[1613] loss: 0.014\n",
            "[1614] loss: 0.013\n",
            "[1615] loss: 0.013\n",
            "[1616] loss: 0.014\n",
            "[1617] loss: 0.014\n",
            "[1618] loss: 0.014\n",
            "[1619] loss: 0.012\n",
            "[1620] loss: 0.013\n",
            "[1621] loss: 0.014\n",
            "[1622] loss: 0.013\n",
            "[1623] loss: 0.014\n",
            "[1624] loss: 0.014\n",
            "[1625] loss: 0.014\n",
            "[1626] loss: 0.013\n",
            "[1627] loss: 0.013\n",
            "[1628] loss: 0.013\n",
            "[1629] loss: 0.014\n",
            "[1630] loss: 0.013\n",
            "[1631] loss: 0.013\n",
            "[1632] loss: 0.013\n",
            "[1633] loss: 0.013\n",
            "[1634] loss: 0.013\n",
            "[1635] loss: 0.014\n",
            "[1636] loss: 0.013\n",
            "[1637] loss: 0.013\n",
            "[1638] loss: 0.013\n",
            "[1639] loss: 0.012\n",
            "[1640] loss: 0.013\n",
            "[1641] loss: 0.013\n",
            "[1642] loss: 0.014\n",
            "[1643] loss: 0.013\n",
            "[1644] loss: 0.014\n",
            "[1645] loss: 0.012\n",
            "[1646] loss: 0.013\n",
            "[1647] loss: 0.013\n",
            "[1648] loss: 0.013\n",
            "[1649] loss: 0.013\n",
            "[1650] loss: 0.013\n",
            "[1651] loss: 0.013\n",
            "[1652] loss: 0.013\n",
            "[1653] loss: 0.013\n",
            "[1654] loss: 0.013\n",
            "[1655] loss: 0.013\n",
            "[1656] loss: 0.014\n",
            "[1657] loss: 0.014\n",
            "[1658] loss: 0.013\n",
            "[1659] loss: 0.013\n",
            "[1660] loss: 0.013\n",
            "[1661] loss: 0.013\n",
            "[1662] loss: 0.013\n",
            "[1663] loss: 0.013\n",
            "[1664] loss: 0.012\n",
            "[1665] loss: 0.012\n",
            "[1666] loss: 0.013\n",
            "[1667] loss: 0.013\n",
            "[1668] loss: 0.013\n",
            "[1669] loss: 0.013\n",
            "[1670] loss: 0.013\n",
            "[1671] loss: 0.013\n",
            "[1672] loss: 0.013\n",
            "[1673] loss: 0.013\n",
            "[1674] loss: 0.013\n",
            "[1675] loss: 0.013\n",
            "[1676] loss: 0.013\n",
            "[1677] loss: 0.012\n",
            "[1678] loss: 0.013\n",
            "[1679] loss: 0.013\n",
            "[1680] loss: 0.013\n",
            "[1681] loss: 0.013\n",
            "[1682] loss: 0.013\n",
            "[1683] loss: 0.013\n",
            "[1684] loss: 0.013\n",
            "[1685] loss: 0.013\n",
            "[1686] loss: 0.013\n",
            "[1687] loss: 0.013\n",
            "[1688] loss: 0.013\n",
            "[1689] loss: 0.013\n",
            "[1690] loss: 0.013\n",
            "[1691] loss: 0.014\n",
            "[1692] loss: 0.012\n",
            "[1693] loss: 0.013\n",
            "[1694] loss: 0.013\n",
            "[1695] loss: 0.013\n",
            "[1696] loss: 0.013\n",
            "[1697] loss: 0.013\n",
            "[1698] loss: 0.013\n",
            "[1699] loss: 0.013\n",
            "[1700] loss: 0.013\n",
            "[1701] loss: 0.013\n",
            "[1702] loss: 0.013\n",
            "[1703] loss: 0.013\n",
            "[1704] loss: 0.013\n",
            "[1705] loss: 0.012\n",
            "[1706] loss: 0.012\n",
            "[1707] loss: 0.012\n",
            "[1708] loss: 0.013\n",
            "[1709] loss: 0.013\n",
            "[1710] loss: 0.013\n",
            "[1711] loss: 0.013\n",
            "[1712] loss: 0.012\n",
            "[1713] loss: 0.013\n",
            "[1714] loss: 0.013\n",
            "[1715] loss: 0.013\n",
            "[1716] loss: 0.013\n",
            "[1717] loss: 0.013\n",
            "[1718] loss: 0.013\n",
            "[1719] loss: 0.013\n",
            "[1720] loss: 0.013\n",
            "[1721] loss: 0.012\n",
            "[1722] loss: 0.013\n",
            "[1723] loss: 0.013\n",
            "[1724] loss: 0.013\n",
            "[1725] loss: 0.013\n",
            "[1726] loss: 0.013\n",
            "[1727] loss: 0.013\n",
            "[1728] loss: 0.013\n",
            "[1729] loss: 0.013\n",
            "[1730] loss: 0.013\n",
            "[1731] loss: 0.013\n",
            "[1732] loss: 0.013\n",
            "[1733] loss: 0.013\n",
            "[1734] loss: 0.013\n",
            "[1735] loss: 0.013\n",
            "[1736] loss: 0.013\n",
            "[1737] loss: 0.014\n",
            "[1738] loss: 0.013\n",
            "[1739] loss: 0.013\n",
            "[1740] loss: 0.013\n",
            "[1741] loss: 0.013\n",
            "[1742] loss: 0.013\n",
            "[1743] loss: 0.013\n",
            "[1744] loss: 0.013\n",
            "[1745] loss: 0.013\n",
            "[1746] loss: 0.012\n",
            "[1747] loss: 0.013\n",
            "[1748] loss: 0.013\n",
            "[1749] loss: 0.013\n",
            "[1750] loss: 0.013\n",
            "[1751] loss: 0.012\n",
            "[1752] loss: 0.012\n",
            "[1753] loss: 0.013\n",
            "[1754] loss: 0.013\n",
            "[1755] loss: 0.013\n",
            "[1756] loss: 0.013\n",
            "[1757] loss: 0.013\n",
            "[1758] loss: 0.013\n",
            "[1759] loss: 0.013\n",
            "[1760] loss: 0.013\n",
            "[1761] loss: 0.012\n",
            "[1762] loss: 0.013\n",
            "[1763] loss: 0.013\n",
            "[1764] loss: 0.012\n",
            "[1765] loss: 0.012\n",
            "[1766] loss: 0.012\n",
            "[1767] loss: 0.013\n",
            "[1768] loss: 0.013\n",
            "[1769] loss: 0.013\n",
            "[1770] loss: 0.013\n",
            "[1771] loss: 0.013\n",
            "[1772] loss: 0.012\n",
            "[1773] loss: 0.013\n",
            "[1774] loss: 0.012\n",
            "[1775] loss: 0.013\n",
            "[1776] loss: 0.013\n",
            "[1777] loss: 0.013\n",
            "[1778] loss: 0.014\n",
            "[1779] loss: 0.012\n",
            "[1780] loss: 0.014\n",
            "[1781] loss: 0.013\n",
            "[1782] loss: 0.012\n",
            "[1783] loss: 0.014\n",
            "[1784] loss: 0.013\n",
            "[1785] loss: 0.013\n",
            "[1786] loss: 0.014\n",
            "[1787] loss: 0.013\n",
            "[1788] loss: 0.012\n",
            "[1789] loss: 0.013\n",
            "[1790] loss: 0.012\n",
            "[1791] loss: 0.013\n",
            "[1792] loss: 0.013\n",
            "[1793] loss: 0.013\n",
            "[1794] loss: 0.014\n",
            "[1795] loss: 0.012\n",
            "[1796] loss: 0.013\n",
            "[1797] loss: 0.013\n",
            "[1798] loss: 0.012\n",
            "[1799] loss: 0.013\n",
            "[1800] loss: 0.012\n",
            "[1801] loss: 0.012\n",
            "[1802] loss: 0.013\n",
            "[1803] loss: 0.012\n",
            "[1804] loss: 0.012\n",
            "[1805] loss: 0.012\n",
            "[1806] loss: 0.013\n",
            "[1807] loss: 0.013\n",
            "[1808] loss: 0.012\n",
            "[1809] loss: 0.012\n",
            "[1810] loss: 0.012\n",
            "[1811] loss: 0.012\n",
            "[1812] loss: 0.013\n",
            "[1813] loss: 0.013\n",
            "[1814] loss: 0.012\n",
            "[1815] loss: 0.013\n",
            "[1816] loss: 0.012\n",
            "[1817] loss: 0.012\n",
            "[1818] loss: 0.013\n",
            "[1819] loss: 0.013\n",
            "[1820] loss: 0.013\n",
            "[1821] loss: 0.012\n",
            "[1822] loss: 0.012\n",
            "[1823] loss: 0.013\n",
            "[1824] loss: 0.012\n",
            "[1825] loss: 0.012\n",
            "[1826] loss: 0.012\n",
            "[1827] loss: 0.011\n",
            "[1828] loss: 0.012\n",
            "[1829] loss: 0.013\n",
            "[1830] loss: 0.012\n",
            "[1831] loss: 0.012\n",
            "[1832] loss: 0.014\n",
            "[1833] loss: 0.014\n",
            "[1834] loss: 0.013\n",
            "[1835] loss: 0.012\n",
            "[1836] loss: 0.013\n",
            "[1837] loss: 0.013\n",
            "[1838] loss: 0.014\n",
            "[1839] loss: 0.012\n",
            "[1840] loss: 0.013\n",
            "[1841] loss: 0.012\n",
            "[1842] loss: 0.013\n",
            "[1843] loss: 0.013\n",
            "[1844] loss: 0.013\n",
            "[1845] loss: 0.012\n",
            "[1846] loss: 0.012\n",
            "[1847] loss: 0.012\n",
            "[1848] loss: 0.012\n",
            "[1849] loss: 0.013\n",
            "[1850] loss: 0.013\n",
            "[1851] loss: 0.012\n",
            "[1852] loss: 0.012\n",
            "[1853] loss: 0.013\n",
            "[1854] loss: 0.012\n",
            "[1855] loss: 0.013\n",
            "[1856] loss: 0.012\n",
            "[1857] loss: 0.013\n",
            "[1858] loss: 0.012\n",
            "[1859] loss: 0.012\n",
            "[1860] loss: 0.013\n",
            "[1861] loss: 0.012\n",
            "[1862] loss: 0.013\n",
            "[1863] loss: 0.012\n",
            "[1864] loss: 0.013\n",
            "[1865] loss: 0.012\n",
            "[1866] loss: 0.012\n",
            "[1867] loss: 0.013\n",
            "[1868] loss: 0.013\n",
            "[1869] loss: 0.012\n",
            "[1870] loss: 0.012\n",
            "[1871] loss: 0.013\n",
            "[1872] loss: 0.013\n",
            "[1873] loss: 0.012\n",
            "[1874] loss: 0.012\n",
            "[1875] loss: 0.012\n",
            "[1876] loss: 0.012\n",
            "[1877] loss: 0.013\n",
            "[1878] loss: 0.012\n",
            "[1879] loss: 0.013\n",
            "[1880] loss: 0.013\n",
            "[1881] loss: 0.012\n",
            "[1882] loss: 0.012\n",
            "[1883] loss: 0.012\n",
            "[1884] loss: 0.012\n",
            "[1885] loss: 0.013\n",
            "[1886] loss: 0.012\n",
            "[1887] loss: 0.013\n",
            "[1888] loss: 0.012\n",
            "[1889] loss: 0.012\n",
            "[1890] loss: 0.012\n",
            "[1891] loss: 0.012\n",
            "[1892] loss: 0.012\n",
            "[1893] loss: 0.012\n",
            "[1894] loss: 0.012\n",
            "[1895] loss: 0.012\n",
            "[1896] loss: 0.012\n",
            "[1897] loss: 0.012\n",
            "[1898] loss: 0.012\n",
            "[1899] loss: 0.012\n",
            "[1900] loss: 0.012\n",
            "[1901] loss: 0.012\n",
            "[1902] loss: 0.012\n",
            "[1903] loss: 0.012\n",
            "[1904] loss: 0.012\n",
            "[1905] loss: 0.012\n",
            "[1906] loss: 0.012\n",
            "[1907] loss: 0.013\n",
            "[1908] loss: 0.012\n",
            "[1909] loss: 0.012\n",
            "[1910] loss: 0.012\n",
            "[1911] loss: 0.013\n",
            "[1912] loss: 0.012\n",
            "[1913] loss: 0.012\n",
            "[1914] loss: 0.011\n",
            "[1915] loss: 0.012\n",
            "[1916] loss: 0.012\n",
            "[1917] loss: 0.011\n",
            "[1918] loss: 0.013\n",
            "[1919] loss: 0.012\n",
            "[1920] loss: 0.012\n",
            "[1921] loss: 0.012\n",
            "[1922] loss: 0.012\n",
            "[1923] loss: 0.012\n",
            "[1924] loss: 0.012\n",
            "[1925] loss: 0.012\n",
            "[1926] loss: 0.012\n",
            "[1927] loss: 0.012\n",
            "[1928] loss: 0.012\n",
            "[1929] loss: 0.012\n",
            "[1930] loss: 0.013\n",
            "[1931] loss: 0.012\n",
            "[1932] loss: 0.012\n",
            "[1933] loss: 0.012\n",
            "[1934] loss: 0.012\n",
            "[1935] loss: 0.012\n",
            "[1936] loss: 0.012\n",
            "[1937] loss: 0.012\n",
            "[1938] loss: 0.012\n",
            "[1939] loss: 0.012\n",
            "[1940] loss: 0.012\n",
            "[1941] loss: 0.012\n",
            "[1942] loss: 0.013\n",
            "[1943] loss: 0.012\n",
            "[1944] loss: 0.012\n",
            "[1945] loss: 0.012\n",
            "[1946] loss: 0.012\n",
            "[1947] loss: 0.012\n",
            "[1948] loss: 0.012\n",
            "[1949] loss: 0.013\n",
            "[1950] loss: 0.013\n",
            "[1951] loss: 0.013\n",
            "[1952] loss: 0.013\n",
            "[1953] loss: 0.012\n",
            "[1954] loss: 0.012\n",
            "[1955] loss: 0.012\n",
            "[1956] loss: 0.013\n",
            "[1957] loss: 0.012\n",
            "[1958] loss: 0.013\n",
            "[1959] loss: 0.012\n",
            "[1960] loss: 0.012\n",
            "[1961] loss: 0.013\n",
            "[1962] loss: 0.012\n",
            "[1963] loss: 0.012\n",
            "[1964] loss: 0.013\n",
            "[1965] loss: 0.013\n",
            "[1966] loss: 0.012\n",
            "[1967] loss: 0.012\n",
            "[1968] loss: 0.013\n",
            "[1969] loss: 0.012\n",
            "[1970] loss: 0.012\n",
            "[1971] loss: 0.013\n",
            "[1972] loss: 0.012\n",
            "[1973] loss: 0.012\n",
            "[1974] loss: 0.013\n",
            "[1975] loss: 0.012\n",
            "[1976] loss: 0.012\n",
            "[1977] loss: 0.013\n",
            "[1978] loss: 0.012\n",
            "[1979] loss: 0.012\n",
            "[1980] loss: 0.013\n",
            "[1981] loss: 0.012\n",
            "[1982] loss: 0.013\n",
            "[1983] loss: 0.012\n",
            "[1984] loss: 0.013\n",
            "[1985] loss: 0.012\n",
            "[1986] loss: 0.012\n",
            "[1987] loss: 0.012\n",
            "[1988] loss: 0.012\n",
            "[1989] loss: 0.012\n",
            "[1990] loss: 0.012\n",
            "[1991] loss: 0.012\n",
            "[1992] loss: 0.012\n",
            "[1993] loss: 0.012\n",
            "[1994] loss: 0.012\n",
            "[1995] loss: 0.012\n",
            "[1996] loss: 0.012\n",
            "[1997] loss: 0.012\n",
            "[1998] loss: 0.012\n",
            "[1999] loss: 0.012\n",
            "[2000] loss: 0.012\n",
            "Finished Training\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Xqz-DlqmCT",
        "colab_type": "text"
      },
      "source": [
        "Run Deep Teacher model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI5BL9YF2dzp",
        "colab_type": "code",
        "outputId": "d4ff7c33-ab73-4d93-c954-c0b4803a85c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Instantiate model and load saved network parameters\n",
        "net = Model().to(device)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# Run model on test set and determine accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = convert_labels(labels).to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, target = torch.max(target.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "# Output model accuracy to user\n",
        "print('Accuracy of the network on the 10000 test images: %d %% (%d wrong out of %d)' % (\n",
        "    100 * correct / total, total - correct, total))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 99 % (61 wrong out of 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigRhVYyqvL1",
        "colab_type": "text"
      },
      "source": [
        "Train student model to mimic the teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOPgLsQ6D3Lj",
        "colab_type": "code",
        "outputId": "0ad51908-5c85-4ca7-9d31-fe8127d8ac9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "# Custom student loss: linear combination of 2 cross-entropy losses\n",
        "#     The first one between student output and hard labels\n",
        "#     The second one between student output and soft labels from teacher\n",
        "\n",
        "def student_loss(outputs, labels, teacher_outputs, alpha, temperature):\n",
        "\n",
        "    #loss = torch.mean((weight*(outputA - targetA)**2) + (1-weight)*(outputB-targetB)**2)\n",
        "\n",
        "    #loss = weight*(F.cross_entropy(outputA, targetA)) + (1-weight)*(F.cross_entropy(outputB, targetB))\n",
        "\n",
        "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "# Setup student model and move it to the GPU\n",
        "student_net = Model(hidden_size = 800)\n",
        "student_net.to(device)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "\n",
        "optimizer = optim.SGD(student_net.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = torch.optim.Adam(student_net.parameters(), lr=0.0001)\n",
        "\n",
        "# Run over 100 epochs (1 epoch = visited all items in dataset)\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = labels.to(device).long() #convert_labels(labels).to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Set temperature and the weights for losses linear combination\n",
        "        w = 0.3\n",
        "        T = 20\n",
        "\n",
        "        # Compute soft labels using deep teacher model previously trained\n",
        "        outputs_teacher = net(inputs)\n",
        "        #soft_labels = F.softmax(outputs_teacher/T, dim = 1)\n",
        "\n",
        "        # Abomination to obtain hard_labels for custom cross entropy loss\n",
        "        #teacher_hard_labels = torch.from_numpy(np.array([np.argmax(l.cpu().detach().numpy()) for l in soft_labels])).to(device).long()\n",
        "\n",
        "        # Student forward + backward + optimize\n",
        "        outputs_stud = student_net(inputs)\n",
        "        #outputs_stud = F.softmax(output_stud/T, dim = 1)\n",
        "        \n",
        "        loss = student_loss(outputs_stud, target, outputs_teacher, w, T)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += len(data)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # print every epoch\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save model after having finished training\n",
        "STUD_PATH = './mnist_student_100_epoch.pth'\n",
        "torch.save(student_net.state_dict(), STUD_PATH)\n",
        "\n",
        "print('Saved Model')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 1.408\n",
            "[2] loss: 0.948\n",
            "[3] loss: 0.811\n",
            "[4] loss: 0.748\n",
            "[5] loss: 0.694\n",
            "[6] loss: 0.643\n",
            "[7] loss: 0.590\n",
            "[8] loss: 0.530\n",
            "[9] loss: 0.475\n",
            "[10] loss: 0.420\n",
            "[11] loss: 0.375\n",
            "[12] loss: 0.338\n",
            "[13] loss: 0.308\n",
            "[14] loss: 0.285\n",
            "[15] loss: 0.265\n",
            "[16] loss: 0.248\n",
            "[17] loss: 0.234\n",
            "[18] loss: 0.220\n",
            "[19] loss: 0.209\n",
            "[20] loss: 0.197\n",
            "[21] loss: 0.187\n",
            "[22] loss: 0.180\n",
            "[23] loss: 0.172\n",
            "[24] loss: 0.166\n",
            "[25] loss: 0.159\n",
            "[26] loss: 0.153\n",
            "[27] loss: 0.149\n",
            "[28] loss: 0.143\n",
            "[29] loss: 0.139\n",
            "[30] loss: 0.135\n",
            "[31] loss: 0.130\n",
            "[32] loss: 0.127\n",
            "[33] loss: 0.124\n",
            "[34] loss: 0.120\n",
            "[35] loss: 0.117\n",
            "[36] loss: 0.115\n",
            "[37] loss: 0.112\n",
            "[38] loss: 0.111\n",
            "[39] loss: 0.107\n",
            "[40] loss: 0.105\n",
            "[41] loss: 0.103\n",
            "[42] loss: 0.102\n",
            "[43] loss: 0.099\n",
            "[44] loss: 0.096\n",
            "[45] loss: 0.095\n",
            "[46] loss: 0.094\n",
            "[47] loss: 0.091\n",
            "[48] loss: 0.089\n",
            "[49] loss: 0.089\n",
            "[50] loss: 0.089\n",
            "[51] loss: 0.087\n",
            "[52] loss: 0.085\n",
            "[53] loss: 0.085\n",
            "[54] loss: 0.082\n",
            "[55] loss: 0.081\n",
            "[56] loss: 0.080\n",
            "[57] loss: 0.080\n",
            "[58] loss: 0.079\n",
            "[59] loss: 0.077\n",
            "[60] loss: 0.077\n",
            "[61] loss: 0.076\n",
            "[62] loss: 0.075\n",
            "[63] loss: 0.074\n",
            "[64] loss: 0.073\n",
            "[65] loss: 0.073\n",
            "[66] loss: 0.072\n",
            "[67] loss: 0.069\n",
            "[68] loss: 0.070\n",
            "[69] loss: 0.070\n",
            "[70] loss: 0.068\n",
            "[71] loss: 0.068\n",
            "[72] loss: 0.067\n",
            "[73] loss: 0.066\n",
            "[74] loss: 0.066\n",
            "[75] loss: 0.066\n",
            "[76] loss: 0.063\n",
            "[77] loss: 0.064\n",
            "[78] loss: 0.064\n",
            "[79] loss: 0.062\n",
            "[80] loss: 0.061\n",
            "[81] loss: 0.062\n",
            "[82] loss: 0.061\n",
            "[83] loss: 0.061\n",
            "[84] loss: 0.061\n",
            "[85] loss: 0.060\n",
            "[86] loss: 0.059\n",
            "[87] loss: 0.057\n",
            "[88] loss: 0.057\n",
            "[89] loss: 0.058\n",
            "[90] loss: 0.058\n",
            "[91] loss: 0.056\n",
            "[92] loss: 0.056\n",
            "[93] loss: 0.055\n",
            "[94] loss: 0.056\n",
            "[95] loss: 0.055\n",
            "[96] loss: 0.055\n",
            "[97] loss: 0.055\n",
            "[98] loss: 0.054\n",
            "[99] loss: 0.054\n",
            "[100] loss: 0.053\n",
            "Finished Training\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8FmA6zWq2wW",
        "colab_type": "text"
      },
      "source": [
        "Running student model on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1fxqFeILgTp",
        "colab_type": "code",
        "outputId": "d02a3b77-d5e2-46a6-81cb-7b1851f8e95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stud_net = Model(hidden_size = 800).to(device)\n",
        "stud_net.load_state_dict(torch.load(STUD_PATH))\n",
        "\n",
        "# Run model on test set and determine accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
        "        target = convert_labels(labels).to(device)\n",
        "        outputs = stud_net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, target = torch.max(target.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "# Output model accuracy to user\n",
        "print('Accuracy of the network on the 10000 test images: %d %% (%d wrong out of %d)' % (\n",
        "    100 * correct / total, total - correct, total))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 98 % (124 wrong out of 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}