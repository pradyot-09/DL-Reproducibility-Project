{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyiJkYk06At4dSkebuDTwz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradyot-09/DL-Reproducibility-Project/blob/master/CNNModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZwuHLh0DnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pytorch basic functions/classes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import torchvision functions/classes for MNIST import and data loaders\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set device on which code is run\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5siRsiAE0H9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transform from PIL image to tensor and normalize to 1x768 pixels\n",
        "transform_train = transforms.Compose([\n",
        "  transforms.RandomAffine(0, (1/14, 1/14)),\n",
        "  transforms.Resize(28),\n",
        "  transforms.CenterCrop(28),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, ), (0.5, ))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "  transforms.Resize(28),\n",
        "  transforms.CenterCrop(28),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, ), (0.5, ))\n",
        "])\n",
        "\n",
        "# Set batch size for data loaders\n",
        "batch_size = 32\n",
        "\n",
        "# (Down)load training set\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# (Down)load test set\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJoaRGdx3K5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define CNN model and its layers\n",
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=128, dropout=0.25, hidden_dropout=0.5):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, padding=4)\n",
        "        self.conv2 = nn.Conv2d(8,16, kernel_size=5, stride=2)\n",
        "        self.dropout1 = nn.Dropout2d(dropout)\n",
        "        self.dropout2 = nn.Dropout2d(hidden_dropout)\n",
        "        self.fc1 = nn.Linear(784, hidden_size*2)\n",
        "        self.fc2 = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolution and activation\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        # Second convolution, max pool and activation\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(x)\n",
        "        # Flatten input into 1d tensor and random dropout to increase generalization\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout1(x)\n",
        "        # First FFN layer, activation and dropout\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        # Second FFN layer and activation\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        # Last FFN layer\n",
        "        return self.fc3(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV8-u1FI0g4e",
        "colab_type": "code",
        "outputId": "7079f5a6-bbd6-4b17-9088-9f2166a91dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Setup model and move it to the GPU\n",
        "net = CNNModel()\n",
        "net.to(device)\n",
        "\n",
        "# Set up loss function and optimizer:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=0.00001)\n",
        "\n",
        "# Run over 100 epochs (1 epoch = visited all items in dataset)\n",
        "for epoch in range(100): #200 for SGD reaches 0.002 loss\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = inputs.to(device)\n",
        "        target = labels.to(device).long()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += len(inputs)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    # print every epoch\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / total))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save model after having finished training\n",
        "PATH = './mnist_dropout_100_epoch.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Saved Model')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] loss: 0.025\n",
            "[2] loss: 0.009\n",
            "[3] loss: 0.007\n",
            "[4] loss: 0.006\n",
            "[5] loss: 0.005\n",
            "[6] loss: 0.004\n",
            "[7] loss: 0.004\n",
            "[8] loss: 0.004\n",
            "[9] loss: 0.003\n",
            "[10] loss: 0.003\n",
            "[11] loss: 0.003\n",
            "[12] loss: 0.003\n",
            "[13] loss: 0.003\n",
            "[14] loss: 0.003\n",
            "[15] loss: 0.003\n",
            "[16] loss: 0.003\n",
            "[17] loss: 0.003\n",
            "[18] loss: 0.002\n",
            "[19] loss: 0.002\n",
            "[20] loss: 0.002\n",
            "[21] loss: 0.002\n",
            "[22] loss: 0.002\n",
            "[23] loss: 0.002\n",
            "[24] loss: 0.002\n",
            "[25] loss: 0.002\n",
            "[26] loss: 0.002\n",
            "[27] loss: 0.002\n",
            "[28] loss: 0.002\n",
            "[29] loss: 0.002\n",
            "[30] loss: 0.002\n",
            "[31] loss: 0.002\n",
            "[32] loss: 0.002\n",
            "[33] loss: 0.002\n",
            "[34] loss: 0.002\n",
            "[35] loss: 0.002\n",
            "[36] loss: 0.002\n",
            "[37] loss: 0.002\n",
            "[38] loss: 0.002\n",
            "[39] loss: 0.002\n",
            "[40] loss: 0.002\n",
            "[41] loss: 0.002\n",
            "[42] loss: 0.002\n",
            "[43] loss: 0.002\n",
            "[44] loss: 0.002\n",
            "[45] loss: 0.002\n",
            "[46] loss: 0.002\n",
            "[47] loss: 0.002\n",
            "[48] loss: 0.002\n",
            "[49] loss: 0.002\n",
            "[50] loss: 0.002\n",
            "[51] loss: 0.002\n",
            "[52] loss: 0.001\n",
            "[53] loss: 0.002\n",
            "[54] loss: 0.001\n",
            "[55] loss: 0.001\n",
            "[56] loss: 0.001\n",
            "[57] loss: 0.001\n",
            "[58] loss: 0.001\n",
            "[59] loss: 0.001\n",
            "[60] loss: 0.001\n",
            "[61] loss: 0.001\n",
            "[62] loss: 0.001\n",
            "[63] loss: 0.001\n",
            "[64] loss: 0.001\n",
            "[65] loss: 0.001\n",
            "[66] loss: 0.001\n",
            "[67] loss: 0.001\n",
            "[68] loss: 0.001\n",
            "[69] loss: 0.001\n",
            "[70] loss: 0.001\n",
            "[71] loss: 0.001\n",
            "[72] loss: 0.001\n",
            "[73] loss: 0.001\n",
            "[74] loss: 0.001\n",
            "[75] loss: 0.001\n",
            "[76] loss: 0.001\n",
            "[77] loss: 0.001\n",
            "[78] loss: 0.001\n",
            "[79] loss: 0.001\n",
            "[80] loss: 0.001\n",
            "[81] loss: 0.001\n",
            "[82] loss: 0.001\n",
            "[83] loss: 0.001\n",
            "[84] loss: 0.001\n",
            "[85] loss: 0.001\n",
            "[86] loss: 0.001\n",
            "[87] loss: 0.001\n",
            "[88] loss: 0.001\n",
            "[89] loss: 0.001\n",
            "[90] loss: 0.001\n",
            "[91] loss: 0.001\n",
            "[92] loss: 0.001\n",
            "[93] loss: 0.001\n",
            "[94] loss: 0.001\n",
            "[95] loss: 0.001\n",
            "[96] loss: 0.001\n",
            "[97] loss: 0.001\n",
            "[98] loss: 0.001\n",
            "[99] loss: 0.001\n",
            "[100] loss: 0.001\n",
            "Finished Training\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_cGzny4acEs",
        "colab_type": "code",
        "outputId": "c40a47cd-dba2-44a2-cc44-aa1aaf750c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Instantiate model and load saved network parameters\n",
        "net = CNNModel(dropout=0.0, hidden_dropout=0.0)\n",
        "net.to(device)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# Run model on test set and determine accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for (inputs, labels) in testloader:\n",
        "        inputs = inputs.to(device)\n",
        "        target = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        predicted = torch.argmax(outputs.data, 1)\n",
        "        target = target.data\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "# Output model accuracy to user\n",
        "print('Accuracy of the network on the 10000 test images: %f %% (%d wrong out of %d)' % (\n",
        "    100 * correct / total, total - correct, total))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 99.530000 % (47 wrong out of 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}