# The Reproduction of “Distilling the Knowledge in a Neural Network”


![Neural Networks](https://img.shields.io/badge/Neural%20Networks-Distillation-brightgreen)
![Reproducibility](https://img.shields.io/badge/Reproducibility-Blog-blue)

## Introduction

The aim of this project was to reproduce results from section 3 of [1].

Distillation is a technique aiming to transfer the knowledge acquired by a cumbersome model (teacher) to a simpler one (student): very complex models could be computationally expensive to deploy in a production environment, hence the advantage of having a smaller and lighter model with the same knowledge.

We will start by describing how we structured our teacher and student models and how did we train the latter to mimic the former. Then we will describe our experiments and results, comparing these to the authors achievements.

## Methodology



## Experiment Setup


## Results


## References

[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arXiv preprint arXiv:1503.02531 (2015).
